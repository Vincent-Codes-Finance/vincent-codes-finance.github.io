<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vincent Grégoire, PhD, CFA">
<meta name="dcterms.date" content="2024-03-08">
<meta name="description" content="Explore the benefits of using Ollama as a ChatGPT replacement for empirical finance research, focusing on privacy, customization, and computational flexibility.">

<title>Vincent Codes Finance - Using Ollama as a ChatGPT Replacement</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XG6JSNQZJK"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XG6JSNQZJK', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  


<meta property="og:title" content="Vincent Codes Finance - Using Ollama as a ChatGPT Replacement">
<meta property="og:description" content="A blog about coding (mostly) in Python for empirical research in finance">
<meta property="og:image" content="https://vincent.codes.finance/posts/ollama-chatgpt/futuristic_llama.webp">
<meta property="og:site_name" content="Vincent Codes Finance">
<meta property="og:locale" content="en_US">
<meta name="twitter:title" content="Vincent Codes Finance - Using Ollama as a ChatGPT Replacement">
<meta name="twitter:description" content="A blog about coding (mostly) in Python for empirical research in finance">
<meta name="twitter:image" content="https://vincent.codes.finance/posts/ollama-chatgpt/futuristic_llama.webp">
<meta name="twitter:card" content="summary_large_image">
<link rel="canonical" href="https://vincent.codes.finance/posts/ollama-chatgpt/" />
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../vcf-logo-small.webp" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Vincent Codes Finance</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-python-tutorials" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Python tutorials</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-python-tutorials">    
        <li>
    <a class="dropdown-item" href="../../posts/install-python-312/index.html">
 <span class="dropdown-text">Install Python 3.12</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../intro-to-python/python-basics/index.html">
 <span class="dropdown-text">Python basics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About VCF</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Vincent-Codes-Finance"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCuNrwdpvF1LBGJ2cNL5b68w"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/CodesFinance"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.threads.net/@codesfinance"> <i class="bi bi-threads" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/company/vincent-codes-finance/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.facebook.com/profile.php?id=61559283113665"> <i class="bi bi-facebook" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Using Ollama as a ChatGPT Replacement</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Local AI</div>
                <div class="quarto-category">LLM</div>
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">AI</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Vincent Grégoire, PhD, CFA </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 8, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">

<!--Zoho Campaigns Web-Optin Form's Header Code Starts Here-->

<script type="text/javascript" src="https://zmp-glf.maillist-manage.com/js/optin.min.js" onload="setupSF('sf3zebf6c0313d0ddfd85656d496c903350205cbbde44a7d630868cbe868c84c5a8a','ZCFORMVIEW',false,'light',false,'0')"></script>
<script type="text/javascript">
    function runOnFormSubmit_sf3zebf6c0313d0ddfd85656d496c903350205cbbde44a7d630868cbe868c84c5a8a(th){
        /*Before submit, if you want to trigger your event, "include your code here"*/
    };
</script>

<style>
.quick_form_8_css * {
    -webkit-box-sizing: border-box !important;
    -moz-box-sizing: border-box !important;
    box-sizing: border-box !important;
    overflow-wrap: break-word
}
@media only screen and (max-width: 600px) {.quick_form_8_css[name="SIGNUP_BODY"] { width: 100% !important; min-width: 100% !important; margin: 0px auto !important; padding: 0px !important } .SIGNUP_FLD { width: 90% !important; margin: 10px 5% !important; padding: 0px !important } .SIGNUP_FLD input { margin: 0 !important; border-radius: 25px !important } }
</style>

<!--Zoho Campaigns Web-Optin Form's Header Code Ends Here--><!--Zoho Campaigns Web-Optin Form Starts Here-->

<div id="sf3zebf6c0313d0ddfd85656d496c903350205cbbde44a7d630868cbe868c84c5a8a" data-type="signupform" style="opacity: 1;">
    <div id="customForm">
        <div class="quick_form_8_css" style=" z-index: 2; border-width: 1px; border-style: solid; border-color: #b4befe; overflow: hidden; width: 208px" name="SIGNUP_BODY">
            <div>
                <div style="font-size: 14px; font-weight: bold; text-align: left; padding: 10px 20px 5px; display: block; height: 20px; width: 293px;" id="SIGNUP_HEADING">Subscribe to newsletter</div>
                <div style="position:relative;">
                    <div id="Zc_SignupSuccess" style="display:none;position:absolute;margin-left:4%;width:90%;background-color: white; padding: 3px; border: 3px solid rgb(194, 225, 154);  margin-top: 10px;margin-bottom:10px;word-break:break-all">
                        
<table class="caption-top table" data-quarto-postprocess="true" width="100%" data-cellpadding="0" data-cellspacing="0" data-border="0">
<tbody>
<tr class="odd">
<td width="10%"><img src="https://zmp-glf.maillist-manage.com/images/challangeiconenable.jpg" class="successicon img-fluid" data-align="absmiddle"></td>
<td><span id="signupSuccessMsg" style="color: rgb(73, 140, 132); font-size: 14px;word-break:break-word">&nbsp;&nbsp;Thank you for Signing Up</span></td>
</tr>
</tbody>
</table>

                    </div>
                </div>
                <form method="POST" id="zcampaignOptinForm" style="margin: 0px; width: 100%" action="https://zmp-glf.maillist-manage.com/weboptin.zc" target="_zcSignup">
                    <div style="background-color: rgb(255, 235, 232); padding: 10px; color: rgb(210, 0, 0); font-size: 11px; margin: 20px 10px 0px; border: 1px solid rgb(255, 217, 211); opacity: 1; display: none" id="errorMsgDiv">Invalid email.</div>
                    <div style="position: relative; margin: 10px; height: 30px; display: inline-block; width: 185px" class="SIGNUP_FLD">
                        <input type="text" style="border: 1px solid rgb(221, 221, 221); border-radius: 0px; width: 100%; height: 100%; z-index: 4; outline: none; padding: 5px 10px; color: rgb(136, 136, 136); text-align: left; background-color: rgb(255, 255, 255); box-sizing: border-box; font-size: 11px" placeholder="Email" changeitem="SIGNUP_FORM_FIELD" name="CONTACT_EMAIL" id="EMBED_FORM_EMAIL_LABEL">
                    </div>
                    <div style="position: relative; margin: 10px; width: 100px; height: 30px; text-align: left; display: inline-block" class="SIGNUP_FLD">
                        <input type="button" style="text-align: center; border-radius: 5px; width: 100%; height: 100%; z-index: 5; border: 0px; color: rgb(255, 255, 255); cursor: pointer; outline: none; font-size: 14px; background-color: #fab387; margin: 0px 0px 0px -5px" name="SIGNUP_SUBMIT_BUTTON" id="zcWebOptin" value="Subscribe">
                    </div>
                    <input type="hidden" id="fieldBorder" value="">
                    <input type="hidden" id="submitType" name="submitType" value="optinCustomView">
                    <input type="hidden" id="emailReportId" name="emailReportId" value="">
                    <input type="hidden" id="formType" name="formType" value="QuickForm">
                    <input type="hidden" name="zx" id="cmpZuid" value="132186a6c">
                    <input type="hidden" name="zcvers" value="3.0">
                    <input type="hidden" name="oldListIds" id="allCheckedListIds" value="">
                    <input type="hidden" id="mode" name="mode" value="OptinCreateView">
                    <input type="hidden" id="zcld" name="zcld" value="1109faaf6213b38dc">
                    <input type="hidden" id="zctd" name="zctd" value="1109faaf6213b37a9">
                    <input type="hidden" id="document_domain" value="">
                    <input type="hidden" id="zc_Url" value="zmp-glf.maillist-manage.com">
                    <input type="hidden" id="new_optin_response_in" value="0">
                    <input type="hidden" id="duplicate_optin_response_in" value="0">
                    <input type="hidden" name="zc_trackCode" id="zc_trackCode" value="ZCFORMVIEW">
                    <input type="hidden" id="zc_formIx" name="zc_formIx" value="3zebf6c0313d0ddfd85656d496c903350205cbbde44a7d630868cbe868c84c5a8a">
                    <input type="hidden" id="viewFrom" value="URL_ACTION">
                    <span style="display: none" id="dt_CONTACT_EMAIL">1,true,6,Contact Email,2</span>
                </form>
            </div>
        </div>
    </div>
    <img src="https://zmp-glf.maillist-manage.com/images/spacer.gif" id="refImage" onload="referenceSetter(this)" style="display:none;">
</div>
<input type="hidden" id="signupFormType" value="QuickForm_Horizontal">
<div id="zcOptinOverLay" oncontextmenu="return false" style="display:none;text-align: center; background-color: rgb(0, 0, 0); opacity: 0.5; z-index: 100; position: fixed; width: 100%; top: 0px; left: 0px; height: 988px;"></div>
<div id="zcOptinSuccessPopup" style="display:none;z-index: 9999;width: 800px; height: 40%;top: 84px;position: fixed; left: 26%;background-color: #FFFFFF;border-color: #E6E6E6; border-style: solid; border-width: 1px;  box-shadow: 0 1px 10px #424242;padding: 35px;">
    <span style="position: absolute;top: -16px;right:-14px;z-index:99999;cursor: pointer;" id="closeSuccess">
        <img src="https://zmp-glf.maillist-manage.com/images/videoclose.png">
    </span>
    <div id="zcOptinSuccessPanel"></div>
</div>

<!--Zoho Campaigns Web-Optin Form Ends Here-->

<div style="padding: 15px 0px"><script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="codesfinance" data-color="#FFDD00" data-emoji="☕" data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff"></script></div>

</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#video-tutorial" id="toc-video-tutorial" class="nav-link active" data-scroll-target="#video-tutorial">Video tutorial</a></li>
  <li><a href="#ollama" id="toc-ollama" class="nav-link" data-scroll-target="#ollama">Ollama</a>
  <ul class="collapse">
  <li><a href="#what-is-ollama" id="toc-what-is-ollama" class="nav-link" data-scroll-target="#what-is-ollama">What is Ollama?</a></li>
  <li><a href="#the-power-of-local-processing" id="toc-the-power-of-local-processing" class="nav-link" data-scroll-target="#the-power-of-local-processing">The Power of Local Processing</a></li>
  <li><a href="#limitations-of-local-llms" id="toc-limitations-of-local-llms" class="nav-link" data-scroll-target="#limitations-of-local-llms">Limitations of Local LLMs</a></li>
  <li><a href="#front-end-applications-for-enhanced-interaction" id="toc-front-end-applications-for-enhanced-interaction" class="nav-link" data-scroll-target="#front-end-applications-for-enhanced-interaction">Front-End Applications for Enhanced Interaction</a></li>
  <li><a href="#other-use-cases" id="toc-other-use-cases" class="nav-link" data-scroll-target="#other-use-cases">Other Use Cases</a></li>
  </ul></li>
  <li><a href="#setting-up-ollama" id="toc-setting-up-ollama" class="nav-link" data-scroll-target="#setting-up-ollama">Setting Up Ollama</a>
  <ul class="collapse">
  <li><a href="#download-and-installation" id="toc-download-and-installation" class="nav-link" data-scroll-target="#download-and-installation">Download and Installation</a></li>
  <li><a href="#downloading-and-running-llm-models" id="toc-downloading-and-running-llm-models" class="nav-link" data-scroll-target="#downloading-and-running-llm-models">Downloading and Running LLM Models</a></li>
  <li><a href="#model-variants-and-sizes" id="toc-model-variants-and-sizes" class="nav-link" data-scroll-target="#model-variants-and-sizes">Model Variants and Sizes</a></li>
  <li><a href="#models-i-use" id="toc-models-i-use" class="nav-link" data-scroll-target="#models-i-use">Models I Use</a></li>
  </ul></li>
  <li><a href="#setting-up-front-end-applications" id="toc-setting-up-front-end-applications" class="nav-link" data-scroll-target="#setting-up-front-end-applications">Setting Up Front-End Applications</a>
  <ul class="collapse">
  <li><a href="#ollama-ui" id="toc-ollama-ui" class="nav-link" data-scroll-target="#ollama-ui">Ollama-UI</a></li>
  <li><a href="#open-webui" id="toc-open-webui" class="nav-link" data-scroll-target="#open-webui">Open WebUI</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Local Large Language Models (LLMs) like Ollama offer a powerful alternative to cloud-based solutions like ChatGPT. In this post, explore the benefits of using Ollama as a ChatGPT replacement for empirical finance research, focusing on privacy, customization, and computational flexibility. ChatGPT has become a daily driver for many researchers and professionals, including myself, offering a powerful tool for generating text, code, and insights across a wide range of domains. However, the reliance on closed, cloud-based solutions for leveraging Large Language Models (LLMs) like ChatGPT comes with inherent privacy, data security, and replicability concerns, especially in fields like empirical finance research.</p>
<p>Enter Ollama, a tool that allows researchers and professionals to manage and run open-source LLMs such as Meta’s Llamma 2 and Mistral AI’s Mistral, bypassing the need for cloud-based solutions. This post, the first in a series of three, aims to demystify the process of installing and using Ollama as a ChatGPT replacement. We’ll delve into the advantages of running LLMs on your own machines, offering you full control over your data while catering to the specific needs of finance-related research. Subsequent posts will explore Ollama’s prowess as a replacement for GitHub Copilot, enhancing coding efficiency, and its capabilities as a Python API for advanced text processing, opening new doors for empirical finance analysis and beyond.</p>
<section id="video-tutorial" class="level2">
<h2 class="anchored" data-anchor-id="video-tutorial">Video tutorial</h2>
<p>This post is also available as a video tutorial on YouTube.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/UmUDpxnmLW4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="ollama" class="level2">
<h2 class="anchored" data-anchor-id="ollama">Ollama</h2>
<p>The open-source community was set ablaze with excitement following Meta’s release of LLaMA, the first “big” open-source Large Language Model (LLM). This pivotal moment marked a shift towards democratizing access to powerful AI tools, sparking a fervent race to discover the most efficient ways to harness such models on consumer hardware. Amidst this bustling innovation, one solution emerged as a beacon for those seeking to leverage the immense potential of LLMs within their own computing environments: Ollama.</p>
<section id="what-is-ollama" class="level3">
<h3 class="anchored" data-anchor-id="what-is-ollama">What is Ollama?</h3>
<p>Ollama is a cutting-edge software designed to simplify the process of downloading, managing, and running open-source LLMs directly on your computer. Recognizing the complexities and technical challenges involved in setting up and deploying LLMs, Ollama offers a streamlined solution that makes these powerful tools more accessible to a wider audience. Whether you’re a researcher, developer, or enthusiast, Ollama provides the necessary “back-end” infrastructure to run these models smoothly on your local machine.</p>
</section>
<section id="the-power-of-local-processing" class="level3">
<h3 class="anchored" data-anchor-id="the-power-of-local-processing">The Power of Local Processing</h3>
<p>At its core, Ollama harnesses the power of local processing, offering users complete control over their data and the AI models they interact with. This approach not only enhances privacy and security but also allows for greater flexibility and customization to meet specific needs or research goals. With Ollama, the complexities of running LLMs are abstracted away, leaving users free to focus on their work without worrying about the underlying technicalities.</p>
</section>
<section id="limitations-of-local-llms" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-local-llms">Limitations of Local LLMs</h3>
<p>While the benefits of local LLMs are clear, it’s important to acknowledge that running these models on consumer hardware comes with its own set of limitations. The computational resources required to run LLMs are substantial, and while Ollama provides a robust solution for managing these resources, users should be mindful of the hardware and memory constraints of their local machines. For best results, it’s recommended to run Ollama on a machine with a powerful GPU and ample memory to ensure smooth and efficient operation. Apple’s M-series chips, because they share memory between the CPU and GPU, are particularly well-suited for running Ollama and LLMs.</p>
</section>
<section id="front-end-applications-for-enhanced-interaction" class="level3">
<h3 class="anchored" data-anchor-id="front-end-applications-for-enhanced-interaction">Front-End Applications for Enhanced Interaction</h3>
<p>While Ollama serves as the robust engine running the models, most users seek a more intuitive way to interact with their LLMs beyond the command line. To address this, Ollama supports the integration of various “front-end” applications, each offering a unique interface and set of features tailored to different user preferences and use cases. In this post, we will explore two such front-end applications that serve as excellent ChatGPT replacements:</p>
<ol type="1">
<li><strong>Ollama-UI</strong>: A Chrome extension that enables users to access and interact with their LLMs directly from their web browser, offering convenience and flexibility for those who prefer web-based tools. Easy to install but limited in features.</li>
<li><strong>Open WebUI</strong>: A comprehensive ChatGPT replacement that offers a full-featured web interface, catering to users looking for a robust and feature-rich platform to leverage the capabilities of their local LLMs.</li>
</ol>
<p>As we delve into the specifics of these front-end applications, it’s clear that Ollama is not just a tool but a gateway to unlocking the full potential of open-source LLMs. By bridging the gap between powerful AI models and everyday users, Ollama is setting the stage for a new era of innovation and accessibility in the world of artificial intelligence.</p>
</section>
<section id="other-use-cases" class="level3">
<h3 class="anchored" data-anchor-id="other-use-cases">Other Use Cases</h3>
<p>While this post focuses on using Ollama as a ChatGPT replacement, it’s important to note that Ollama’s capabilities extend far beyond text generation. In subsequent posts, we will explore two additional use cases for Ollama:</p>
<ul>
<li><strong>GitHub Copilot Replacement</strong>: Some models like CodeLlama and Mistral are designed to assist with code generation and programming tasks, making them ideal replacements for GitHub Copilot. Combined with Visual Studio Code extensions, Ollama offers a powerful alternative for developers seeking to enhance their coding efficiency and productivity.</li>
<li><strong>OpenAI API Replacement</strong>: Ollama can serve as a Python API for advanced text processing, enabling users to leverage the capabilities of open-source LLMs for a wide range of natural language processing tasks. From sentiment analysis to language translation, Ollama opens new doors for researchers and professionals seeking to harness the power of AI in their work. For researchers in empirical finance, this means being able to use open-source LLMs to analyze and interpret financial documents, news articles, and other textual data with greater control, replicability, and privacy.</li>
</ul>
</section>
</section>
<section id="setting-up-ollama" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-ollama">Setting Up Ollama</h2>
<p>Ollama is a command-line tool that is available for macOS, Linux, and (experimental) Windows, making it accessible to a wide range of users. In this section, we’ll walk through the process of downloading and installing Ollama, setting up the necessary LLM models, and running Ollama to interact with these models. We’ll also explore the front-end applications that can be used to enhance the user experience when working with Ollama.</p>
<section id="download-and-installation" class="level3">
<h3 class="anchored" data-anchor-id="download-and-installation">Download and Installation</h3>
<p>The first step in setting up Ollama is to download and install the tool on your local machine. The installation process is straightforward and involves running a few commands in your terminal. Ollama’s <a href="https://ollama.com/download">download page</a> provides installers for macOS and Windows, as well as instructions for Linux users. Once you’ve downloaded the installer, follow the installation instructions to set up Ollama on your machine.</p>
<p>If you’re using a Mac, you can install Ollama using Homebrew by running the following command in your terminal:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">brew</span> install ollama</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The benefit of using Homebrew is that it simplifies the installation process and also sets up Ollama as a service, allowing it to run in the background and manage the LLM models you download.</p>
</section>
<section id="downloading-and-running-llm-models" class="level3">
<h3 class="anchored" data-anchor-id="downloading-and-running-llm-models">Downloading and Running LLM Models</h3>
<p>The list of available LLM models that can be run using Ollama is constantly expanding, with new models being added regularly. To see the current list, you can check the <a href="https://ollama.com/library">Ollama Library</a> page. Once you’ve identified the models you’d like to use, you can download them using the <code>ollama pull</code> command followed by the model name. For example, to download the <code>llama2</code> model, you would run:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull llama2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In order to run the models, you will need to start the Ollama service. If you installed using Homebrew or activated the service during installation, you have nothing to do. If not, you can start the service using the <code>ollama serve</code> command. This will initialize the Ollama service and allow you to interact with the models you’ve downloaded.</p>
<p>After downloading the model, you can run it using the <code>ollama run</code> command followed by the model name. For example, to start the <code>llama2</code> model, you would run:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llama2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This will let you interact with the model directly from the command line, allowing you to generate text, code, or other outputs based on your input. However, for a more user-friendly experience, you will want to explore the front-end applications that Ollama supports instead of running it directly from the command line.</p>
<p>The most popular models available for use with Ollama are <code>llama2</code>, <code>mistral</code>, and <code>mixtral</code>. Each of these models offers unique capabilities and performance characteristics, catering to different use cases and hardware configurations, but according to the latest benchmark, <code>mixtral</code> is the most powerful and capable model, offering the best performance across a wide range of tasks.</p>
</section>
<section id="model-variants-and-sizes" class="level3">
<h3 class="anchored" data-anchor-id="model-variants-and-sizes">Model Variants and Sizes</h3>
<p>Models like Llama 2 and Mistral come in different sizes and variants, each offering a unique balance of computational power and specificity.</p>
<p>For example, the <code>llama2</code> model is available in multiple variants along three dimensions:</p>
<ul>
<li><strong>Number of parameters</strong>: <code>7b</code>, <code>13b</code>, and <code>70b</code>, each representing the number of parameters in the model in billions. The larger the model, the more powerful and capable it is, but it also requires more computational resources to run.</li>
<li><strong>Quantization</strong>: <code>q***</code>, where <code>***</code> represents the number of bits used to represent the model’s parameters and the quantization method used. LLM models weights are typically stored as 32-bit floating-point numbers, but quantization allows for the use of lower precision representations, such as 16-bit, 8-bit and as low as 2-bit. Lower quantization levels result in smaller model sizes and faster inference times, but may come at the cost of reduced model performance.</li>
<li><strong>Tuning</strong>: <code>text</code> or <code>chat</code>, indicating whether the model has been fine-tuned for text generation or chat.</li>
</ul>
<p>The <code>mistral</code> and <code>mixtral</code> models also come in different sizes and variants, each tailored to specific use cases and hardware configurations. By understanding the available model variants and sizes, you can choose the one that best suits your needs and computing environment, ensuring optimal performance and efficiency when running Ollama. This will require some experimentation to find the right balance between model size and performance for your specific use case and hardware configuration.</p>
<p>Other models like <code>CodeLlama</code> and <code>Mistral</code> are designed specifically for code generation and programming tasks, offering a powerful alternative to GitHub Copilot. These models are optimized for understanding and generating code, making them ideal for developers and researchers working on programming-related projects.</p>
</section>
<section id="models-i-use" class="level3">
<h3 class="anchored" data-anchor-id="models-i-use">Models I Use</h3>
<p>Here are the commands to install the models I use on my MacBook Pro M3 Max with 64GB of RAM:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull llama2</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull mistral</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull Mixtral</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull llama2-uncensored</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull CodeLlama</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull deepseek-coder</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Mixtral is the most powerful, but it requires a lot of memory and a powerful GPU to run. I use it for generating long-form content and for more complex tasks. Llama2 is a good all-rounder, and I use it for most of my text-generation tasks. Mistral is a smaller model that is well-suited for tasks that require less computational power, and CodeLlama is my go-to model for code generation and programming tasks.</p>
<p>These are also some of the most popular models available for use with Ollama, but I have yet to explore the other models available in the Ollama Library.</p>
</section>
</section>
<section id="setting-up-front-end-applications" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-front-end-applications">Setting Up Front-End Applications</h2>
<p>I use two front-end applications to interact with Ollama: Ollama-UI and Open WebUI. These applications provide a more user-friendly interface for interacting with the LLM models, offering a range of features and capabilities that enhance the overall user experience. In this section, we’ll explore the installation and setup process for each of these front-end applications, highlighting their unique features and use cases.</p>
<section id="ollama-ui" class="level3">
<h3 class="anchored" data-anchor-id="ollama-ui">Ollama-UI</h3>
<p>Ollama-UI is a Chrome extension that allows users to access and interact with their LLM models directly from their web browser. This convenient front-end application offers a simple and intuitive interface for generating text, code, and other outputs based on user input. To install Ollama-UI, you need to get it from the <a href="https://chromewebstore.google.com/detail/ollama-ui/cmgdpmlhgjhoadnonobjeekmfcehffco">Chrome Web Store</a>. Once installed, you can access it by clicking on the Ollama icon in your browser’s toolbar.</p>
<p>For it to work, you need to have the Ollama service running on your local machine. The interface is simple and easy to use, allowing you to input text and receive outputs from the LLM models you’ve downloaded.</p>
</section>
<section id="open-webui" class="level3">
<h3 class="anchored" data-anchor-id="open-webui">Open WebUI</h3>
<p>Open WebUI is a full-featured web interface that offers a comprehensive platform for interacting with your local LLM models. This powerful front-end application provides a range of features and capabilities, including the ability to manage and run multiple models simultaneously, customize model settings, and access advanced options for generating text, code, and other outputs. Installing it is a bit more involved than Ollama-UI, but it offers a more robust and feature-rich platform for leveraging the capabilities of your local LLM models.</p>
<p>To install Open WebUI, you will need to first install Docker if you don’t already have it on your machine. Docker is a platform running applications using containerization, i.e.&nbsp;in self-contained environments. You can download and install Docker from the <a href="https://www.docker.com/products/docker-desktop">official website</a>.</p>
<p>If you’re using a Mac, you can install Docker and Docker Desktop using Homebrew by running the following command in your terminal:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">brew</span> install docker</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">brew</span> install <span class="at">--cask</span> docker</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Once you have Docker installed, you can run the following command in your terminal to start Open WebUI:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> run <span class="at">-d</span> <span class="at">-p</span> 3000:8080 <span class="at">--add-host</span><span class="op">=</span>host.docker.internal:host-gateway <span class="at">-v</span> open-webui:/app/backend/data <span class="at">--name</span> open-webui <span class="at">--restart</span> always ghcr.io/open-webui/open-webui:main</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This command will start the Open WebUI service and allow you to access the web interface by navigating to <code>http://localhost:3000</code> in your web browser. From there, you can interact with your local LLM models, customize settings, and generate text, code, and other outputs based on your input.</p>
<p>Once you have Open WebUI up and running, you can explore its various features and capabilities. Note that the first time you start a new chat, Ollama will need to load the model, which can take a few seconds or longer depending on the model size and your hardware configuration. Once the model is loaded, you can start generating text and interacting with the model in real-time.</p>
<p>In the chat interface, you can select the model you want to use, input text, and receive outputs from the model. You can even select multiple models to run simultaneously, allowing you to compare the outputs and performance of different models. Aside from basic chat, here are the main features of Open WebUI that are available in the sidebar:</p>
<ul>
<li><strong>Modelfiles</strong>: These are WebUI’s equivalent of ChatGPT’s “GTPs”. They are pre-defined combinations of prompts and settings that can be used to generate specific types of outputs. For example, you might have a modelfile for generating code, another for summarizing text, and another for answering questions. You can start by looking at the modelfiles created by the community and available at <a href="https://openwebui.com/">https://openwebui.com</a>.</li>
<li><strong>Prompts</strong>: A place to save and manage prompt templates that you frequently use.</li>
<li><strong>Documents</strong>: A place to save and manage documents that you want your models to be able to refer to. Note that the documents are not accessible as a whole to the model, instead they are available through a RAG (retrieval-augmented generation) mechanism. In practice, this means that the model can search for information in the documents and get the most relevant “chunks” of information to generate a response, but it doesn’t have access to the full documents. It is thus useful for accessing reference material, but not for summarizing or generating text based on the full content of the documents.</li>
</ul>
<p>Open WebUI offers a range of advanced options and settings that allow you to customize the behavior of your local LLM models, making it a powerful platform for leveraging the capabilities of Ollama. Other features that go beyond the scope of this post include the ability to have multiple registered users, to generate images, and to access text-to-speech and speech-to-text capabilities.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The rise of open-source LLMs has ushered in a new era of innovation and accessibility in the world of artificial intelligence. There is so much potential for these models to transform the way we work and interact with AI, and Ollama is at the forefront of this revolution. By providing a streamlined solution for running LLMs on consumer hardware, Ollama is empowering researchers, developers, and enthusiasts to harness the full potential of these powerful models without relying on cloud-based solutions. It’s now up to us to explore the possibilities and push the boundaries of what’s possible with Ollama and open-source LLMs.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/vincent\.codes\.finance");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Vincent Grégoire</p>
</div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>