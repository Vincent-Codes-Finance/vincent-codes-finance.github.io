[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About VCF",
    "section": "",
    "text": "YouTube\n  \n  \n    \n     \n  \n  \n    \n     \n  \n  \n    \n     \n  \n  \n    \n     \n  \n\n  \n  \n\n\nVCF is a blog dedicated to coding in Python for empirical research in finance with a companion YouTube channel. My goal is to provide a platform for sharing knowledge and resources about coding in finance. The content is aimed at students, academics, and finance professionals who want to learn more about coding and empirical finance research.\nI use this platform to complement my teaching material and make it available to a wider audience. I also use it to share my experience with coding in finance and to document my journey as I learn new things."
  },
  {
    "objectID": "about.html#about-vincent-codes-finance",
    "href": "about.html#about-vincent-codes-finance",
    "title": "About VCF",
    "section": "",
    "text": "VCF is a blog dedicated to coding in Python for empirical research in finance with a companion YouTube channel. My goal is to provide a platform for sharing knowledge and resources about coding in finance. The content is aimed at students, academics, and finance professionals who want to learn more about coding and empirical finance research.\nI use this platform to complement my teaching material and make it available to a wider audience. I also use it to share my experience with coding in finance and to document my journey as I learn new things."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About VCF",
    "section": "‚úâÔ∏è Contact",
    "text": "‚úâÔ∏è Contact\nFor any comments and inquiries about this blog or the YouTube Channel, please contact me at:\nEMAIL: vincent@codes.finance"
  },
  {
    "objectID": "about.html#about-vincent",
    "href": "about.html#about-vincent",
    "title": "About VCF",
    "section": "üë®‚Äçüè´ About Vincent",
    "text": "üë®‚Äçüè´ About Vincent\n\n\nHello, my name is Vincent Gr√©goire. I‚Äôm an Associate Professor of Finance at HEC Montr√©al where I have been teaching Empirical Finance at the Master‚Äôs for a couple of years. My research interests include information economics and everything at the intersection of finance and technology, such as market microstructure, big data and machine learning applications in finance, fintech, and cybersecurity in finance. I‚Äôm also a Python enthusiast and curious about anything related to coding, data science, and machine learning.\n\n\n\n\n\n\n\n\nMy academic journey led me from undergraduate and master‚Äôs degrees in Computer Engineering to a Master‚Äôs in Financial Engineering at Universit√© Laval and a Ph.D.¬†in Finance from the University of British Columbia.\nSee my academic page for more details."
  },
  {
    "objectID": "about.html#how-to-support-me",
    "href": "about.html#how-to-support-me",
    "title": "About VCF",
    "section": "‚ù§Ô∏è How to support me",
    "text": "‚ù§Ô∏è How to support me\nIf you find the content on this website useful and would like to support me, there are a few ways you can do so:\n\nSpread the word about this website and the YouTube channel!\nSubscribe to the YouTube channel\nBuy me a coffee\n\nI am committed to transparency about any potential monetization of this website and the YouTube channel. I may use affiliate links related to products or services. If you click on an affiliate link and make a purchase, I may receive a commission at no extra cost to you. There will always be a disclosure when an affiliate link is used. I‚Äôm also trialing ads on the website, but I‚Äôm committed to keeping them as unobtrusive as possible."
  },
  {
    "objectID": "about.html#license-and-reuse",
    "href": "about.html#license-and-reuse",
    "title": "About VCF",
    "section": "üîì License and reuse",
    "text": "üîì License and reuse\nAll content on this website is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You are free to share and adapt the material for non-commercial purposes as long as you give appropriate credit and distribute your contributions under the same license.\nIf you want to reuse any of the content on this website for teaching purposes, please feel free to do so. I would appreciate it if you could include a link to this website in your course material and let me know by sending me an email at vincent@codes.finance. I will be happy to provide you with the source files for any of the content on this website upon request.\nIf for any reason you would like to reuse any of the content on this website for a purpose that is not covered by the above license, please contact me at vincent@codes.finance.\nThe code examples on this website are available on GitHub under the MIT License. You are free to use the code for any purpose, including commercial purposes, as long as you give appropriate credit and include a copy of the license in your distribution."
  },
  {
    "objectID": "about.html#how-is-this-blog-built",
    "href": "about.html#how-is-this-blog-built",
    "title": "About VCF",
    "section": "üõ†Ô∏è How is this blog built?",
    "text": "üõ†Ô∏è How is this blog built?\nThis website is built using Quarto, an open-source document system that allows you to write content in Markdown or Jupyter Notebooks and publish it in a variety of formats, including static HTML files. Quarto is on my list of topics to cover in the future, so stay tuned for content on that topic.\nThe blog is currently hosted using GitHub Pages."
  },
  {
    "objectID": "about.html#disclaimer",
    "href": "about.html#disclaimer",
    "title": "About VCF",
    "section": "üìù Disclaimer",
    "text": "üìù Disclaimer\nWhile I make every effort to ensure that the content on this website is accurate and up-to-date, I cannot guarantee that it is free from errors or omissions. The content is provided for informational purposes only and should not be construed as financial, legal, or professional advice. You should always consult with a qualified professional before making any financial or investment decisions."
  },
  {
    "objectID": "about.html#use-of-ai",
    "href": "about.html#use-of-ai",
    "title": "About VCF",
    "section": "ü§ñ Use of AI",
    "text": "ü§ñ Use of AI\nWhile I make use of AI-powered tools for editing and proofreading, I am committed to ensuring that all content is original and accurate. I will always cite my sources and give credit where credit is due."
  },
  {
    "objectID": "posts/vscode-latex/index.html",
    "href": "posts/vscode-latex/index.html",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "",
    "text": "LaTeX is a powerful typesetting system that is widely used for producing scientific publications and academic papers. Visual Studio Code is a versatile and extensible code editor that has gained popularity among developers and researchers. This extensibility made it possible to create a rich ecosystem of extensions that enhance the functionality of VS Code for various tasks, including writing and managing LaTeX documents. The LaTeX Workshop extension is one such tool that transforms VS Code into a feature-rich LaTeX editor, providing seamless compilation, real-time preview, and intelligent autocomplete, among other capabilities.\nIn this post, I will guide you through the process of setting up VS Code for LaTeX writing, including installing the necessary extensions, configuring the environment, and exploring additional tools to enhance your productivity. By the end of this guide, you will have a well-optimized LaTeX writing environment in VS Code, equipped with the tools and features to streamline your research and writing tasks."
  },
  {
    "objectID": "posts/vscode-latex/index.html#video-tutorial",
    "href": "posts/vscode-latex/index.html#video-tutorial",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Video tutorial",
    "text": "Video tutorial\nThis post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/vscode-latex/index.html#what-is-latex-and-how-to-install-it",
    "href": "posts/vscode-latex/index.html#what-is-latex-and-how-to-install-it",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "What is \\(\\LaTeX\\) and how to install it",
    "text": "What is \\(\\LaTeX\\) and how to install it\nLaTeX is more than just a typesetting system; it‚Äôs a powerful tool that has become indispensable for many people involved in academic writing. Its capability to handle complex mathematical formulas, precise formatting, and automatic generation of bibliographies and indexes makes it the go-to choice for producing scientific documents of high typographical quality. Unlike word processors that focus on the appearance of the document on the screen, LaTeX emphasizes structure and content, allowing writers to focus on their work without worrying about formatting details.\nIn this post, I assume that you have a basic understanding of LaTeX and its syntax. If you‚Äôre new to LaTeX, I recommend exploring an introductory guide to familiarize yourself with its fundamental concepts and syntax. You can also explore online LaTeX editors like Overleaf to experiment with LaTeX without installing it on your local machine. Overleaf also provides a tutorial on learning LaTeX in 30 minutes.\nI also assume that you have a LaTeX distribution installed on your system. Here are the most popular LaTeX distributions for different operating systems:\n\n macOS Windows Linux\n\n\nThe most popular LaTeX distribution for macOS is MacTeX, which includes all the major TeX-related programs and packages. You can download the MacTeX installer from the official website and follow the on-screen instructions to install it on your system.\nIf you‚Äôre like me and prefer to use homebrew to install software on your Mac, you can install MacTeX using the following command:\nbrew install --cask mactex\nHomebrew also provides a cask named mactex-no-gui that installs MacTeX without the extra graphical user interface applications and a cask named basictex that installs a minimal TeX Live distribution.\n\n\nFor Windows users, MiKTeX is a popular choice for installing LaTeX. MiKTeX is known for its user-friendly installation process and on-the-fly package installation feature. You can download the MiKTeX installer from the official website and follow the on-screen instructions to install it on your system.\n\n\nFor Linux users, TeX Live is a comprehensive TeX system that can be used on various Unix-based systems, including Linux. You can install TeX Live using your distribution‚Äôs package manager. For example, on Ubuntu, you can install TeX Live using the following command:"
  },
  {
    "objectID": "posts/vscode-latex/index.html#why-vs-code",
    "href": "posts/vscode-latex/index.html#why-vs-code",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Why VS Code?",
    "text": "Why VS Code?\nThere are plenty of LaTeX editors available,1 so why choose Visual Studio Code for writing LaTeX documents?\nVS Code is not just any code editor; it‚Äôs a feature-rich, extensible, and highly customizable environment that supports a wide array of programming languages and tools, making it an ideal choice for developers and researchers alike. At its core, LaTeX is a programming language for typesetting documents, so it makes sense to use a powerful code editor like VS Code to write and manage LaTeX documents. Here are the main reasons why I decided to use VS Code for LaTeX writing:\n\nUnified Workspace: VS Code provides a single environment for both coding and writing, eliminating the need to switch between different applications for different aspects of your project.\nExtension Ecosystem: The vast library of extensions available in VS Code can augment and tailor the editor to your specific needs, enhancing productivity and simplifying your writing workflow. This includes spell checkers, grammar checkers, and even AI-powered tools.\nBuilt-in Source Control: With Git integration out of the box, managing versions of your LaTeX documents becomes seamless, facilitating collaboration and historical tracking of your work.\nCustomization: VS Code‚Äôs extensive customization options allow you to create a writing environment that suits your preferences, from visual themes to keyboard shortcuts and snippets. And if you require them, VS Code includes many accessibility features, such as screen reader support and high contrast themes, making it an inclusive environment for users with diverse needs."
  },
  {
    "objectID": "posts/vscode-latex/index.html#getting-started-with-latex-workshop-for-vs-code",
    "href": "posts/vscode-latex/index.html#getting-started-with-latex-workshop-for-vs-code",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Getting Started with LaTeX Workshop for VS Code",
    "text": "Getting Started with LaTeX Workshop for VS Code\nThe LaTeX Workshop extension transforms VS Code into a powerful LaTeX editor, offering a comprehensive set of features tailored specifically for LaTeX users:\n\nSeamless Compilation: Automatically compile your LaTeX documents within VS Code, with customizable recipes to handle various compilation flows.\nReal-time Preview: View your compiled PDF documents directly in VS Code, providing immediate feedback on how your document looks.\nIntelligent Autocomplete: Get smart suggestions for LaTeX commands as you type, speeding up your writing process.\nError Logging and Navigation: Quickly identify and navigate to errors in your LaTeX source, simplifying the debugging process.\nCustomizable Build and Clean-Up: Configure build options and clean-up routines to match your project‚Äôs requirements, keeping your workspace organized.\n\nTo begin using LaTeX Workshop in VS Code, I recommend creating a new profile in VS Code for your LaTeX projects, which will allow you to customize the editor settings specifically for LaTeX writing. Then you can install the LaTeX Workshop extension and configure it to suit your workflow. In my case, I have not touched the default settings of LaTeX Workshop because I found them to be quite good for my needs. I also use the following extensions to enhance my LaTeX writing experience:\n\nGrammarly: The Grammarly extension brings the advanced grammar, spelling, and style checking capabilities of Grammarly to VS Code. I found it to be quite useful for working with LaTeX documents. There is a premium plan to get more advanced features, but the free version is already quite good.\nGitHub Copilot ($): GitHub‚Äôs ChatGPT-powered code completion tool that can provide suggestions for LaTeX commands and environments. It also works well with text in LaTeX documents, offering suggestions based on the context.\n\n\n\n\n\n\n\nTipGrammarly and Copilot settings\n\n\n\n\n\nIf you decide to use Grammarly and Copilot, I recommend configuring them to work with LaTeX documents. For Grammarly, you can enable it for LaTeX files by adding the following to your settings.json file:2\n    \"grammarly.files.include\": [\n      \"**/*.tex\",\n    ],\n    \"github.copilot.enable\": {\n      \"latex\": true,\n    }\n\n\n\nSome other extensions that you might find useful include:\n\nSpell Right: A lightweight, offline spell checker that supports multiple languages and can be used within LaTeX documents in VS Code. It uses the OS spellchecker on macOS and Windows.\nAntidote ($): Antidote is a comprehensive writing assistance tool offering corrections for spelling, grammar, and typography, along with rich dictionaries and language guides for French and English. The Antidote extension integrates these features into VS Code, making it accessible for LaTeX projects. Note that Antidote is aware of the LaTeX syntax.\nLTeX ($): LaTeX-aware extension for access to LanguageTool from within VS Code. LanguageTool is a style and grammar checker that can be used to improve your writing style and grammar.\n\nIn the following sections, I will guide you on using LaTeX Workshop and the other extensions to use your LaTeX writing environment in VS Code, making the most of the tools and features to streamline your research and writing tasks."
  },
  {
    "objectID": "posts/vscode-latex/index.html#compiling-latex-documents-in-vs-code",
    "href": "posts/vscode-latex/index.html#compiling-latex-documents-in-vs-code",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Compiling LaTeX Documents in VS Code",
    "text": "Compiling LaTeX Documents in VS Code\nBy default, LaTeX Workshop will automatically compile your LaTeX documents when you save them. This means that you can focus on writing your document and let LaTeX Workshop take care of the compilation process in the background. The extension provides a real-time preview of your compiled PDF document, allowing you to see the changes you make to your LaTeX source code immediately. You can change this behavior and the recipe used to compile your document by modifying the settings of LaTeX Workshop.\nAdditional options are available in the left sidebar of VS Code, under the \\(\\tex\\) icon. There you can find different build commands, additional options for viewing the PDF, and view the compilation logs. You can also navigate the document structure and access the quick symbol view.\n\n\n\nSidebar"
  },
  {
    "objectID": "posts/vscode-latex/index.html#multi-file-projects",
    "href": "posts/vscode-latex/index.html#multi-file-projects",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Multi-file projects",
    "text": "Multi-file projects\nLaTeX Workshop supports multi-file projects, which are common in LaTeX documents. This means that you can split your document into multiple files and include them in a main file using the \\input{} or \\include{} commands. LaTeX Workshop will automatically detect the structure of your project and compile the main file, including all the subfiles, when you save it. This feature is particularly useful for organizing large documents and collaborating with others, as it allows you to work on different parts of the document independently.\nBy default, if the current document does contain a \\documentclass[...]{...} or \\begin{document} tag, LaTeX Workshop will identify the root file of your project by looking for a file with the .tex extension in the root directory of your workspace that contains such a tag. You can also specify explicitly the root file of your project by adding a comment at the beginning of your LaTeX source code: % !TEX root = relative/or/absolute/path/to/root/file.tex. See the LaTeX Workshop documentation for more details."
  },
  {
    "objectID": "posts/vscode-latex/index.html#syncing-source-and-preview",
    "href": "posts/vscode-latex/index.html#syncing-source-and-preview",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Syncing source and preview",
    "text": "Syncing source and preview\nLaTeX Workshop provides a feature called ‚ÄúSyncTeX‚Äù that allows you to synchronize the source and preview of your LaTeX document. This means that when you click on a section of your PDF, the corresponding part of your LaTeX source code is highlighted, and vice versa. This feature is particularly useful for navigating large documents and reviewing changes.\nSyncTeX is enabled by default. It supports both forward and reverse search, meaning you can jump from your source to the PDF and back:\n\nForward Search (source to pdf): Place the cursor where you want in the source code then type cmd+option+j on Mac or ctrl+alt+j on Windows/Linux, and the PDF viewer will jump to the corresponding location.\nReverse Search (pdf to source): Click on a location in the PDF while holding cmd on Mac or ctrl on Windows/Linux, and the source code will jump to the corresponding location.\n\nSyncTeX also allows you to do a forward search on references. For that, just over on the reference and a View on PDF button will appear.\nIf you only want to see a preview of an equation or bib entry, all you have to do is place the cursor over the equation or reference to trigger a preview.\n\n\n\nFormula preview"
  },
  {
    "objectID": "posts/vscode-latex/index.html#formatting",
    "href": "posts/vscode-latex/index.html#formatting",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Formatting",
    "text": "Formatting\nI always found LaTeX to be a bit of a pain to format, and since I‚Äôm too lazy to do it myself, my documents often look like a mess. My life changed when I discovered the concept of a code formatter, which automatically formats your code according to a set of rules. LaTeX Workshop supports formatting using the latexindent tool, which can automatically format your LaTeX source code according to your preferred style. To enable this feature, you need to install latexindent on your system. It is a Perl script that is also available as a standalone executable for macOS, Windows, and Linux. Once you install it, make sure it is accessible from your PATH.\nOn macOS, you can install it using Homebrew with the following commands:\nbrew install perl\nbrew install cpanm\n\ncpanm YAML::Tiny\ncpanm File::HomeDir\nI recommend configuring VS Code to automatically format your LaTeX documents whenever saving a file. To do this, open your settings, search for Format On Save, and enable it. This will ensure that your LaTeX documents are automatically formatted according to your preferred style whenever you save them.\n\n\n\n\n\n\nTipAn essential when using source control\n\n\n\nFormatting your LaTeX documents is especially important when using version control. By having a consistent style, you can avoid unnecessary merge conflicts and make it easier to review changes in your documents."
  },
  {
    "objectID": "posts/vscode-latex/index.html#working-with-bibliographies-in-latex-and-vs-code",
    "href": "posts/vscode-latex/index.html#working-with-bibliographies-in-latex-and-vs-code",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Working with Bibliographies in LaTeX and VS Code",
    "text": "Working with Bibliographies in LaTeX and VS Code\nBibTeX is a widely used bibliography management tool for LaTeX documents. It allows you to create a bibliography database (.bib file) containing references and citations, which can be cited in your LaTeX documents using the \\cite{} command. BibTeX is a powerful tool for managing references, but it requires careful management of the .bib file and can be cumbersome to maintain manually.\nBibTex is supported out of the box by LaTeX Workshop, and it can be used to compile your bibliography and generate a list of references in your document. It will automatically detect your bibliography from the root file of your project and use it to suggest references as you type \\cite{} commands in your LaTeX source code.\n\n\n\nA formatted Bibtex file\n\n\nLaTex Workshop also provides a formatter for BibTeX files, which not only formats your .bib file but can also sort the entries alphabetically. To enable automatic formatting of your .bib files, including automatic sorting of entries, add the following to your settings.json file:3\n    \"latex-workshop.bibtex-fields.sort.enabled\": true,\n    \"latex-workshop.bibtex-format.sort.enabled\": true,\n    \"[bibtex]\": {\n      \"editor.defaultFormatter\": \"James-Yu.latex-workshop\"\n    }\nIf you use the Zotero reference manager, you can use the Zotero LaTex extension to integrate Zotero with VS Code. This extension allows you to search and insert references from your Zotero library directly into your LaTeX documents, streamlining the citation process."
  },
  {
    "objectID": "posts/vscode-latex/index.html#customizing-the-writing-environment-in-vs-code-for-latex",
    "href": "posts/vscode-latex/index.html#customizing-the-writing-environment-in-vs-code-for-latex",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Customizing the Writing Environment in VS Code for LaTeX",
    "text": "Customizing the Writing Environment in VS Code for LaTeX\nCrafting a comfortable and efficient writing environment is crucial for academic writers, especially when working on complex LaTeX documents. Visual Studio Code (VS Code) offers a plethora of customization options to tailor the editor to your specific needs, enhancing productivity and making the writing process more enjoyable. This section explores various ways to customize your VS Code environment for LaTeX writing, from visual tweaks to functional enhancements.\n\nTips when using VS Code for LaTeX Writing\n\nUse code snippets: LaTeX Workshop provides built-in snippets for common LaTeX commands and environments. For example, typing BEQ and pressing Tab will expand to \\begin{equation} ... \\end{equation}. See the full list of snippets in the LaTeX Workshop documentation or create your own custom snippets to speed up your writing process.\nCustomize the build process: As discussed in previous sections, customizing the build process for your LaTeX projects can streamline your workflow. Use tasks in VS Code to define custom build commands, specifying your preferred LaTeX distribution and any additional steps required for your document, such as glossary or index processing.\nMinimize Distractions: Utilize Zen Mode (View: Toggle Zen Mode) to focus on your writing by hiding the VS Code interface, leaving only the text editor visible."
  },
  {
    "objectID": "posts/vscode-latex/index.html#debugging-latex-documents-in-vs-code",
    "href": "posts/vscode-latex/index.html#debugging-latex-documents-in-vs-code",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Debugging LaTeX Documents in VS Code",
    "text": "Debugging LaTeX Documents in VS Code\nWriting in LaTeX can sometimes feel like a smooth sail‚Äîuntil you encounter an error that halts your compilation or messes up your document formatting. Debugging LaTeX documents can be a frustrating task, especially when dealing with complex structures and multiple files. However, Visual Studio Code (VS Code), coupled with the LaTeX Workshop extension, provides simple yet powerful tools and features to simplify the debugging process. Here are the places you can look for information when debugging LaTeX documents in VS Code:\n\nError Messages: LaTeX error messages often include the line number and a description of the issue, which can be cryptic at first glance. The first step in debugging is to carefully read these messages and identify the problematic code snippet. Fortunately, VS Code‚Äôs LaTeX Workshop extension highlights errors directly in the editor, making it easier to locate and address them.\n\n\n\n\nAn underlined error\n\n\n\nLog Files: When LaTeX compiles a document, it generates a log file (.log) that contains detailed information about the compilation process, including warnings and errors. Reviewing the log file can provide insights into what went wrong and why. You can access the log file through the LaTeX Workshop extension‚Äôs ‚ÄúView LaTeX Workshop log‚Äù command, or by looking at the output of the LaTeX compiler in the output panel.\nVersion Control: If you use Git to track changes in your work you can use the version control features of VS Code to review the history of your document and identify when an error was introduced. This can be especially helpful in pinpointing the source of an error and reverting to earlier versions of your document if necessary.\nGitHub Copilot: GitHub Copilot can provide suggestions for LaTeX commands and environments, which can be helpful when you‚Äôre unsure about the correct syntax or usage of a particular command. You can ask Copilot for help fixing an error by typing a description of the problem. It will then provide suggestions based on the context of your document.\n\n\n\n\nFixing with Copilot"
  },
  {
    "objectID": "posts/vscode-latex/index.html#conclusion-and-further-resources",
    "href": "posts/vscode-latex/index.html#conclusion-and-further-resources",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Conclusion and Further Resources",
    "text": "Conclusion and Further Resources\nIn this guide, we‚Äôve covered the basics of setting up and optimizing Visual Studio Code (VS Code) for LaTeX projects. We‚Äôve discussed installation, configuration, debugging, and version control best practices. Additionally, we‚Äôve highlighted some useful extensions that can improve your writing workflow in VS Code.\nI strongly recommend that you use a version control system for your LaTeX projects. This will allow you to track changes, collaborate with others, and maintain a history of your work. If you are new to version control, I recommend reading my post on Using GitHub for academic research.\nIf you want to further customize your LaTeX writing environment, you can! Have a look at the LaTeX Workshop extension documentation to see all the features it offers."
  },
  {
    "objectID": "posts/vscode-latex/index.html#footnotes",
    "href": "posts/vscode-latex/index.html#footnotes",
    "title": "Writing LaTeX Documents in VS Code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI used TeXstudio for over a decade before switching to VS Code, and before that I used Texmaker, TeXnicCenter, and TeXShop for many years each.‚Ü©Ô∏é\nYou can access your settings.json file by opening the Command Palette (Cmd+Shift+P on Mac, Ctrl+Shift+P on Windows/Linux) and searching for Preferences: Open User Settings (JSON).‚Ü©Ô∏é\nYou can access your settings.json file by opening the Command Palette (Cmd+Shift+P on Mac, Ctrl+Shift+P on Windows/Linux) and searching for Preferences: Open User Settings (JSON).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/edgartools/index.html",
    "href": "posts/edgartools/index.html",
    "title": "Scrape financial data from SEC Edgar with Python and edgartools",
    "section": "",
    "text": "In the field of finance, the application of machine learning, especially text analysis, has become increasingly popular. Researchers are constantly seeking out rich datasets to mine for insights, with company filings offering a treasure trove of information. One of the most comprehensive sources of such data is the Edgar database, maintained by the Securities and Exchange Commission (SEC). This publicly available resource contains a vast array of filings from companies, making it an invaluable tool for empirical finance research.\nIn this tutorial, I will show you how to scrape financial data from the SEC Edgar database using Python and the edgartools library."
  },
  {
    "objectID": "posts/edgartools/index.html#video-tutorial",
    "href": "posts/edgartools/index.html#video-tutorial",
    "title": "Scrape financial data from SEC Edgar with Python and edgartools",
    "section": "Video tutorial",
    "text": "Video tutorial\nThis post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/edgartools/index.html#sec-edgar-api",
    "href": "posts/edgartools/index.html#sec-edgar-api",
    "title": "Scrape financial data from SEC Edgar with Python and edgartools",
    "section": "SEC Edgar API",
    "text": "SEC Edgar API\nWhile the Edgar database is freely accessible to anyone interested in browsing company filings, the manual retrieval of data can be time-consuming and inefficient. For researchers and analysts looking to conduct large-scale studies or apply complex algorithms, a more programmatic approach is necessary. This is where the SEC‚Äôs APIs come into play, allowing automated access to Edgar‚Äôs wealth of data.\nHowever, even with API access, the task of scraping, parsing, and processing the data remains a challenge. The raw data requires significant cleaning and manipulation before it can be effectively used in research. This is a hurdle that has long stood in the way of efficient data utilization from Edgar. This is where the edgartools Python library comes in."
  },
  {
    "objectID": "posts/edgartools/index.html#getting-started-with-edgartools",
    "href": "posts/edgartools/index.html#getting-started-with-edgartools",
    "title": "Scrape financial data from SEC Edgar with Python and edgartools",
    "section": "Getting started with edgartools",
    "text": "Getting started with edgartools\n\nInstallation\nedgartools is a Python library designed to simplify the process of accessing and working with data from the SEC‚Äôs Edgar database. Installing edgartools is straightforward and can be done using either pip or poetry. Open your terminal or command prompt and enter one of the following commands:\npip install edgartools\nor, if you‚Äôre using poetry:\npoetry add edgartools\nedgartools leverages several powerful libraries, including pyarrow and pandas, to manage data efficiently. It also means that the data you retrieve is easily available as pandas dataframes, which makes it simple to work with and integrate into your existing data analysis workflows. These dependencies are automatically installed with edgartools, so you don‚Äôt need to worry about setting them up separately.\n\n\nBasic usage\nOnce edgartools is installed, you can begin by importing the necessary components from the library. The first step in interacting with the Edgar API through edgartools is to set your identity. The SEC requires all API users to provide a name and email address. This requirement is easily met with edgartools as shown below:\n\nfrom edgar import set_identity\n\nset_identity(\"Vincent Gregoire vincent@codes.finance\")\n\n[11:07:42] INFO     Identity of the Edgar REST client set to [Vincent Gregoire vincent@codes.finance]   core.py:154\n\n\n\nWith your identity set, you‚Äôre now ready to start querying the Edgar database programmatically. Whether you‚Äôre interested in specific company filings, financial summaries, or historical data, edgartools provides a streamlined, efficient pathway to obtaining this information.\nIn the following sections, we‚Äôll dive deeper into how to retrieve company information, access filings, and work with the data to extract valuable insights for your research."
  },
  {
    "objectID": "posts/edgartools/index.html#retrieving-company-filings",
    "href": "posts/edgartools/index.html#retrieving-company-filings",
    "title": "Scrape financial data from SEC Edgar with Python and edgartools",
    "section": "Retrieving company filings",
    "text": "Retrieving company filings\n\nQuerying Company Data\nTo begin working with a specific company, you need to define a Company object in edgartools. This object serves as your gateway to a wealth of information about the company, including its filings, financial summaries, and historical data. You can define a company object using either the company‚Äôs ticker symbol or its Central Index Key (CIK) code.\n\nfrom edgar import Company\n\napple = Company(\"AAPL\")\napple\n\n\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Apple Inc. (AAPL) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ   CIK      Category                  Industry               Incorporated                                        ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                       ‚îÇ\n‚îÇ   320193   Large accelerated filer   Electronic Computers   CA                                                  ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚úâ Mailing Address ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ                 ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üè¢ Business Address ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ               ‚îÇ\n‚îÇ ‚îÇ ONE APPLE PARK WAY                   ‚îÇ                 ‚îÇ ONE APPLE PARK WAY                   ‚îÇ               ‚îÇ\n‚îÇ ‚îÇ CUPERTINO, CA 95014                  ‚îÇ                 ‚îÇ CUPERTINO, CA 95014                  ‚îÇ               ‚îÇ\n‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                 ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ               ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nOnce you have a company object, you can easily retrieve basic company information, financial summaries, former names, and other metadata. Here are some examples:\n\napple.financials\n\n\n\n\n\n                                                                                                                   \n                           Balance Sheet                                                                           \n                                                                                                                   \n                                                      2023-09-30                                                   \n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ                                                  \n    ASSETS:                                                                                                        \n    CURRENT ASSETS:                                                                                                \n      Cash and Cash Equivalents                   29,965,000,000                                                   \n      Other Current Assets                        14,695,000,000                                                   \n      Current Assets                             143,566,000,000                                                   \n                                                                                                                   \n    NONCURRENT ASSETS:                                                                                             \n      Marketable Securities                      100,544,000,000                                                   \n      Property, Plant and Equipment               43,715,000,000                                                   \n      Other Noncurrent Assets                     64,758,000,000                                                   \n      Total Noncurrent Assets                    209,017,000,000                                                   \n                                                                                                                   \n    Total Assets                                 352,583,000,000                                                   \n                                                                                                                   \n    LIABILITIES AND STOCKHOLDERS' EQUITY:                                                                          \n    CURRENT LIABILITIES:                                                                                           \n      Accounts Payable                            62,611,000,000                                                   \n      Other Current Liabilities                   58,829,000,000                                                   \n      Deferred Revenue                             8,061,000,000                                                   \n      Commercial Paper                             5,985,000,000                                                   \n      Term Debt                                    9,822,000,000                                                   \n      Total Current Liabilities                  145,308,000,000                                                   \n                                                                                                                   \n    NONCURRENT LIABILITIES:                                                                                        \n      Non-current Long Term Debt                  95,281,000,000                                                   \n      Other Noncurrent Liabilities                49,848,000,000                                                   \n      Total Noncurrent Liabilities               145,129,000,000                                                   \n                                                                                                                   \n    Total Liabilities                            290,437,000,000                                                   \n                                                                                                                   \n    STOCKHOLDERS' EQUITY:                                                                                          \n      Common Stock and paid-in Capital            73,812,000,000                                                   \n      Retained Earnings                             -214,000,000                                                   \n      Accumulated Other Comprehensive Income     -11,452,000,000                                                   \n      Total Stockholders' Equity                  62,146,000,000                                                   \n                                                                                                                   \n    Total Liabilities and Stockholders' Equity   352,583,000,000                                                   \n                                                                                                                   \n                               Cashflow Statement                                                                  \n                                                                                                                   \n                                                                  2023-09-30                                       \n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ                                      \n    OPERATING ACTIVITIES:                                                                                          \n      Net Income                                              96,995,000,000                                       \n      Depreciation and Amortization                           11,519,000,000                                       \n      Stock-based Compensation                                10,833,000,000                                       \n      Other Noncash Income/Expense                             2,227,000,000                                       \n      Changes in Accounts Payable                             -1,889,000,000                                       \n      Changes in Inventories                                   1,618,000,000                                       \n      Net Cash Provided by Operating Activities              110,543,000,000                                       \n                                                                                                                   \n    INVESTING ACTIVITIES:                                                                                          \n      Purchases of Marketable Securities                      29,513,000,000                                       \n      Proceeds from Maturities of Marketable Securities       39,686,000,000                                       \n      Proceeds from Sale of Marketable Securities              5,828,000,000                                       \n      Purchases of Property, Plant and Equipment              10,959,000,000                                       \n      Other Investing Activities                               1,337,000,000                                       \n      Net Cash Provided by Investing Activities                3,705,000,000                                       \n                                                                                                                   \n    FINANCING ACTIVITIES:                                                                                          \n      Payments of Tax for Share-based Compensation             5,431,000,000                                       \n      Dividends Paid                                          15,025,000,000                                       \n      Repurchases of Common Stock                             77,550,000,000                                       \n      Repayments of Long-term Debt                            11,151,000,000                                       \n      Net Cash Provided by Financing Activities             -108,488,000,000                                       \n                                                                                                                   \n    Changes in Cash, cash equivalents and restricted cash      5,760,000,000                                       \n    Cash, cash equivalents and restricted cash                30,737,000,000                                       \n                                                                                                                   \n                 Consolidated Statement of Operations                                                              \n                                                                                                                   \n                                                         2023-09-30                                                \n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ                                               \n    Total Net Sales                                 383,285,000,000                                                \n                                                                                                                   \n    COST OF SALES:                                                                                                 \n    Cost Goods and Services Sold                    214,137,000,000                                                \n                                                                                                                   \n    Gross Profit                                    169,148,000,000                                                \n                                                                                                                   \n    OPERATING EXPENSES:                                                                                            \n      Research and Development Expenses              29,915,000,000                                                \n      Selling General and Administrative Expenses    24,932,000,000                                                \n    Total Operating Expenses                         54,847,000,000                                                \n                                                                                                                   \n    Operating Income                                114,301,000,000                                                \n                                                                                                                   \n    OTHER INCOME/EXPENSE:                                                                                          \n      Interest Expense                                3,933,000,000                                                \n      Nonoperating Income                              -565,000,000                                                \n    Income Before Taxes                             113,736,000,000                                                \n                                                                                                                   \n    Income Tax Expense                               16,741,000,000                                                \n    Net Income                                       96,995,000,000                                                \n                                                                                                                   \n    EARNINGS PER SHARE:                                                                                            \n      Basic                                                    6.16                                                \n      Diluted                                                  6.13                                                \n    WEIGHTED AVERAGE SHARES OUTSTANDING:                                                                           \n      Basic                                          15,744,231,000                                                \n      Diluted                                        15,812,547,000                                                \n                                                                                                                   \n                                                                                                                   \n\n\n\n\napple.former_names\n\n[{'name': 'APPLE INC',\n  'from': '2007-01-10T00:00:00.000Z',\n  'to': '2019-08-05T00:00:00.000Z'},\n {'name': 'APPLE COMPUTER INC',\n  'from': '1994-01-26T00:00:00.000Z',\n  'to': '2007-01-04T00:00:00.000Z'},\n {'name': 'APPLE COMPUTER INC/ FA',\n  'from': '1997-07-28T00:00:00.000Z',\n  'to': '1997-07-28T00:00:00.000Z'}]\n\n\n\n\nCompany Filings\nYou can also use this Company object to access the company‚Äôs filings. The get_filings method allows you to retrieve a list of filings for the company, filtered by form type, date range, and other criteria. Here‚Äôs an example of how to retrieve the 10-Q filings (quarterly earnings) for a company:\n\nfilings = apple.get_filings(form=\"10-Q\")\nfilings\n\n\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Filings for Apple Inc. [320193] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ        form   filed        accession_number       xbrl                                                          ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                         ‚îÇ\n‚îÇ   0    10-Q   2024-02-02   0000320193-24-000006   ‚úì                                                             ‚îÇ\n‚îÇ   1    10-Q   2023-08-04   0000320193-23-000077   ‚úì                                                             ‚îÇ\n‚îÇ   2    10-Q   2023-05-05   0000320193-23-000064   ‚úì                                                             ‚îÇ\n‚îÇ   3    10-Q   2023-02-03   0000320193-23-000006   ‚úì                                                             ‚îÇ\n‚îÇ   4    10-Q   2022-07-29   0000320193-22-000070   ‚úì                                                             ‚îÇ\n‚îÇ   5    10-Q   2022-04-29   0000320193-22-000059   ‚úì                                                             ‚îÇ\n‚îÇ   6    10-Q   2022-01-28   0000320193-22-000007   ‚úì                                                             ‚îÇ\n‚îÇ   7    10-Q   2021-07-28   0000320193-21-000065   ‚úì                                                             ‚îÇ\n‚îÇ   8    10-Q   2021-04-29   0000320193-21-000056   ‚úì                                                             ‚îÇ\n‚îÇ   9    10-Q   2021-01-28   0000320193-21-000010   ‚úì                                                             ‚îÇ\n‚îÇ   10   10-Q   2020-07-31   0000320193-20-000062   ‚úì                                                             ‚îÇ\n‚îÇ   11   10-Q   2020-05-01   0000320193-20-000052   ‚úì                                                             ‚îÇ\n‚îÇ   12   10-Q   2020-01-29   0000320193-20-000010   ‚úì                                                             ‚îÇ\n‚îÇ   13   10-Q   2019-07-31   0000320193-19-000076   ‚úì                                                             ‚îÇ\n‚îÇ   14   10-Q   2019-05-01   0000320193-19-000066   ‚úì                                                             ‚îÇ\n‚îÇ   15   10-Q   2019-01-30   0000320193-19-000010   ‚úì                                                             ‚îÇ\n‚îÇ   16   10-Q   2018-08-01   0000320193-18-000100   ‚úì                                                             ‚îÇ\n‚îÇ   17   10-Q   2018-05-02   0000320193-18-000070   ‚úì                                                             ‚îÇ\n‚îÇ   18   10-Q   2018-02-02   0000320193-18-000007   ‚úì                                                             ‚îÇ\n‚îÇ   19   10-Q   2017-08-02   0000320193-17-000009   ‚úì                                                             ‚îÇ\n‚îÇ   20   10-Q   2017-05-03   0001628280-17-004790   ‚úì                                                             ‚îÇ\n‚îÇ   21   10-Q   2017-02-01   0001628280-17-000717   ‚úì                                                             ‚îÇ\n‚îÇ   22   10-Q   2016-07-27   0001628280-16-017809   ‚úì                                                             ‚îÇ\n‚îÇ   23   10-Q   2016-04-27   0001193125-16-559625   ‚úì                                                             ‚îÇ\n‚îÇ   24   10-Q   2016-01-27   0001193125-16-439878   ‚úì                                                             ‚îÇ\n‚îÇ   25   10-Q   2015-07-22   0001193125-15-259935   ‚úì                                                             ‚îÇ\n‚îÇ   26   10-Q   2015-04-28   0001193125-15-153166   ‚úì                                                             ‚îÇ\n‚îÇ   27   10-Q   2015-01-28   0001193125-15-023697   ‚úì                                                             ‚îÇ\n‚îÇ   28   10-Q   2014-07-23   0001193125-14-277160   ‚úì                                                             ‚îÇ\n‚îÇ   29   10-Q   2014-04-24   0001193125-14-157311   ‚úì                                                             ‚îÇ\n‚îÇ   30   10-Q   2014-01-28   0001193125-14-024487   ‚úì                                                             ‚îÇ\n‚îÇ   31   10-Q   2013-07-24   0001193125-13-300670   ‚úì                                                             ‚îÇ\n‚îÇ   32   10-Q   2013-04-24   0001193125-13-168288   ‚úì                                                             ‚îÇ\n‚îÇ   33   10-Q   2013-01-24   0001193125-13-022339   ‚úì                                                             ‚îÇ\n‚îÇ   34   10-Q   2012-07-25   0001193125-12-314552   ‚úì                                                             ‚îÇ\n‚îÇ   35   10-Q   2012-04-25   0001193125-12-182321   ‚úì                                                             ‚îÇ\n‚îÇ   36   10-Q   2012-01-25   0001193125-12-023398   ‚úì                                                             ‚îÇ\n‚îÇ   37   10-Q   2011-07-20   0001193125-11-192493   ‚úì                                                             ‚îÇ\n‚îÇ   38   10-Q   2011-04-21   0001193125-11-104388   ‚úì                                                             ‚îÇ\n‚îÇ   39   10-Q   2011-01-19   0001193125-11-010144   ‚úì                                                             ‚îÇ\n‚îÇ   40   10-Q   2010-07-21   0001193125-10-162840   ‚úì                                                             ‚îÇ\n‚îÇ   41   10-Q   2010-04-21   0001193125-10-088957   ‚úì                                                             ‚îÇ\n‚îÇ   42   10-Q   2010-01-25   0001193125-10-012085   ‚úì                                                             ‚îÇ\n‚îÇ   43   10-Q   2009-07-22   0001193125-09-153165   ‚úì                                                             ‚îÇ\n‚îÇ   44   10-Q   2009-04-23   0001193125-09-085781                                                                 ‚îÇ\n‚îÇ   45   10-Q   2009-01-23   0001193125-09-009937                                                                 ‚îÇ\n‚îÇ   46   10-Q   2008-07-23   0001193125-08-156421                                                                 ‚îÇ\n‚îÇ   47   10-Q   2008-05-01   0001193125-08-097759                                                                 ‚îÇ\n‚îÇ   48   10-Q   2008-02-01   0001193125-08-017426                                                                 ‚îÇ\n‚îÇ   49   10-Q   2007-08-08   0001104659-07-059873                                                                 ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ Showing 50 filings of 90 total                                                                                  ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nAfter retrieving a list of filings, you might want to analyze the metadata or convert it into a format that‚Äôs easier to work with, such as a pandas DataFrame:\n\nfilings.to_pandas()\n\n\n\n\n\n\n\n\naccession_number\nfiling_date\nreportDate\nacceptanceDateTime\nact\nform\nfileNumber\nitems\nsize\nisXBRL\nisInlineXBRL\nprimaryDocument\nprimaryDocDescription\n\n\n\n\n0\n0000320193-24-000006\n2024-02-02\n2023-12-30\n2024-02-01T18:03:38.000Z\n34\n10-Q\n001-36743\n\n4984121\n1\n1\naapl-20231230.htm\n10-Q\n\n\n1\n0000320193-23-000077\n2023-08-04\n2023-07-01\n2023-08-03T18:04:43.000Z\n34\n10-Q\n001-36743\n\n5939898\n1\n1\naapl-20230701.htm\n10-Q\n\n\n2\n0000320193-23-000064\n2023-05-05\n2023-04-01\n2023-05-04T18:03:52.000Z\n34\n10-Q\n001-36743\n\n6314786\n1\n1\naapl-20230401.htm\n10-Q\n\n\n3\n0000320193-23-000006\n2023-02-03\n2022-12-31\n2023-02-02T18:01:30.000Z\n34\n10-Q\n001-36743\n\n5915088\n1\n1\naapl-20221231.htm\n10-Q\n\n\n4\n0000320193-22-000070\n2022-07-29\n2022-06-25\n2022-07-28T18:06:56.000Z\n34\n10-Q\n001-36743\n\n6109993\n1\n1\naapl-20220625.htm\n10-Q\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n85\n0000320193-95-000011\n1995-08-11\n1995-06-30\n1995-08-11T00:00:00.000Z\n\n10-Q\n000-10030\n\n213500\n0\n0\n\n\n\n\n86\n0000320193-95-000008\n1995-05-15\n1995-03-31\n1995-05-15T00:00:00.000Z\n\n10-Q\n000-10030\n\n111162\n0\n0\n\n\n\n\n87\n0000320193-95-000003\n1995-02-09\n1994-12-30\n1995-02-09T00:00:00.000Z\n\n10-Q\n000-10030\n\n132577\n0\n0\n\n\n\n\n88\n0000320193-94-000013\n1994-08-12\n1994-07-01\n1994-08-12T00:00:00.000Z\n\n10-Q\n000-10030\n\n138354\n0\n0\n\n\n\n\n89\n0000320193-94-000002\n1994-01-26\n1993-12-31\n1994-01-26T00:00:00.000Z\n\n10-Q\n000-10030\n\n233311\n0\n0\n\n\n\n\n\n\n90 rows √ó 13 columns\n\n\n\nThis DataFrame contains metadata about each filing, which can be invaluable for preliminary analyses or for identifying filings of interest.\n\n\nAccessing Filing Data\nBeyond metadata, edgartools facilitates downloading and accessing the content of filings. Let‚Äôs work with the latest filing from the list we retrieved earlier.\n\nlatest_10q = filings.latest()\n\n\nlatest_10q\n\n\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 10-Q üìä filing for Apple Inc. ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ   Accession Number       Filing Date   Company      CIK                                                         ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                     ‚îÇ\n‚îÇ   0000320193-24-000006   2024-02-02    Apple Inc.   320193                                                      ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ   Links: üè† Homepage üìÑ Primary Document üìú Full Submission Text                                                ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÇ\n‚îÇ   üè† https://sec.gov/Archives/edgar/data/320193/0000320193-24-000006-index.html                                 ‚îÇ\n‚îÇ   üìÑ https://sec.gov/Archives/edgar/data/320193/000032019324000006/aapl-20231230.htm                            ‚îÇ\n‚îÇ   üìú https://sec.gov/Archives/edgar/data/320193/000032019324000006/0000320193-24-000006.txt                     ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nTo download the filing content, you can use the obj() method, which in this case will return a TenQ object. This object contains the filing‚Äôs content.\n\ntenq = latest_10q.obj()\ntenq\n\n\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 10-Q üìä filing for Apple Inc. ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ\n‚îÇ ‚îÇ                                                                                                             ‚îÇ ‚îÇ\n‚îÇ ‚îÇ   Accession Number       Filing Date   Company      CIK                                                     ‚îÇ ‚îÇ\n‚îÇ ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                 ‚îÇ ‚îÇ\n‚îÇ ‚îÇ   0000320193-24-000006   2024-02-02    Apple Inc.   320193                                                  ‚îÇ ‚îÇ\n‚îÇ ‚îÇ                                                                                                             ‚îÇ ‚îÇ\n‚îÇ ‚îÇ                                                                                                             ‚îÇ ‚îÇ\n‚îÇ ‚îÇ   Links: üè† Homepage üìÑ Primary Document üìú Full Submission Text                                            ‚îÇ ‚îÇ\n‚îÇ ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                ‚îÇ ‚îÇ\n‚îÇ ‚îÇ   üè† https://sec.gov/Archives/edgar/data/320193/0000320193-24-000006-index.html                             ‚îÇ ‚îÇ\n‚îÇ ‚îÇ   üìÑ https://sec.gov/Archives/edgar/data/320193/000032019324000006/aapl-20231230.htm                        ‚îÇ ‚îÇ\n‚îÇ ‚îÇ   üìú https://sec.gov/Archives/edgar/data/320193/000032019324000006/0000320193-24-000006.txt                 ‚îÇ ‚îÇ\n‚îÇ ‚îÇ                                                                                                             ‚îÇ ‚îÇ\n‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                            Balance Sheet                                                                        ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                                                       2023-12-30                                                ‚îÇ\n‚îÇ    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ                                               ‚îÇ\n‚îÇ     ASSETS:                                                                                                     ‚îÇ\n‚îÇ     CURRENT ASSETS:                                                                                             ‚îÇ\n‚îÇ       Cash and Cash Equivalents                   40,760,000,000                                                ‚îÇ\n‚îÇ       Other Current Assets                        13,979,000,000                                                ‚îÇ\n‚îÇ       Current Assets                             143,692,000,000                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     NONCURRENT ASSETS:                                                                                          ‚îÇ\n‚îÇ       Marketable Securities                       99,475,000,000                                                ‚îÇ\n‚îÇ       Property, Plant and Equipment               43,666,000,000                                                ‚îÇ\n‚îÇ       Other Noncurrent Assets                     66,681,000,000                                                ‚îÇ\n‚îÇ       Total Noncurrent Assets                    209,822,000,000                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     Total Assets                                 353,514,000,000                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     LIABILITIES AND STOCKHOLDERS' EQUITY:                                                                       ‚îÇ\n‚îÇ     CURRENT LIABILITIES:                                                                                        ‚îÇ\n‚îÇ       Accounts Payable                            58,146,000,000                                                ‚îÇ\n‚îÇ       Other Current Liabilities                   54,611,000,000                                                ‚îÇ\n‚îÇ       Deferred Revenue                             8,264,000,000                                                ‚îÇ\n‚îÇ       Commercial Paper                             1,998,000,000                                                ‚îÇ\n‚îÇ       Term Debt                                   10,954,000,000                                                ‚îÇ\n‚îÇ       Total Current Liabilities                  133,973,000,000                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     NONCURRENT LIABILITIES:                                                                                     ‚îÇ\n‚îÇ       Non-current Long Term Debt                  95,088,000,000                                                ‚îÇ\n‚îÇ       Other Noncurrent Liabilities                50,353,000,000                                                ‚îÇ\n‚îÇ       Total Noncurrent Liabilities               145,441,000,000                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     Total Liabilities                            279,414,000,000                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     STOCKHOLDERS' EQUITY:                                                                                       ‚îÇ\n‚îÇ       Common Stock and paid-in Capital            75,236,000,000                                                ‚îÇ\n‚îÇ       Retained Earnings                            8,242,000,000                                                ‚îÇ\n‚îÇ       Accumulated Other Comprehensive Income      -9,378,000,000                                                ‚îÇ\n‚îÇ       Total Stockholders' Equity                  74,100,000,000                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     Total Liabilities and Stockholders' Equity   353,514,000,000                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                               Cashflow Statement                                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                                                                  2023-12-30                                     ‚îÇ\n‚îÇ    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ                                    ‚îÇ\n‚îÇ     OPERATING ACTIVITIES:                                                                                       ‚îÇ\n‚îÇ       Net Income                                             33,916,000,000                                     ‚îÇ\n‚îÇ       Depreciation and Amortization                           2,848,000,000                                     ‚îÇ\n‚îÇ       Stock-based Compensation                                2,997,000,000                                     ‚îÇ\n‚îÇ       Other Noncash Income/Expense                              989,000,000                                     ‚îÇ\n‚îÇ       Changes in Accounts Payable                            -4,542,000,000                                     ‚îÇ\n‚îÇ       Changes in Inventories                                    137,000,000                                     ‚îÇ\n‚îÇ       Net Cash Provided by Operating Activities              39,895,000,000                                     ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     INVESTING ACTIVITIES:                                                                                       ‚îÇ\n‚îÇ       Purchases of Marketable Securities                      9,780,000,000                                     ‚îÇ\n‚îÇ       Proceeds from Maturities of Marketable Securities      13,046,000,000                                     ‚îÇ\n‚îÇ       Proceeds from Sale of Marketable Securities             1,337,000,000                                     ‚îÇ\n‚îÇ       Purchases of Property, Plant and Equipment              2,392,000,000                                     ‚îÇ\n‚îÇ       Other Investing Activities                                284,000,000                                     ‚îÇ\n‚îÇ       Net Cash Provided by Investing Activities               1,927,000,000                                     ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     FINANCING ACTIVITIES:                                                                                       ‚îÇ\n‚îÇ       Payments of Tax for Share-based Compensation            2,591,000,000                                     ‚îÇ\n‚îÇ       Dividends Paid                                          3,825,000,000                                     ‚îÇ\n‚îÇ       Repurchases of Common Stock                            20,139,000,000                                     ‚îÇ\n‚îÇ       Repayments of Long-term Debt                                        0                                     ‚îÇ\n‚îÇ       Net Cash Provided by Financing Activities             -30,585,000,000                                     ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     Changes in Cash, cash equivalents and restricted cash    11,237,000,000                                     ‚îÇ\n‚îÇ     Cash, cash equivalents and restricted cash               41,974,000,000                                     ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                  Consolidated Statement of Operations                                                           ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                                                          2023-12-30                                             ‚îÇ\n‚îÇ    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ                                            ‚îÇ\n‚îÇ     Total Net Sales                                 119,575,000,000                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     COST OF SALES:                                                                                              ‚îÇ\n‚îÇ     Cost Goods and Services Sold                     64,720,000,000                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     Gross Profit                                     54,855,000,000                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     OPERATING EXPENSES:                                                                                         ‚îÇ\n‚îÇ       Research and Development Expenses               7,696,000,000                                             ‚îÇ\n‚îÇ       Selling General and Administrative Expenses     6,786,000,000                                             ‚îÇ\n‚îÇ     Total Operating Expenses                         14,482,000,000                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     Operating Income                                 40,373,000,000                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     OTHER INCOME/EXPENSE:                                                                                       ‚îÇ\n‚îÇ       Nonoperating Income                               -50,000,000                                             ‚îÇ\n‚îÇ     Income Before Taxes                              40,323,000,000                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     Income Tax Expense                                6,407,000,000                                             ‚îÇ\n‚îÇ     Net Income                                       33,916,000,000                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ     EARNINGS PER SHARE:                                                                                         ‚îÇ\n‚îÇ       Basic                                                    2.19                                             ‚îÇ\n‚îÇ       Diluted                                                  2.18                                             ‚îÇ\n‚îÇ     WEIGHTED AVERAGE SHARES OUTSTANDING:                                                                        ‚îÇ\n‚îÇ       Basic                                          15,509,763,000                                             ‚îÇ\n‚îÇ       Diluted                                        15,576,641,000                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nThe TenQ object provides access to the text content of the filing in a structured format, split by sections called ‚ÄúItems‚Äù. You can access the list of items in the filing using the .item attribute:\n\ntenq.items\n\n['Item 1', 'Item 2', 'Item 3', 'Item 4', 'Item 1A', 'Item 5', 'Item 6']\n\n\nYou can then access the text content of each item using by passing the item name in square brackets:\n\nprint(tenq[\"Item 2\"])\n\nItem 2.    Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\nThis Item and other sections of this Quarterly Report on Form 10-Q (‚ÄúForm 10-Q‚Äù) contain forward-looking statements, within the meaning of the Private Securities Litigation Reform Act of 1995, that involve risks and uncertainties. Forward-looking statements provide current expectations of future events based on certain assumptions and include any statement that does not directly relate to any historical or current fact. For example, statements in this Form 10-Q regarding the potential future impact of macroeconomic conditions on the Company‚Äôs business and results of operations are forward-looking statements. Forward-looking statements can also be identified by words such as ‚Äúfuture,‚Äù ‚Äúanticipates,‚Äù ‚Äúbelieves,‚Äù ‚Äúestimates,‚Äù ‚Äúexpects,‚Äù ‚Äúintends,‚Äù ‚Äúplans,‚Äù ‚Äúpredicts,‚Äù ‚Äúwill,‚Äù ‚Äúwould,‚Äù ‚Äúcould,‚Äù ‚Äúcan,‚Äù ‚Äúmay,‚Äù and similar terms. Forward-looking statements are not guarantees of future performance and the Company‚Äôs actual results may differ significantly from the results discussed in the forward-looking statements. Factors that might cause such differences include, but are not limited to, those discussed in Part I, Item 1A of the 2023 Form 10-K under the heading ‚ÄúRisk Factors.‚Äù The Company assumes no obligation to revise or update any forward-looking statements for any reason, except as required by law.\nUnless otherwise stated, all information presented herein is based on the Company‚Äôs fiscal calendar, and references to particular years, quarters, months or periods refer to the Company‚Äôs fiscal years ended in September and the associated quarters, months and periods of those fiscal years.\nThe following discussion should be read in conjunction with the 2023 Form 10-K filed with the U.S. Securities and Exchange Commission (the ‚ÄúSEC‚Äù) and the condensed consolidated financial statements and accompanying notes included in Part I, Item 1 of this Form 10-Q.\nAvailable Information\nThe Company periodically provides certain information for investors on its corporate website, www.apple.com, and its investor relations website, investor.apple.com. This includes press releases and other information about financial performance, information on environmental, social and governance matters, and details related to the Company‚Äôs annual meeting of shareholders. The information contained on the websites referenced in this Form 10-Q is not incorporated by reference into this filing. Further, the Company‚Äôs references to website URLs are intended to be inactive textual references only.\nBusiness Seasonality and Product Introductions\nThe Company has historically experienced higher net sales in its first quarter compared to other quarters in its fiscal year due in part to seasonal holiday demand. Additionally, new product and service introductions can significantly impact net sales, cost of sales and operating expenses. The timing of product introductions can also impact the Company‚Äôs net sales to its indirect distribution channels as these channels are filled with new inventory following a product launch, and channel inventory of an older product often declines as the launch of a newer product approaches. Net sales can also be affected when consumers and distributors anticipate a product introduction.\nFiscal Period\nThe Company‚Äôs fiscal year is the 52- or 53-week period that ends on the last Saturday of September. An additional week is included in the first fiscal quarter every five or six years to realign the Company‚Äôs fiscal quarters with calendar quarters, which occurred in the first quarter of 2023. The Company‚Äôs fiscal years 2024 and 2023 span 52 and 53 weeks, respectively.\nQuarterly Highlights\nThe Company‚Äôs first quarter of 2024 included 13 weeks, compared to 14 weeks during the first quarter of 2023.\nThe Company‚Äôs total net sales increased 2% or $2.4 billion during the first quarter of 2024 compared to the same quarter in 2023, driven primarily by higher net sales of iPhone and Services, partially offset by lower net sales of iPad and Wearables, Home and Accessories.\nDuring the first quarter of 2024, the Company announced an updated MacBook Pro¬Æ 14-in., MacBook Pro 16-in. and iMac¬Æ.\nThe Company repurchased $20.5 billion of its common stock and paid dividends and dividend equivalents of $3.8 billion during the first quarter of 2024.\nMacroeconomic Conditions\nMacroeconomic conditions, including inflation, changes in interest rates, and currency fluctuations, have directly and indirectly impacted, and could in the future materially impact, the Company‚Äôs results of operations and financial condition.\nApple Inc. | Q1 2024 Form 10-Q | 13\n\n\nSegment Operating Performance\nThe following table shows net sales by reportable segment for the three months ended December 30, 2023 and December 31, 2022 (dollars in millions):\n | Three Months Ended | | ---------------------------------+--------------------+---------+----------------- | December 30,2023 | | December 31,2022 | | Change | | | | | ---------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Net sales by reportable segment: | | | | | | | | | | ---------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Americas | $ | 50,430 | | | $ | 49,278 | | | 2 | % ---------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Europe | 30,397 | | | 27,681 | | | 10 | % | | ---------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Greater China | 20,819 | | | 23,905 | | | (13) | % | | ---------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Japan | 7,767 | | | 6,755 | | | 15 | % | | ---------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Rest of Asia Pacific | 10,162 | | | 9,535 | | | 7 | % | | ---------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Total net sales | $ | 119,575 | | | $ | 117,154 | | | 2 | % ---------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Americas\nAmericas net sales increased 2% or $1.2 billion during the first quarter of 2024 compared to the same quarter in 2023 due primarily to higher net sales of Services and iPhone, partially offset by lower net sales of iPad. The strength in foreign currencies relative to the U.S. dollar had a net favorable year-over-year impact on Americas net sales during the first quarter of 2024. \nEurope\nEurope net sales increased 10% or $2.7 billion during the first quarter of 2024 compared to the same quarter in 2023 due primarily to higher net sales of iPhone. The strength in foreign currencies relative to the U.S. dollar had a net favorable year-over-year impact on Europe net sales during the first quarter of 2024.\nGreater China\nGreater China net sales decreased 13% or $3.1 billion during the first quarter of 2024 compared to the same quarter in 2023 due primarily to lower net sales of iPhone, iPad and Wearables, Home and Accessories. The weakness in the renminbi relative to the U.S. dollar had an unfavorable year-over-year impact on Greater China net sales during the first quarter of 2024.\nJapan\nJapan net sales increased 15% or $1.0 billion during the first quarter of 2024 compared to the same quarter in 2023 due primarily to higher net sales of iPhone. The weakness in the yen relative to the U.S. dollar had an unfavorable year-over-year impact on Japan net sales during the first quarter of 2024.\nRest of Asia Pacific\nRest of Asia Pacific net sales increased 7% or $627 million during the first quarter of 2024 compared to the same quarter in 2023 due primarily to higher net sales of iPhone, partially offset by lower net sales of Wearables, Home and Accessories.\nApple Inc. | Q1 2024 Form 10-Q | 14\n\n\nProducts and Services Performance\nThe following table shows net sales by category for the three months ended December 30, 2023 and December 31, 2022 (dollars in millions):\n | Three Months Ended | | --------------------------------+--------------------+---------+----------------- | December 30,2023 | | December 31,2022 | | Change | | | | | --------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Net sales by category: | | | | | | | | | | --------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- iPhone | $ | 69,702 | | | $ | 65,775 | | | 6 | % --------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Mac | 7,780 | | | 7,735 | | | 1 | % | | --------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- iPad | 7,023 | | | 9,396 | | | (25) | % | | --------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Wearables, Home and Accessories | 11,953 | | | 13,482 | | | (11) | % | | --------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Services | 23,117 | | | 20,766 | | | 11 | % | | --------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- Total net sales | $ | 119,575 | | | $ | 117,154 | | | 2 | % --------------------------------+--------------------+---------+------------------+--------+--------+---------+------+---+---+-- iPhone\niPhone net sales increased 6% or $3.9 billion during the first quarter of 2024 compared to the same quarter in 2023 due primarily to higher net sales of Pro models, partially offset by lower net sales of other models.\nMac\nMac net sales were relatively flat during the first quarter of 2024 compared to the same quarter in 2023.\niPad\niPad net sales decreased 25% or $2.4 billion during the first quarter of 2024 compared to the same quarter in 2023 due primarily to lower net sales of iPad Pro, iPad 9th generation and iPad Air.\nWearables, Home and Accessories\nWearables, Home and Accessories net sales decreased 11% or $1.5 billion during the first quarter of 2024 compared to the same quarter in 2023 due primarily to lower net sales of Wearables and Accessories.\nServices\nServices net sales increased 11% or $2.4 billion during the first quarter of 2024 compared to the same quarter in 2023 due primarily to higher net sales from advertising, video and cloud services.\nApple Inc. | Q1 2024 Form 10-Q | 15\n\n\nGross Margin\nProducts and Services gross margin and gross margin percentage for the three months ended December 30, 2023 and December 31, 2022 were as follows (dollars in millions):\n | Three Months Ended | | -------------------+--------------------+--------+----------------- | December 30,2023 | | December 31,2022 | | | -------------------+--------------------+--------+------------------+--------+---+------- Gross margin: | | | | | | -------------------+--------------------+--------+------------------+--------+---+------- Products | $ | 38,018 | | | $ | 35,623 -------------------+--------------------+--------+------------------+--------+---+------- Services | 16,837 | | | 14,709 | | -------------------+--------------------+--------+------------------+--------+---+------- Total gross margin | $ | 54,855 | | | $ | 50,332 -------------------+--------------------+--------+------------------+--------+---+------- Gross margin percentage: | | | | ------------------------------+------+---+------+-- Products | 39.4 | % | 37.0 | % ------------------------------+------+---+------+-- Services | 72.8 | % | 70.8 | % ------------------------------+------+---+------+-- Total gross margin percentage | 45.9 | % | 43.0 | % ------------------------------+------+---+------+-- Products Gross Margin\nProducts gross margin increased during the first quarter of 2024 compared to the same quarter in 2023 due primarily to cost savings and a different Products mix, partially offset by the weakness in foreign currencies relative to the U.S. dollar and lower Products volume.\nProducts gross margin percentage increased during the first quarter of 2024 compared to the same quarter in 2023 due primarily to cost savings and a different Products mix, partially offset by the weakness in foreign currencies relative to the U.S. dollar.\nServices Gross Margin\nServices gross margin increased during the first quarter of 2024 compared to the same quarter in 2023 due primarily to higher Services net sales and a different Services mix.\nServices gross margin percentage increased during the first quarter of 2024 compared to the same quarter in 2023 due primarily to a different Services mix.\nThe Company‚Äôs future gross margins can be impacted by a variety of factors, as discussed in Part I, Item 1A of the 2023 Form 10-K under the heading ‚ÄúRisk Factors.‚Äù As a result, the Company believes, in general, gross margins will be subject to volatility and downward pressure.\nApple Inc. | Q1 2024 Form 10-Q | 16\n\n\nOperating Expenses\nOperating expenses for the three months ended December 30, 2023 and December 31, 2022 were as follows (dollars in millions):\n | Three Months Ended | | ------------------------------------+--------------------+--------+----------------- | December 30,2023 | | December 31,2022 | | | ------------------------------------+--------------------+--------+------------------+----+---+------- Research and development | $ | 7,696 | | | $ | 7,709 ------------------------------------+--------------------+--------+------------------+----+---+------- Percentage of total net sales | 6 | % | | 7 | % | ------------------------------------+--------------------+--------+------------------+----+---+------- Selling, general and administrative | $ | 6,786 | | | $ | 6,607 ------------------------------------+--------------------+--------+------------------+----+---+------- Percentage of total net sales | 6 | % | | 6 | % | ------------------------------------+--------------------+--------+------------------+----+---+------- Total operating expenses | $ | 14,482 | | | $ | 14,316 ------------------------------------+--------------------+--------+------------------+----+---+------- Percentage of total net sales | 12 | % | | 12 | % | ------------------------------------+--------------------+--------+------------------+----+---+------- Research and Development\nResearch and development (‚ÄúR&D‚Äù) expense was relatively flat during the first quarter of 2024 compared to the same quarter in 2023.\nSelling, General and Administrative\nSelling, general and administrative expense increased 3% or $179 million during the first quarter of 2024 compared to the same quarter in 2023.\nProvision for Income Taxes\nProvision for income taxes, effective tax rate and statutory federal income tax rate for the three months ended December 30, 2023 and December 31, 2022 were as follows (dollars in millions):\n | Three Months Ended | | ----------------------------------+--------------------+-------+----------------- | December 30,2023 | | December 31,2022 | | | ----------------------------------+--------------------+-------+------------------+------+---+------ Provision for income taxes | $ | 6,407 | | | $ | 5,625 ----------------------------------+--------------------+-------+------------------+------+---+------ Effective tax rate | 15.9 | % | | 15.8 | % | ----------------------------------+--------------------+-------+------------------+------+---+------ Statutory federal income tax rate | 21 | % | | 21 | % | ----------------------------------+--------------------+-------+------------------+------+---+------ The Company‚Äôs effective tax rate for the first quarter of 2024 was lower than the statutory federal income tax rate due primarily to a lower effective tax rate on foreign earnings, tax benefits from share-based compensation, and the impact of the U.S. federal R&D credit, partially offset by state income taxes.\nThe Company‚Äôs effective tax rate for the first quarter of 2024 was relatively flat compared to the same quarter in 2023.\nLiquidity and Capital Resources\nThe Company believes its balances of cash, cash equivalents and unrestricted marketable securities, along with cash generated by ongoing operations and continued access to debt markets, will be sufficient to satisfy its cash requirements and capital return program over the next 12 months and beyond.\nThe Company‚Äôs contractual cash requirements have not changed materially since the 2023 Form 10-K, except for manufacturing purchase obligations.\nManufacturing Purchase Obligations\nThe Company utilizes several outsourcing partners to manufacture subassemblies for the Company‚Äôs products and to perform final assembly and testing of finished products. The Company also obtains individual components for its products from a wide variety of individual suppliers. As of December 30, 2023, the Company had manufacturing purchase obligations of $38.0 billion, with $37.9 billion payable within 12 months.\nApple Inc. | Q1 2024 Form 10-Q | 17\n\n\nCapital Return Program\nIn addition to its contractual cash requirements, the Company has an authorized share repurchase program. The program does not obligate the Company to acquire a minimum amount of shares. As of December 30, 2023, the Company‚Äôs quarterly cash dividend was $0.24 per share. The Company intends to increase its dividend on an annual basis, subject to declaration by the Board of Directors.\nRecent Accounting Pronouncements\nIncome Taxes\nIn December 2023, the Financial Accounting Standards Board (the ‚ÄúFASB‚Äù) issued Accounting Standards Update (‚ÄúASU‚Äù) No. 2023-09, Income Taxes (Topic 740): Improvements to Income Tax Disclosures (‚ÄúASU 2023-09‚Äù), which will require the Company to disclose specified additional information in its income tax rate reconciliation and provide additional information for reconciling items that meet a quantitative threshold. ASU 2023-09 will also require the Company to disaggregate its income taxes paid disclosure by federal, state and foreign taxes, with further disaggregation required for significant individual jurisdictions. The Company will adopt ASU 2023-09 in its fourth quarter of 2026. ASU 2023-09 allows for adoption using either a prospective or retrospective transition method.\nSegment Reporting\nIn November 2023, the FASB issued ASU No. 2023-07, Segment Reporting (Topic 280): Improvements to Reportable Segment Disclosures (‚ÄúASU 2023-07‚Äù), which will require the Company to disclose segment expenses that are significant and regularly provided to the Company‚Äôs chief operating decision maker (‚ÄúCODM‚Äù). In addition, ASU 2023-07 will require the Company to disclose the title and position of its CODM and how the CODM uses segment profit or loss information in assessing segment performance and deciding how to allocate resources. The Company will adopt ASU 2023-07 in its fourth quarter of 2025 using a retrospective transition method.\nCritical Accounting Estimates\nThe preparation of financial statements and related disclosures in conformity with GAAP and the Company‚Äôs discussion and analysis of its financial condition and operating results require the Company‚Äôs management to make judgments, assumptions and estimates that affect the amounts reported. Note 1, ‚ÄúSummary of Significant Accounting Policies‚Äù of the Notes to Condensed Consolidated Financial Statements in Part I, Item 1 of this Form 10-Q and in the Notes to Consolidated Financial Statements in Part II, Item 8 of the 2023 Form 10-K describe the significant accounting policies and methods used in the preparation of the Company‚Äôs condensed consolidated financial statements. There have been no material changes to the Company‚Äôs critical accounting estimates since the 2023 Form 10-K.\nItem 2.    Unregistered Sales of Equity Securities and Use of Proceeds\nPurchases of Equity Securities by the Issuer and Affiliated Purchasers\nShare repurchase activity during the three months ended December 30, 2023 was as follows (in millions, except number of shares, which are reflected in thousands, and per-share amounts):\nPeriods | Total Numberof Shares Purchased | Average PricePaid Per Share | | Total Number of SharesPurchased as Part of PubliclyAnnounced Plans or Programs | | Approximate Dollar Value ofShares That May Yet Be PurchasedUnder the Plans or Programs (1) -----------------------------------------------+---------------------------------+-----------------------------+-----+--------------------------------------------------------------------------------+-------+------------------------------------------------------------------------------------------- October 1, 2023 to November 4, 2023: | | | | | | -----------------------------------------------+---------------------------------+-----------------------------+-----+--------------------------------------------------------------------------------+-------+------------------------------------------------------------------------------------------- August 2023 ASRs | 6,498 | | (2) | | 6,498 | | | -----------------------------------------------+---------------------------------+-----------------------------+-----+--------------------------------------------------------------------------------+-------+--------------------------------------------------------------------------------------------+--------+------- Open market and privately negotiated purchases | 45,970 | | $ | 174.03 | | | 45,970 | -----------------------------------------------+---------------------------------+-----------------------------+-----+--------------------------------------------------------------------------------+-------+--------------------------------------------------------------------------------------------+--------+------- November 5, 2023 to December 2, 2023: | | | | | | -----------------------------------------------+---------------------------------+-----------------------------+-----+--------------------------------------------------------------------------------+-------+------------------------------------------------------------------------------------------- Open market and privately negotiated purchases | 33,797 | | $ | 187.14 | | | 33,797 | -----------------------------------------------+---------------------------------+-----------------------------+-----+--------------------------------------------------------------------------------+-------+--------------------------------------------------------------------------------------------+--------+------- December 3, 2023 to December 30, 2023: | | | | | | -----------------------------------------------+---------------------------------+-----------------------------+-----+--------------------------------------------------------------------------------+-------+------------------------------------------------------------------------------------------- Open market and privately negotiated purchases | 31,782 | | $ | 194.29 | | | 31,782 | -----------------------------------------------+---------------------------------+-----------------------------+-----+--------------------------------------------------------------------------------+-------+--------------------------------------------------------------------------------------------+--------+------- Total | 118,047 | | | | | | $ | 53,569 -----------------------------------------------+---------------------------------+-----------------------------+-----+--------------------------------------------------------------------------------+-------+--------------------------------------------------------------------------------------------+--------+------- (1)As of December 30, 2023, the Company was authorized by the Board of Directors to purchase up to $90 billion of the Company‚Äôs common stock under a share repurchase program announced on May 4, 2023, of which $36.4 billion had been utilized. The program does not obligate the Company to acquire a minimum amount of shares. Under the program, shares may be repurchased in privately negotiated or open market transactions, including under plans complying with Rule 10b5-1 under the Exchange Act.\n(2)In August 2023, the Company entered into accelerated share repurchase agreements (‚ÄúASRs‚Äù) to purchase up to a total of $5.0 billion of the Company‚Äôs common stock. In October 2023, the purchase periods for these ASRs ended and an additional 6 million shares were delivered and retired. In total, 29 million shares were delivered under these ASRs at an average repurchase price of $174.93 per share.\nApple Inc. | Q1 2024 Form 10-Q | 20\n\n\n\n\n\n\n\nXBRL Data\nXBRL (eXtensible Business Reporting Language) data offers a standardized format for financial reporting, making it easier to compare financial information across companies. SEC has required companies to submit their financial statements in XBRL format for many years, and this data is available through the Edgar database. edgartools provides tools to access this parsed data directly:\n\nxbrl = latest_10q.xbrl()\nxbrl\n\n\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ Form 10-Q Extracted XBRL                                                                                        ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                cik      form   namespaces   facts                                                               ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                              ‚îÇ\n‚îÇ   Apple Inc.   320193   10-Q   11           629                                                                 ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ Facts                                                                                                           ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ         namespace   fact                         value        units   end_date                                  ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                               ‚îÇ\n‚îÇ   0     dei         AmendmentFlag                False        None    2023-12-30                                ‚îÇ\n‚îÇ   1     dei         DocumentFiscalYearFocus      2024         None    2023-12-30                                ‚îÇ\n‚îÇ   2     dei         DocumentFiscalPeriodFocus    Q1           None    2023-12-30                                ‚îÇ\n‚îÇ   3     dei         EntityCentralIndexKey        0000320193   None    2023-12-30                                ‚îÇ\n‚îÇ   4     dei         CurrentFiscalYearEndDate     --09-28      None    2023-12-30                                ‚îÇ\n‚îÇ   ...   ...         ...                          ...          ...     ...                                       ‚îÇ\n‚îÇ   624   ecd         Rule10b51ArrAdoptedFlag      True         None    2023-12-30                                ‚îÇ\n‚îÇ   625   ecd         Rule10b51ArrAdoptedFlag      True         None    2023-12-30                                ‚îÇ\n‚îÇ   626   ecd         NonRule10b51ArrAdoptedFlag   False        None    2023-12-30                                ‚îÇ\n‚îÇ   627   ecd         Rule10b51ArrTrmntdFlag       False        None    2023-12-30                                ‚îÇ\n‚îÇ   628   ecd         NonRule10b51ArrTrmntdFlag    False        None    2023-12-30                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ Taxonomies                                                                                                      ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ        taxonomy   namespace                                                                                     ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                    ‚îÇ\n‚îÇ   0    aapl       http://www.apple.com/20231230                                                                 ‚îÇ\n‚îÇ   1    dei        http://xbrl.sec.gov/dei/2023                                                                  ‚îÇ\n‚îÇ   2    ecd        http://xbrl.sec.gov/ecd/2023                                                                  ‚îÇ\n‚îÇ   3    iso4217    http://www.xbrl.org/2003/iso4217                                                              ‚îÇ\n‚îÇ   4    lang       en-US                                                                                         ‚îÇ\n‚îÇ   5    link       http://www.xbrl.org/2003/linkbase                                                             ‚îÇ\n‚îÇ   6    srt        http://fasb.org/srt/2023                                                                      ‚îÇ\n‚îÇ   7    us-gaap    http://fasb.org/us-gaap/2023                                                                  ‚îÇ\n‚îÇ   8    xbrldi     http://xbrl.org/2006/xbrldi                                                                   ‚îÇ\n‚îÇ   9    xlink      http://www.w3.org/1999/xlink                                                                  ‚îÇ\n‚îÇ   10   xsi        http://www.w3.org/2001/XMLSchema-instance                                                     ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nYou can also access the XBRL data as a pandas DataFrame, using the .facts.data attribute:\n\nxbrl.facts.data\n\n\n\n\n\n\n\n\nnamespace\nfact\nvalue\nunits\nstart_date\nend_date\ndimensions\nperiod\n\n\n\n\n0\ndei\nAmendmentFlag\nFalse\nNone\n2023-10-01\n2023-12-30\nNone\n2023-10-01 to 2023-12-30\n\n\n1\ndei\nDocumentFiscalYearFocus\n2024\nNone\n2023-10-01\n2023-12-30\nNone\n2023-10-01 to 2023-12-30\n\n\n2\ndei\nDocumentFiscalPeriodFocus\nQ1\nNone\n2023-10-01\n2023-12-30\nNone\n2023-10-01 to 2023-12-30\n\n\n3\ndei\nEntityCentralIndexKey\n0000320193\nNone\n2023-10-01\n2023-12-30\nNone\n2023-10-01 to 2023-12-30\n\n\n4\ndei\nCurrentFiscalYearEndDate\n--09-28\nNone\n2023-10-01\n2023-12-30\nNone\n2023-10-01 to 2023-12-30\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n624\necd\nRule10b51ArrAdoptedFlag\nTrue\nNone\n2023-10-01\n2023-12-30\n{'ecd:IndividualAxis': 'aapl:LucaMaestriMember'}\n2023-10-01 to 2023-12-30\n\n\n625\necd\nRule10b51ArrAdoptedFlag\nTrue\nNone\n2023-10-01\n2023-12-30\n{'ecd:IndividualAxis': 'aapl:KatherineLAdamsMe...\n2023-10-01 to 2023-12-30\n\n\n626\necd\nNonRule10b51ArrAdoptedFlag\nFalse\nNone\n2023-10-01\n2023-12-30\nNone\n2023-10-01 to 2023-12-30\n\n\n627\necd\nRule10b51ArrTrmntdFlag\nFalse\nNone\n2023-10-01\n2023-12-30\nNone\n2023-10-01 to 2023-12-30\n\n\n628\necd\nNonRule10b51ArrTrmntdFlag\nFalse\nNone\n2023-10-01\n2023-12-30\nNone\n2023-10-01 to 2023-12-30\n\n\n\n\n629 rows √ó 8 columns\n\n\n\nIf you just wanted to access the latest XBRL data for a company, you can do so directly using the get_facts() method:\n\napple.get_facts().to_pandas()\n\n\n\n\n\n\n\n\nnamespace\nfact\nval\naccn\nend\nfy\nfp\nform\nfiled\nframe\nstart\n\n\n\n\n0\ndei\nEntityCommonStockSharesOutstanding\n8.958168e+08\n0001193125-09-153165\n2009-06-27\n2009\nQ3\n10-Q\n2009-07-22\nCY2009Q2I\nNone\n\n\n1\ndei\nEntityCommonStockSharesOutstanding\n9.006785e+08\n0001193125-09-214859\n2009-10-16\n2009\nFY\n10-K\n2009-10-27\nNone\nNone\n\n\n2\ndei\nEntityCommonStockSharesOutstanding\n9.006785e+08\n0001193125-10-012091\n2009-10-16\n2009\nFY\n10-K/A\n2010-01-25\nCY2009Q3I\nNone\n\n\n3\ndei\nEntityCommonStockSharesOutstanding\n9.067946e+08\n0001193125-10-012085\n2010-01-15\n2010\nQ1\n10-Q\n2010-01-25\nCY2009Q4I\nNone\n\n\n4\ndei\nEntityCommonStockSharesOutstanding\n9.099384e+08\n0001193125-10-088957\n2010-04-09\n2010\nQ2\n10-Q\n2010-04-21\nCY2010Q1I\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22323\nus-gaap\nIncrementalCommonSharesAttributableToShareBase...\n6.299500e+07\n0000320193-24-000006\n2022-12-31\n2024\nQ1\n10-Q\n2024-02-02\nCY2022Q4\n2022-09-25\n\n\n22324\nus-gaap\nIncrementalCommonSharesAttributableToShareBase...\n6.831600e+07\n0000320193-23-000106\n2023-09-30\n2023\nFY\n10-K\n2023-11-03\nCY2023\n2022-09-25\n\n\n22325\nus-gaap\nIncrementalCommonSharesAttributableToShareBase...\n6.687800e+07\n0000320193-24-000006\n2023-12-30\n2024\nQ1\n10-Q\n2024-02-02\nCY2023Q4\n2023-10-01\n\n\n22326\nus-gaap\nOtherAssetsMiscellaneousNoncurrent\n3.905300e+10\n0000320193-23-000106\n2022-09-24\n2023\nFY\n10-K\n2023-11-03\nCY2022Q3I\nNone\n\n\n22327\nus-gaap\nOtherAssetsMiscellaneousNoncurrent\n4.690600e+10\n0000320193-23-000106\n2023-09-30\n2023\nFY\n10-K\n2023-11-03\nCY2023Q3I\nNone\n\n\n\n\n22328 rows √ó 11 columns"
  },
  {
    "objectID": "posts/edgartools/index.html#working-with-8-ks-and-press-releases",
    "href": "posts/edgartools/index.html#working-with-8-ks-and-press-releases",
    "title": "Scrape financial data from SEC Edgar with Python and edgartools",
    "section": "Working with 8-Ks and press releases",
    "text": "Working with 8-Ks and press releases\n8-K filings are an essential source of up-to-date information about significant corporate events. These documents can contain anything from financial results to news of mergers and acquisitions, making them invaluable for financial analysis. edgartools simplifies the process of accessing these filings and extracting press releases, if available.\n\nRetrieving 8-K Filings\nTo start, you‚Äôll need to query the Edgar database for 8-K filings for a specific company:\n\nlatest_8ks = apple.get_filings(form=\"8-K\")\nlatest_8ks\n\n\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Filings for Apple Inc. [320193] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ        form   filed        accession_number       xbrl                                                          ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                         ‚îÇ\n‚îÇ   0    8-K    2024-02-28   0001140361-24-010155   ‚úì                                                             ‚îÇ\n‚îÇ   1    8-K    2024-02-01   0000320193-24-000005   ‚úì                                                             ‚îÇ\n‚îÇ   2    8-K    2023-11-02   0000320193-23-000104   ‚úì                                                             ‚îÇ\n‚îÇ   3    8-K    2023-08-03   0000320193-23-000075   ‚úì                                                             ‚îÇ\n‚îÇ   4    8-K    2023-05-10   0001140361-23-023909   ‚úì                                                             ‚îÇ\n‚îÇ   5    8-K    2023-05-04   0000320193-23-000063   ‚úì                                                             ‚îÇ\n‚îÇ   6    8-K    2023-03-10   0001140361-23-011192   ‚úì                                                             ‚îÇ\n‚îÇ   7    8-K    2023-02-02   0000320193-23-000005   ‚úì                                                             ‚îÇ\n‚îÇ   8    8-K    2022-11-07   0001193125-22-278435   ‚úì                                                             ‚îÇ\n‚îÇ   9    8-K    2022-10-27   0000320193-22-000107   ‚úì                                                             ‚îÇ\n‚îÇ   10   8-K    2022-08-19   0001193125-22-225365   ‚úì                                                             ‚îÇ\n‚îÇ   11   8-K    2022-08-08   0001193125-22-214914   ‚úì                                                             ‚îÇ\n‚îÇ   12   8-K    2022-07-28   0000320193-22-000069   ‚úì                                                             ‚îÇ\n‚îÇ   13   8-K    2022-04-28   0000320193-22-000058   ‚úì                                                             ‚îÇ\n‚îÇ   14   8-K    2022-03-04   0001193125-22-066169   ‚úì                                                             ‚îÇ\n‚îÇ   15   8-K    2022-01-27   0000320193-22-000006   ‚úì                                                             ‚îÇ\n‚îÇ   16   8-K    2021-11-12   0001193125-21-328151   ‚úì                                                             ‚îÇ\n‚îÇ   17   8-K    2021-10-28   0000320193-21-000104   ‚úì                                                             ‚îÇ\n‚îÇ   18   8-K    2021-08-05   0001193125-21-237787   ‚úì                                                             ‚îÇ\n‚îÇ   19   8-K    2021-07-27   0000320193-21-000063   ‚úì                                                             ‚îÇ\n‚îÇ   20   8-K    2021-04-28   0000320193-21-000055   ‚úì                                                             ‚îÇ\n‚îÇ   21   8-K    2021-02-24   0001193125-21-054710   ‚úì                                                             ‚îÇ\n‚îÇ   22   8-K    2021-02-08   0001193125-21-032394   ‚úì                                                             ‚îÇ\n‚îÇ   23   8-K    2021-01-27   0000320193-21-000009   ‚úì                                                             ‚îÇ\n‚îÇ   24   8-K    2021-01-05   0001193125-21-001982   ‚úì                                                             ‚îÇ\n‚îÇ   25   8-K    2020-10-29   0000320193-20-000094   ‚úì                                                             ‚îÇ\n‚îÇ   26   8-K    2020-08-20   0001193125-20-225672   ‚úì                                                             ‚îÇ\n‚îÇ   27   8-K    2020-08-07   0001193125-20-213158   ‚úì                                                             ‚îÇ\n‚îÇ   28   8-K    2020-07-30   0000320193-20-000060   ‚úì                                                             ‚îÇ\n‚îÇ   29   8-K    2020-05-11   0001193125-20-139112   ‚úì                                                             ‚îÇ\n‚îÇ   30   8-K    2020-04-30   0000320193-20-000050   ‚úì                                                             ‚îÇ\n‚îÇ   31   8-K    2020-02-27   0001193125-20-050884   ‚úì                                                             ‚îÇ\n‚îÇ   32   8-K    2020-02-18   0001193125-20-039203   ‚úì                                                             ‚îÇ\n‚îÇ   33   8-K    2020-01-28   0000320193-20-000008   ‚úì                                                             ‚îÇ\n‚îÇ   34   8-K    2019-11-15   0001193125-19-292676   ‚úì                                                             ‚îÇ\n‚îÇ   35   8-K    2019-10-30   0000320193-19-000117   ‚úì                                                             ‚îÇ\n‚îÇ   36   8-K    2019-09-13   0000320193-19-000093   ‚úì                                                             ‚îÇ\n‚îÇ   37   8-K    2019-09-11   0001193125-19-242975   ‚úì                                                             ‚îÇ\n‚îÇ   38   8-K    2019-07-30   0000320193-19-000073                                                                 ‚îÇ\n‚îÇ   39   8-K    2019-04-30   0000320193-19-000063                                                                 ‚îÇ\n‚îÇ   40   8-K    2019-03-04   0000320193-19-000032                                                                 ‚îÇ\n‚îÇ   41   8-K    2019-02-06   0000320193-19-000026                                                                 ‚îÇ\n‚îÇ   42   8-K    2019-01-29   0000320193-19-000007                                                                 ‚îÇ\n‚îÇ   43   8-K    2019-01-02   0000320193-19-000002                                                                 ‚îÇ\n‚îÇ   44   8-K    2018-11-01   0000320193-18-000142                                                                 ‚îÇ\n‚îÇ   45   8-K    2018-07-31   0000320193-18-000098                                                                 ‚îÇ\n‚îÇ   46   8-K    2018-05-07   0001193125-18-154515                                                                 ‚îÇ\n‚îÇ   47   8-K    2018-05-01   0000320193-18-000067                                                                 ‚îÇ\n‚îÇ   48   8-K    2018-02-14   0001193125-18-045761                                                                 ‚îÇ\n‚îÇ   49   8-K    2018-02-01   0000320193-18-000005                                                                 ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ Showing 50 filings of 210 total                                                                                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nYou can then access the content of the latest 8-K filing using the obj() method, which returns an EightK object:\n\neightk = [x.obj() for x in latest_8ks.latest(10)]\n\n\n\nExtracting press releases\nNot all 8-K filings will include press releases. To filter out the filings that contain press releases, you will need to inspect each filing individually using the has_press_release attribute. Here‚Äôs a simplified approach to identify and collect filings with press releases:\n\neightks = [x for x in eightk if x.has_press_release]\n\n\nlen(eightks)\n\n7\n\n\nTo access the press release content, you can use the press_releases attribute of the EightK object, which is an object of type PressReleases:\n\npr = eightks[0].press_releases\npr\n\n\n\n\n\n                                                                      \n      Description   Document                      Type      Size      \n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n  1   EX-99.1       a8-kex991q1202412302023.htm   EX-99.1   137.4 KB  \n                                                                      \n\n\n\nThis object contains a list of press releases in the attachments attribute. Most will only have one press release, but some may have multiple attachments:\n\npr.attachments\n\n\n\n\n\n                                                                      \n      Description   Document                      Type      Size      \n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n  1   EX-99.1       a8-kex991q1202412302023.htm   EX-99.1   137.4 KB  \n                                                                      \n\n\n\nYou can open the press release in a browser to read its content using the open() method:\n\npr.attachments[0].open()\n\nAlternatively, you can access the text content of the attachement directly using the download() method:\n\ntext = pr.attachments.get(0).download()\n\nIn most cases, the press release will be in HTML format embedded into a XML file. To extract the text content from the HTML, you will need to do some extra work using libraries like BeautifulSoup.\n\nprint(text)\n\n&lt;DOCUMENT&gt;\n&lt;TYPE&gt;EX-99.1\n&lt;SEQUENCE&gt;2\n&lt;FILENAME&gt;a8-kex991q1202412302023.htm\n&lt;DESCRIPTION&gt;EX-99.1\n&lt;TEXT&gt;\n&lt;html&gt;&lt;head&gt;\n&lt;!-- Document created using Wdesk --&gt;\n&lt;!-- Copyright 2024 Workiva --&gt;\n&lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"id9d7b63400cc4428a185fcc5dc6707e0_1\"&gt;&lt;/div&gt;&lt;div style=\"min-height:42.75pt;width:100%\"&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=\"text-align:right\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:120%\"&gt;Exhibit 99.1&lt;/font&gt;&lt;/div&gt;&lt;div style=\"margin-top:6pt\"&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:15pt;font-weight:700;line-height:120%\"&gt;Apple reports first quarter results&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:11pt;font-weight:700;line-height:120%\"&gt;Services revenue reaches new all-time record&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:11pt;font-weight:700;line-height:120%\"&gt;EPS up 16 percent to new all-time high&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt;CUPERTINO, CALIFORNIA &#8212; Apple&lt;/font&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:5.85pt;font-weight:400;line-height:112%;position:relative;top:-3.15pt;vertical-align:baseline\"&gt;&#174;&lt;/font&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt; today announced financial results for its fiscal 2024 first quarter ended December 30, 2023. The Company posted quarterly revenue of $119.6 billion, up 2 percent year over year, and quarterly earnings per diluted share of $2.18, up 16 percent year over year.&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt;&#8220;Today Apple is reporting revenue growth for the December quarter fueled by iPhone sales, and an all-time revenue record in Services,&#8221; said Tim Cook, Apple&#8217;s CEO. &#8220;We are pleased to announce that our installed base of active devices has now surpassed 2.2 billion, reaching an all-time high across all products and geographic segments. And as customers begin to experience the incredible Apple Vision Pro tomorrow, we are committed as ever to the pursuit of groundbreaking innovation &#8212; in line with our values and on behalf of our customers.&#8221;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt;&#8220;Our December quarter top-line performance combined with margin expansion drove an all-time record EPS of $2.18, up 16 percent from last year,&#8221; said Luca Maestri, Apple&#8217;s CFO. &#8220;During the quarter, we generated nearly $40 billion of operating cash flow, and returned almost $27 billion to our shareholders. We are confident in our future, and continue to make significant investments across our business to support our long-term growth plans.&#8221;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt;Apple&#8217;s board of directors has declared a cash dividend of $0.24 per share of the Company&#8217;s common stock. The dividend is payable on February 15, 2024 to shareholders of record as of the close of business on February 12, 2024.&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt;Based on the Company&#8217;s fiscal calendar, the Company&#8217;s fiscal 2024 first quarter had 13 weeks, while the Company&#8217;s fiscal 2023 first quarter had 14 weeks.&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt;Apple will provide live streaming of its Q1 2024 financial results conference call beginning at 2&#58;00 p.m. PT on February 1, 2024 at apple.com&#47;investor&#47;earnings-call. The webcast will be available for replay for approximately two weeks thereafter.&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt;Apple periodically provides information for investors on its corporate website, apple.com, and its investor relations website, investor.apple.com. This includes press releases and other information about financial performance, reports filed or furnished with the SEC, information on corporate governance, and details related to its annual meeting of shareholders.&lt;/font&gt;&lt;/div&gt;&lt;div style=\"height:42.75pt;position:relative;width:100%\"&gt;&lt;div style=\"bottom:0;position:absolute;width:100%\"&gt;&lt;div style=\"text-align:center\"&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;hr style=\"page-break-after:always\"&gt;&lt;div style=\"min-height:42.75pt;width:100%\"&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt;This press release contains forward-looking statements, within the meaning of the Private Securities Litigation Reform Act of 1995. These forward-looking statements include without limitation those about payment of the Company&#8217;s quarterly dividend. These statements involve risks and uncertainties, and actual results may differ materially from any future results expressed or implied by the forward-looking statements. Risks and uncertainties include without limitation&#58; effects of global and regional economic conditions, including as a result of government policies, war, terrorism, natural disasters, and public health issues&#59; risks relating to the design, manufacture, introduction, and transition of products and services in highly competitive and rapidly changing markets, including from reliance on third parties for components, technology, manufacturing, applications, and content&#59; risks relating to information technology system failures, network disruptions, and failure to protect, loss of, or unauthorized access to, or release of, data&#59; and effects of unfavorable legal proceedings, government investigations, and complex and changing laws and regulations. More information on these risks and other potential factors that could affect the Company&#8217;s business, reputation, results of operations, financial condition, and stock price is included in the Company&#8217;s filings with the SEC, including in the &#8220;Risk Factors&#8221; and &#8220;Management&#8217;s Discussion and Analysis of Financial Condition and Results of Operations&#8221; sections of the Company&#8217;s most recently filed periodic reports on Form 10-K and Form 10-Q and subsequent filings. The Company assumes no obligation to update any forward-looking statements, which speak only as of the date they are made.&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:112%\"&gt;Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, Apple Watch, and Apple TV. Apple&#8217;s five software platforms &#8212; iOS, iPadOS, macOS, watchOS, and tvOS &#8212; provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, and iCloud. Apple&#8217;s more than 100,000 employees are dedicated to making the best products on earth, and to leaving the world better than we found it.&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:120%\"&gt;Press Contact&#58;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;Josh Rosenstock&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;Apple&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;jrosenstock&#64;apple.com&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;(408) 862-1142&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:120%\"&gt;Investor Relations Contact&#58;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;Suhasini Chandramouli&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;Apple&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;suhasini&#64;apple.com&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;(408) 974-3123&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;NOTE TO EDITORS&#58; For additional information visit Apple Newsroom (www.apple.com&#47;newsroom), or email Apple&#8217;s Media Helpline at media.help&#64;apple.com.&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:8pt;font-weight:400;line-height:120%\"&gt;&#169; 2024 Apple Inc. All rights reserved. Apple and the Apple logo are trademarks of Apple. Other company and product names may be trademarks of their respective owners.&lt;/font&gt;&lt;/div&gt;&lt;div style=\"height:42.75pt;position:relative;width:100%\"&gt;&lt;div style=\"bottom:0;position:absolute;width:100%\"&gt;&lt;div style=\"text-align:center\"&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=\"id9d7b63400cc4428a185fcc5dc6707e0_4\"&gt;&lt;/div&gt;&lt;hr style=\"page-break-after:always\"&gt;&lt;div style=\"min-height:42.75pt;width:100%\"&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=\"text-align:center\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:120%\"&gt;Apple Inc.&lt;/font&gt;&lt;/div&gt;&lt;div style=\"margin-top:9pt;text-align:center\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:120%\"&gt;CONDENSED CONSOLIDATED STATEMENTS OF OPERATIONS (Unaudited)&lt;/font&gt;&lt;/div&gt;&lt;div style=\"text-align:center\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;(In millions, except number of shares, which are reflected in thousands, and per-share amounts)&lt;/font&gt;&lt;/div&gt;&lt;div style=\"margin-top:6pt;text-align:justify\"&gt;&lt;table style=\"border-collapse:collapse;display:inline-table;margin-bottom:5pt;vertical-align:text-bottom;width:100.000%\"&gt;&lt;tr&gt;&lt;td style=\"width:1.0%\"&gt;&lt;/td&gt;&lt;td style=\"width:70.976%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:1.0%\"&gt;&lt;/td&gt;&lt;td style=\"width:12.496%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.530%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:1.0%\"&gt;&lt;/td&gt;&lt;td style=\"width:12.498%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"9\" style=\"padding:2px 1pt;text-align:center;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:8pt;font-weight:700;line-height:100%\"&gt;Three Months Ended&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-top:1pt solid #000000;padding:2px 1pt;text-align:center;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:8pt;font-weight:700;line-height:100%\"&gt;December 30,&lt;br&gt;2023&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-top:1pt solid #000000;padding:2px 1pt;text-align:center;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:8pt;font-weight:700;line-height:100%\"&gt;December 31,&lt;br&gt;2022&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Net sales&#58;&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;&#160;&#160;&#160;Products&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;96,458&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;96,388&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;&#160;&#160;&#160;Services&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;23,117&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;20,766&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:18pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total net sales &lt;/font&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:5.85pt;font-weight:400;line-height:100%;position:relative;top:-3.15pt;vertical-align:baseline\"&gt;(1)&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;119,575&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;117,154&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Cost of sales&#58;&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;&#160;&#160;&#160;Products&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;58,440&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;60,765&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;&#160;&#160;&#160;Services&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;6,280&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;6,057&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total cost of sales&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;64,720&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;66,822&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 46pt;text-align:left;text-indent:-9pt;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Gross margin&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;54,855&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;50,332&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Operating expenses&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Research and development&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;7,696&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;7,709&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Selling, general and administrative&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;6,786&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;6,607&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:45pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total operating expenses&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;14,482&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;14,316&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Operating income&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;40,373&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;36,016&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other income&#47;(expense), net&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(50)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(393)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Income before provision for income taxes&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;40,323&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;35,623&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Provision for income taxes&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;6,407&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;5,625&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Net income&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;33,916&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;29,998&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:3pt double #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:3pt double #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Earnings per share&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Basic&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;2.19&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;1.89&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Diluted&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;2.18&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;1.88&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Shares used in computing earnings per share&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Basic&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;15,509,763&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;15,892,723&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Diluted&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;15,576,641&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;15,955,718&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:5.85pt;font-weight:400;line-height:100%;position:relative;top:-3.15pt;vertical-align:baseline\"&gt;(1)&lt;/font&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;&#160;Net sales by reportable segment&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Americas&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;50,430&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;49,278&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Europe&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;30,397&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;27,681&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Greater China&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;20,819&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;23,905&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Japan&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;7,767&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;6,755&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Rest of Asia Pacific&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;10,162&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;9,535&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:45pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total net sales&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;119,575&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;117,154&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:3pt double #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:3pt double #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;div&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:5.85pt;font-weight:400;line-height:100%;position:relative;top:-3.15pt;vertical-align:baseline\"&gt;(1)&lt;/font&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;&#160;Net sales by category&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;iPhone&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;69,702&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;65,775&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Mac&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;7,780&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;7,735&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;iPad&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;7,023&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;9,396&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Wearables, Home and Accessories&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;11,953&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;13,482&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Services&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;23,117&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;20,766&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:45pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total net sales&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;119,575&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;117,154&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;div style=\"height:40.5pt;position:relative;width:100%\"&gt;&lt;div style=\"bottom:0;position:absolute;width:100%\"&gt;&lt;div style=\"text-align:center\"&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=\"id9d7b63400cc4428a185fcc5dc6707e0_7\"&gt;&lt;/div&gt;&lt;hr style=\"page-break-after:always\"&gt;&lt;div style=\"min-height:42.75pt;width:100%\"&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=\"text-align:center\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:120%\"&gt;Apple Inc.&lt;/font&gt;&lt;/div&gt;&lt;div style=\"margin-top:9pt;text-align:center\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:120%\"&gt;CONDENSED CONSOLIDATED BALANCE SHEETS (Unaudited)&lt;/font&gt;&lt;/div&gt;&lt;div style=\"text-align:center\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;(In millions, except number of shares, which are reflected in thousands, and par value)&lt;/font&gt;&lt;/div&gt;&lt;div style=\"text-align:center\"&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div style=\"text-align:justify\"&gt;&lt;table style=\"border-collapse:collapse;display:inline-table;margin-bottom:5pt;vertical-align:text-bottom;width:100.000%\"&gt;&lt;tr&gt;&lt;td style=\"width:1.0%\"&gt;&lt;/td&gt;&lt;td style=\"width:70.976%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:1.0%\"&gt;&lt;/td&gt;&lt;td style=\"width:12.496%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.530%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:1.0%\"&gt;&lt;/td&gt;&lt;td style=\"width:12.498%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-bottom:1pt solid #000;padding:2px 1pt;text-align:center;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:8pt;font-weight:700;line-height:100%\"&gt;December 30,&lt;br&gt;2023&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-bottom:1pt solid #000;padding:2px 1pt;text-align:center;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:8pt;font-weight:700;line-height:100%\"&gt;September 30,&lt;br&gt;2023&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"12\" style=\"padding:2px 1pt;text-align:center;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:100%\"&gt;ASSETS&#58;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Current assets&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Cash and cash equivalents&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;40,760&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;29,965&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Marketable securities&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;32,340&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;31,590&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Accounts receivable, net&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;23,194&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;29,508&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Vendor non-trade receivables&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;26,908&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;31,477&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Inventories&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;6,511&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;6,331&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other current assets&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;13,979&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;14,695&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:45pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total current assets&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;143,692&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;143,566&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Non-current assets&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Marketable securities&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;99,475&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;100,544&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Property, plant and equipment, net&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;43,666&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;43,715&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other non-current assets&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;66,681&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;64,758&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:45pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total non-current assets&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;209,822&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;209,017&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:63pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total assets&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;353,514&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;352,583&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:3pt double #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:3pt double #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"12\" style=\"background-color:#efefef;padding:2px 1pt;text-align:center;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:100%\"&gt;LIABILITIES AND SHAREHOLDERS&#8217; EQUITY&#58;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Current liabilities&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Accounts payable&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;58,146&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;62,611&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other current liabilities&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;54,611&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;58,829&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Deferred revenue&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;8,264&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;8,061&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Commercial paper&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;1,998&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;5,985&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Term debt&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;10,954&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;9,822&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:45pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total current liabilities&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;133,973&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;145,308&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Non-current liabilities&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Term debt&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;95,088&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;95,281&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other non-current liabilities&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;50,353&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;49,848&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:45pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total non-current liabilities&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;145,441&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;145,129&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:63pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total liabilities&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;279,414&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;290,437&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Commitments and contingencies&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:9pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Shareholders&#8217; equity&#58;&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:27pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Common stock and additional paid-in capital, $0.00001 par value&#58; 50,400,000 shares authorized&#59; 15,460,223 and 15,550,061 shares issued and outstanding, respectively&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;75,236&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;73,812&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Retained earnings&#47;(Accumulated deficit)&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;8,242&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(214)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Accumulated other comprehensive loss&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(9,378)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(11,452)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:45pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total shareholders&#8217; equity&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;74,100&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;62,146&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;div style=\"padding-left:63pt;text-indent:-9pt\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Total liabilities and shareholders&#8217; equity&lt;/font&gt;&lt;/div&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;353,514&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;352,583&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-bottom:3pt double #000000;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;div style=\"height:40.5pt;position:relative;width:100%\"&gt;&lt;div style=\"bottom:0;position:absolute;width:100%\"&gt;&lt;div style=\"text-align:center\"&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=\"id9d7b63400cc4428a185fcc5dc6707e0_10\"&gt;&lt;/div&gt;&lt;hr style=\"page-break-after:always\"&gt;&lt;div style=\"min-height:42.75pt;width:100%\"&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=\"text-align:center\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:120%\"&gt;Apple Inc.&lt;/font&gt;&lt;/div&gt;&lt;div style=\"margin-top:9pt;text-align:center\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:700;line-height:120%\"&gt;CONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS (Unaudited)&lt;/font&gt;&lt;/div&gt;&lt;div style=\"text-align:center\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:120%\"&gt;(In millions)&lt;/font&gt;&lt;/div&gt;&lt;div style=\"text-align:center\"&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div style=\"text-align:justify\"&gt;&lt;table style=\"border-collapse:collapse;display:inline-table;margin-bottom:5pt;vertical-align:text-bottom;width:100.000%\"&gt;&lt;tr&gt;&lt;td style=\"width:1.0%\"&gt;&lt;/td&gt;&lt;td style=\"width:70.976%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:1.0%\"&gt;&lt;/td&gt;&lt;td style=\"width:12.496%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.530%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;td style=\"width:1.0%\"&gt;&lt;/td&gt;&lt;td style=\"width:12.498%\"&gt;&lt;/td&gt;&lt;td style=\"width:0.1%\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"9\" style=\"padding:2px 1pt;text-align:center;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:8pt;font-weight:700;line-height:100%\"&gt;Three Months Ended&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-top:1pt solid #000000;padding:2px 1pt;text-align:center;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:8pt;font-weight:700;line-height:100%\"&gt;December 30,&lt;br&gt;2023&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"border-top:1pt solid #000000;padding:2px 1pt;text-align:center;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:8pt;font-weight:700;line-height:100%\"&gt;December 31,&lt;br&gt;2022&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Cash, cash equivalents and restricted cash, beginning balances&lt;/font&gt;&lt;/td&gt;&lt;td style=\"border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;30,737&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"border-top:1pt solid #000000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;24,977&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Operating activities&#58;&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Net income&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;33,916&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;29,998&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Adjustments to reconcile net income to cash generated by operating activities&#58;&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 37pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Depreciation and amortization&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;2,848&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;2,916&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 37pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Share-based compensation expense&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;2,997&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;2,905&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 37pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(989)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(317)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Changes in operating assets and liabilities&#58;&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 37pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Accounts receivable, net&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;6,555&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;4,275&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 37pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Vendor non-trade receivables&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;4,569&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;2,320&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 37pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Inventories&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(137)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(1,807)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 37pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other current and non-current assets&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(1,457)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(4,099)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 37pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Accounts payable&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(4,542)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(6,075)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 37pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other current and non-current liabilities&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(3,865)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;3,889&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 55pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Cash generated by operating activities&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;39,895&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;34,005&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Investing activities&#58;&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Purchases of marketable securities&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(9,780)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(5,153)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Proceeds from maturities of marketable securities&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;13,046&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;7,127&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Proceeds from sales of marketable securities&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;1,337&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;509&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Payments for acquisition of property, plant and equipment&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(2,392)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(3,787)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(284)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(141)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 55pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Cash generated by&#47;(used in) investing activities&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;1,927&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(1,445)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Financing activities&#58;&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Payments for taxes related to net share settlement of equity awards&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(2,591)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(2,316)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Payments for dividends and dividend equivalents&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(3,825)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(3,768)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Repurchases of common stock&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(20,139)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(19,475)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Repayments of term debt&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;&#8212;&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(1,401)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Repayments of commercial paper, net&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(3,984)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(8,214)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Other&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(46)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#efefef;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(389)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 55pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Cash used in financing activities&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(30,585)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(35,563)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;border-top:1pt solid #000000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;border-top:1pt solid #000000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Increase&#47;(Decrease) in cash, cash equivalents and restricted cash&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;11,237&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"2\" style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;(3,003)&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Cash, cash equivalents and restricted cash, ending balances&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;41,974&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;21,974&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#efefef;border-top:1pt solid #000;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"height:14pt\"&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:3pt double #000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;border-top:3pt double #000;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Supplemental cash flow disclosure&#58;&lt;/font&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#efefef;padding:0 1pt\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:2px 1pt 2px 19pt;text-align:left;vertical-align:top\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;Cash paid for income taxes, net&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;7,255&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"background-color:#ffffff;padding:0 1pt\"&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0 2px 1pt;text-align:left;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;$&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;font style=\"color:#000000;font-family:'Helvetica',sans-serif;font-size:9pt;font-weight:400;line-height:100%\"&gt;828&#160;&lt;/font&gt;&lt;/td&gt;&lt;td style=\"background-color:#ffffff;padding:2px 1pt 2px 0;text-align:right;vertical-align:bottom\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"display:none\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;div style=\"height:40.5pt;position:relative;width:100%\"&gt;&lt;div style=\"bottom:0;position:absolute;width:100%\"&gt;&lt;div style=\"text-align:center\"&gt;&lt;font&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\n&lt;/TEXT&gt;\n&lt;/DOCUMENT&gt;"
  },
  {
    "objectID": "posts/edgartools/index.html#scraping-edgar-at-scale",
    "href": "posts/edgartools/index.html#scraping-edgar-at-scale",
    "title": "Scrape financial data from SEC Edgar with Python and edgartools",
    "section": "Scraping Edgar at scale",
    "text": "Scraping Edgar at scale\n\nUsing get_filings() to retrieve filings\nLarge-scale data analysis requires collecting significant amounts of data from Edgar, a task that edgartools facilitates through the get_filings() method. This method allows for broad and targeted scraping activities, enabling researchers to gather extensive datasets for their empirical analyses.Here‚Äôs a basic example of how to use it to retrieve all 10-K filings for a specific year and quarter:\n\nfrom edgar import get_filings\n\n\nfilings = get_filings(year=2024, quarter=1, form=\"10-K\")\nfilings\n\n\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Filings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ        form   company                                       cik       filing_date   accession_number            ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚îÇ\n‚îÇ   0    10-K   1895 Bancorp of Wisconsin, Inc. /MD/          1847360   2024-03-29    0000950170-24-038577        ‚îÇ\n‚îÇ   1    10-K   374Water Inc.                                 933972    2024-03-29    0001654954-24-003957        ‚îÇ\n‚îÇ   2    10-K   AB Private Credit Investors Corp              1634452   2024-03-29    0001193125-24-081951        ‚îÇ\n‚îÇ   3    10-K   AEMETIS, INC                                  738214    2024-03-29    0001437749-24-010002        ‚îÇ\n‚îÇ   4    10-K   AEON Biopharma, Inc.                          1837607   2024-03-29    0001837607-24-000015        ‚îÇ\n‚îÇ   5    10-K   AMBARELLA INC                                 1280263   2024-03-29    0000950170-24-038555        ‚îÇ\n‚îÇ   6    10-K   AN2 Therapeutics, Inc.                        1880438   2024-03-29    0000950170-24-038681        ‚îÇ\n‚îÇ   7    10-K   ARYA Sciences Acquisition Corp IV             1838821   2024-03-29    0001140361-24-016377        ‚îÇ\n‚îÇ   8    10-K   ASPAC I Acquisition Corp.                     1868775   2024-03-29    0001213900-24-027387        ‚îÇ\n‚îÇ   9    10-K   ASPAC II Acquisition Corp.                    1876716   2024-03-29    0001213900-24-027570        ‚îÇ\n‚îÇ   10   10-K   ATHENA GOLD CORP                              1304409   2024-03-29    0001683168-24-001836        ‚îÇ\n‚îÇ   11   10-K   AUGUSTA GOLD CORP.                            1448597   2024-03-29    0001213900-24-027397        ‚îÇ\n‚îÇ   12   10-K   Accelerate Diagnostics, Inc                   727207    2024-03-29    0001628280-24-013670        ‚îÇ\n‚îÇ   13   10-K   Achari Ventures Holdings Corp. I              1844507   2024-03-29    0001213900-24-028085        ‚îÇ\n‚îÇ   14   10-K   Actinium Pharmaceuticals, Inc.                1388320   2024-03-29    0001213900-24-028038        ‚îÇ\n‚îÇ   15   10-K   Akari Therapeutics Plc                        1541157   2024-03-29    0000950170-24-038661        ‚îÇ\n‚îÇ   16   10-K   AltC Acquisition Corp.                        1849056   2024-03-29    0001410578-24-000365        ‚îÇ\n‚îÇ   17   10-K   Altegris Winton Futures Fund, L.P.            1198415   2024-03-29    0001683168-24-001869        ‚îÇ\n‚îÇ   18   10-K   Altisource Asset Management Corp              1555074   2024-03-29    0001555074-24-000013        ‚îÇ\n‚îÇ   19   10-K   Annovis Bio, Inc.                             1477845   2024-03-29    0001558370-24-004393        ‚îÇ\n‚îÇ   20   10-K   Atlantic Coastal Acquisition Corp. II         1893219   2024-03-29    0001193125-24-081194        ‚îÇ\n‚îÇ   21   10-K   Avalo Therapeutics, Inc.                      1534120   2024-03-29    0001628280-24-013786        ‚îÇ\n‚îÇ   22   10-K   BANK 2017-BNK5                                1706303   2024-03-29    0001888524-24-005208        ‚îÇ\n‚îÇ   23   10-K   BANK 2017-BNK8                                1718322   2024-03-29    0001888524-24-005184        ‚îÇ\n‚îÇ   24   10-K   BANK 2018-BNK11                               1731627   2024-03-29    0001888524-24-005222        ‚îÇ\n‚îÇ   25   10-K   BANK 2018-BNK14                               1749973   2024-03-29    0001888524-24-005182        ‚îÇ\n‚îÇ   26   10-K   BANK 2019-BNK17                               1769961   2024-03-29    0001888524-24-005225        ‚îÇ\n‚îÇ   27   10-K   BANK 2019-BNK20                               1784958   2024-03-29    0001888524-24-005194        ‚îÇ\n‚îÇ   28   10-K   BANK 2019-BNK23                               1792414   2024-03-29    0001888524-24-005198        ‚îÇ\n‚îÇ   29   10-K   BANK 2020-BNK26                               1803308   2024-03-29    0001888524-24-005318        ‚îÇ\n‚îÇ   30   10-K   BANK 2020-BNK29                               1830315   2024-03-29    0001888524-24-005228        ‚îÇ\n‚îÇ   31   10-K   BANK 2021-BNK32                               1848411   2024-03-29    0001888524-24-005233        ‚îÇ\n‚îÇ   32   10-K   BANK 2021-BNK35                               1872347   2024-03-29    0001888524-24-005323        ‚îÇ\n‚îÇ   33   10-K   BANK 2021-BNK38                               1895567   2024-03-29    0001888524-24-005330        ‚îÇ\n‚îÇ   34   10-K   BANK 2022-BNK41                               1920442   2024-03-29    0001888524-24-005328        ‚îÇ\n‚îÇ   35   10-K   BANK 2022-BNK44                               1950141   2024-03-29    0001888524-24-005332        ‚îÇ\n‚îÇ   36   10-K   BANK5 2023-5YR2                               1979748   2024-03-29    0001888524-24-005350        ‚îÇ\n‚îÇ   37   10-K   BENCHMARK 2018-B3 COMMERCIAL MORTGAGE TRUST   1734103   2024-03-29    0000950170-24-038358        ‚îÇ\n‚îÇ   38   10-K   BENCHMARK 2018-B6 MORTGAGE TRUST              1751874   2024-03-29    0000950170-24-038337        ‚îÇ\n‚îÇ   39   10-K   BENCHMARK 2019-B9 MORTGAGE TRUST              1764759   2024-03-29    0000950170-24-038247        ‚îÇ\n‚îÇ   40   10-K   BENCHMARK 2020-B19 MORTGAGE TRUST             1821946   2024-03-29    0000950170-24-038347        ‚îÇ\n‚îÇ   41   10-K   BENCHMARK 2021-B23 MORTGAGE TRUST             1840644   2024-03-29    0000950170-24-038315        ‚îÇ\n‚îÇ   42   10-K   BENCHMARK 2021-B27 MORTGAGE TRUST             1862080   2024-03-29    0000950170-24-038316        ‚îÇ\n‚îÇ   43   10-K   BENCHMARK 2021-B31 MORTGAGE TRUST             1894714   2024-03-29    0000950170-24-038327        ‚îÇ\n‚îÇ   44   10-K   BENCHMARK 2022-B35 MORTGAGE TRUST             1921064   2024-03-29    0001888524-24-005316        ‚îÇ\n‚îÇ   45   10-K   BMO 2022-C1 Mortgage Trust                    1903688   2024-03-29    0001888524-24-005311        ‚îÇ\n‚îÇ   46   10-K   BMO 2022-C2 Mortgage Trust                    1932997   2024-03-29    0001888524-24-005308        ‚îÇ\n‚îÇ   47   10-K   BMO 2022-C3 Mortgage Trust                    1942599   2024-03-29    0001888524-24-005312        ‚îÇ\n‚îÇ   48   10-K   BMO 2023-5C1 Mortgage Trust                   1984246   2024-03-29    0001888524-24-005300        ‚îÇ\n‚îÇ   49   10-K   BMO 2023-5C2 Mortgage Trust                   1994754   2024-03-29    0001888524-24-005302        ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ Showing 50 of 5,149 filings                                                                                     ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\n\n\nHandling the SEC API rate limit\nThe SEC imposes rate limits on its APIs to prevent excessive use that could impact system performance. When scraping Edgar at scale, it‚Äôs crucial to be aware of these limits to avoid service interruptions. edgartools is quite good at respecting these limits, but I have found that it does happen that you might get denied access to the API for time to time. This is why it‚Äôs important to handle these exceptions gracefully in your code, and to properly log and monitor your scraping activities so that you can revisit any filings that were missed due to rate limiting or other errors.\nIf you want to make your scraping more robust, you can use tools like tenacity to automatically retry failed requests."
  },
  {
    "objectID": "posts/edgartools/index.html#exploring-other-form-types",
    "href": "posts/edgartools/index.html#exploring-other-form-types",
    "title": "Scrape financial data from SEC Edgar with Python and edgartools",
    "section": "Exploring other form types",
    "text": "Exploring other form types\nWhile 10-Ks and 10-Qs are among the most commonly analyzed filings, Edgar hosts a variety of form types, each offering unique insights into different aspects of finance research. edgartools provides a simple way to access and parse automatically a wide range of form types. At the time of writing, the library supports the following form types:\n\n\n\n\n\n\n\n\nForm\nData Object\nDescription\n\n\n\n\n10-K\nTenK\nAnnual report\n\n\n10-Q\nTenQ\nQuarterly report\n\n\n8-K\nEightK\nCurrent report\n\n\nMA-I\nMunicipalAdvisorForm\nMunicipal advisor initial filing\n\n\nForm 144\nForm144\nNotice of proposed sale of securities\n\n\nC, C-U, C-AR, C-TR\nFormC\nForm C Crowdfunding Offering\n\n\nD\nFormD\nForm D Offering\n\n\n3,4,5\nOwnership\nOwnership reports\n\n\n13F-HR\nThirteenF\n13F Holdings Report\n\n\nNPORT-P\nFundReport\nFund Report\n\n\nEFFECT\nEffect\nNotice of Effectiveness\n\n\nAnd other filing with XBRL\nFilingXbrl\n\n\n\n\nFor example, another commonly analyzed form is the 13F-HR, which discloses institutional holdings of publicly traded companies. Here‚Äôs how you can retrieve the latest 13F-HR filing for a company:\n\nfilings = get_filings(form=\"13F-HR\")\n\n\nfilings\n\n\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Filings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ        form     company                                    cik       filing_date   accession_number             ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÇ\n‚îÇ   0    13F-HR   AGINCOURT CAPITAL MANAGEMENT, LLC          1845254   2024-04-05    0001845254-24-000004         ‚îÇ\n‚îÇ   1    13F-HR   BUTENSKY & COHEN FINANCIAL SECURITY, INC   1632802   2024-04-05    0001214659-24-006203         ‚îÇ\n‚îÇ   2    13F-HR   CHICKASAW CAPITAL MANAGEMENT LLC           1276460   2024-04-05    0001276460-24-000003         ‚îÇ\n‚îÇ   3    13F-HR   CISCO SYSTEMS, INC.                        858877    2024-04-05    0000950123-24-003100         ‚îÇ\n‚îÇ   4    13F-HR   CWM, LLC                                   1535847   2024-04-05    0001580642-24-002001         ‚îÇ\n‚îÇ   5    13F-HR   Canaan Partners IX LLC                     1654586   2024-04-05    0000950123-24-003101         ‚îÇ\n‚îÇ   6    13F-HR   Canaan Partners XI LLC                     1850694   2024-04-05    0000950123-24-003102         ‚îÇ\n‚îÇ   7    13F-HR   Capital Market Strategies LLC              1810158   2024-04-05    0001810158-24-000002         ‚îÇ\n‚îÇ   8    13F-HR   Carolina Wealth Advisors, LLC              1786411   2024-04-05    0001437749-24-011215         ‚îÇ\n‚îÇ   9    13F-HR   Compass Wealth Management LLC              1965653   2024-04-05    0001965653-24-000003         ‚îÇ\n‚îÇ   10   13F-HR   Davis Capital Management                   1811806   2024-04-05    0001811806-24-000002         ‚îÇ\n‚îÇ   11   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044028         ‚îÇ\n‚îÇ   12   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044029         ‚îÇ\n‚îÇ   13   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044030         ‚îÇ\n‚îÇ   14   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044031         ‚îÇ\n‚îÇ   15   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044032         ‚îÇ\n‚îÇ   16   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044033         ‚îÇ\n‚îÇ   17   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044034         ‚îÇ\n‚îÇ   18   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044035         ‚îÇ\n‚îÇ   19   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044036         ‚îÇ\n‚îÇ   20   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044037         ‚îÇ\n‚îÇ   21   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044038         ‚îÇ\n‚îÇ   22   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044039         ‚îÇ\n‚îÇ   23   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044040         ‚îÇ\n‚îÇ   24   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044041         ‚îÇ\n‚îÇ   25   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044043         ‚îÇ\n‚îÇ   26   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044044         ‚îÇ\n‚îÇ   27   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044045         ‚îÇ\n‚îÇ   28   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044046         ‚îÇ\n‚îÇ   29   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044047         ‚îÇ\n‚îÇ   30   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044048         ‚îÇ\n‚îÇ   31   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044049         ‚îÇ\n‚îÇ   32   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044050         ‚îÇ\n‚îÇ   33   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044051         ‚îÇ\n‚îÇ   34   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044052         ‚îÇ\n‚îÇ   35   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044054         ‚îÇ\n‚îÇ   36   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044055         ‚îÇ\n‚îÇ   37   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044056         ‚îÇ\n‚îÇ   38   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044058         ‚îÇ\n‚îÇ   39   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044060         ‚îÇ\n‚îÇ   40   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044061         ‚îÇ\n‚îÇ   41   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044062         ‚îÇ\n‚îÇ   42   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044063         ‚îÇ\n‚îÇ   43   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044064         ‚îÇ\n‚îÇ   44   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044065         ‚îÇ\n‚îÇ   45   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044066         ‚îÇ\n‚îÇ   46   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044067         ‚îÇ\n‚îÇ   47   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044068         ‚îÇ\n‚îÇ   48   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044069         ‚îÇ\n‚îÇ   49   13F-HR   E. Ohman J:or Asset Management AB          2009396   2024-04-05    0001104659-24-044070         ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ Showing 50 of 251 filings                                                                                       ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nAgain, the process is the same. You get a list of filings, then retreive the actual filing content using the obj() method.\n\nfiling = filings[0]\n\n\nreport = filing.obj()\n\n\nreport\n\n\n\n\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 13F-HR Holding Report for AGINCOURT CAPITAL MANAGEMENT, LLC for period 2024-03-31 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ   Report Period   Investment Manager   Signed By     Holdings   Value          Accession Number    Filed        ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ\n‚îÇ   2024-03-31      AGINCOURT CAPITAL    Erika Banks   4          $106,224,110   0001845254-24-00‚Ä¶   2024-04-05   ‚îÇ\n‚îÇ                   MANAGEMENT, LLC                                                                               ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ       Issuer                 Class              Cusip       Ticker   Value         Type     Shares    Voting    ‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ\n‚îÇ   0   VANGUARD INTL EQUITY   TT WRLD ST ETF     922042742   VT       $57,443,978   Shares   519,855   519,855   ‚îÇ\n‚îÇ       INDEX F                                                                                                   ‚îÇ\n‚îÇ   1   ISHARES TR             CORE INTL AGGR     46435G672   IAGG     $22,440,537   Shares   449,260   449,260   ‚îÇ\n‚îÇ   2   ISHARES TR             CORE US AGGBD ET   464287226   AGG      $15,556,300   Shares   158,835   158,835   ‚îÇ\n‚îÇ   3   ISHARES TR             CRE U S REIT ETF   464288521   USRT     $10,783,295   Shares   200,433   200,433   ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 13F-HR Holding Report for AGINCOURT CAPITAL MANAGEMENT, LLC for period 2024-03-31 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nIn the case of the 13F-HR, the holdings are stored in an infotable attribute, which is a a pandas DataFrame:\n\nreport.has_infotable()\n\nTrue\n\n\n\nreport.infotable\n\n\n\n\n\n\n\n\nIssuer\nClass\nCusip\nValue\nSharesPrnAmount\nType\nPutCall\nInvestmentDiscretion\nSoleVoting\nSharedVoting\nNonVoting\nTicker\n\n\n\n\n0\nISHARES TR\nCORE US AGGBD ET\n464287226\n15556300\n158835\nShares\nNone\nSOLE\n158835\n0\n0\nAGG\n\n\n1\nISHARES TR\nCRE U S REIT ETF\n464288521\n10783295\n200433\nShares\nNone\nSOLE\n200433\n0\n0\nUSRT\n\n\n2\nISHARES TR\nCORE INTL AGGR\n46435G672\n22440537\n449260\nShares\nNone\nSOLE\n449260\n0\n0\nIAGG\n\n\n3\nVANGUARD INTL EQUITY INDEX F\nTT WRLD ST ETF\n922042742\n57443978\n519855\nShares\nNone\nSOLE\n519855\n0\n0\nVT"
  },
  {
    "objectID": "posts/edgartools/index.html#final-thoughts",
    "href": "posts/edgartools/index.html#final-thoughts",
    "title": "Scrape financial data from SEC Edgar with Python and edgartools",
    "section": "Final thoughts",
    "text": "Final thoughts\nThe edgartools library is a powerful tool to access and analyze financial data from the SEC‚Äôs EDGAR database. Even with the tremendous amount of work that went into creating this library, many bugs and edge cases remain, which highlights the complexity of working with financial data. However, the library is actively maintained and updated, with bug fixes, new features, and improvements being added regularly. Dwight Gunning, the creator and maintainer, is very reponsive to issues and pull requests. Of all the (many) libraries I‚Äôve used, edgartools is by far the best choice for working with SEC Edgar data in Python in my opinion. Note that I have not tried any of the commercial alternatives, so I can‚Äôt compare them to edgartools."
  },
  {
    "objectID": "posts/pandas-read-csv/index.html",
    "href": "posts/pandas-read-csv/index.html",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "",
    "text": "CSV files are ubiquitous in empirical finance and data science more generally. They are easy to create and read and are supported by most data processing tools. The pandas.read_csv() function is flexible and easy to use, but it can be slow and memory-intensive for large files if not used properly.\nIn this post, I will show you how to read and parse large CSV files efficiently in Python using the pyarrow library with pandas. I will also show you how to convert the CSV file to the Parquet format, which is a columnar storage format that is more efficient than CSV for reading and writing data."
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#video-tutorial",
    "href": "posts/pandas-read-csv/index.html#video-tutorial",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Video tutorial",
    "text": "Video tutorial\nThis post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#apache-arrow-and-the-pyarrow-library",
    "href": "posts/pandas-read-csv/index.html#apache-arrow-and-the-pyarrow-library",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Apache Arrow and the pyarrow library",
    "text": "Apache Arrow and the pyarrow library\nAccording to the website,\n\nApache Arrow is a software development platform for building high performance applications that process and transport large data sets. It is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system (or programming language to another).\nA critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (included nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more.\n\nLet‚Äôs unpack this a bit:\n\nArrow defines a standard way to represent structured data (think table or dataframe) in memory. This is important because:\n\nDevelopers can build tools and libraries using Arrow that are compatible with each other and they do not have to design everything from scratch.\nIt allows different systems and programming languages to share data without having to convert it to a different format.\nIt is designed to be efficient and fast, which is important for large datasets and high-performance computing.\n\nThe data is stored in columns, which is different from the row-based storage of CSV files and most databases. This is important because:\n\nColumnar storage is more efficient for many types of data processing, especially for analytics and data science.\n\nArrow has a rich data type system, which is important because:\n\nData can be represented in a standardized way, which makes it easier for interoperability between different systems and programming languages.\nData can be represented in a way that is more efficient and flexible.\nIt can represent complex data structures, such as nested data and user-defined data types.\n\n\nArrow provides a set of libraries for different programming languages, including C++, Java, Python, and R. The pyarrow library is the Python implementation of Arrow. It provides a way to work with Arrow data in Python, including converting data between Arrow and other formats, such as CSV and Parquet. A big change that came with pandas 2.0 is thatpyarrow can now be used with pandas not only to read and write data, but also as a dtype-backend, which means that pandas can use the Arrow in-memory format to store and process data.\nOf note, Wes McKinney who created pandas is also a co-creator of Apache Arrow.\n\nimport pandas as pd\nimport pyarrow as pa\nimport time\n\nprint(f\"pandas version: {pd.__version__}\")\nprint(f\"pyarrow version: {pa.__version__}\")\n\npandas version: 2.1.4\npyarrow version: 14.0.1"
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#the-problem-reading-large-csv-files-in-python",
    "href": "posts/pandas-read-csv/index.html#the-problem-reading-large-csv-files-in-python",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "The problem: reading large CSV files in Python",
    "text": "The problem: reading large CSV files in Python\nReading large CSV files in Python with pandas can take time and slow down your workflow, especially if it is done repeatedly.\nAs an example, I use a large CSV file containing daily stock information such as prices, returns, and trading volumes for publicly listed U.S. stocks from the Center for Research in Security Prices (CRSP) that I obtained from Wharton Research Data Services (WRDS). The file contains 8,883,156 rows and 64 columns of different data types. The original file is a CSV file compressed in the gzip format (.csv.gz) that is 486.4 MB in size. The uncompressed file is 3.91 GB in size.\nThe timing results presented in this post are obtained on a MacBook Pro with a M3 Max processor and 64 GB of RAM. If doing things properly, RAM should not be a limiting factor for reading and processing the sample file. My tips will focus on speed first and memory usage second.\nFirst, let‚Äôs take a look at the first few rows of the file to understand its structure:\n\ndf = pd.read_csv(\"files/crsp_dsf.csv\", nrows=100)\ndf.head()\n\n\n\n\n\n\n\n\nPERMNO\nSecInfoStartDt\nSecInfoEndDt\nSecurityBegDt\nSecurityEndDt\nSecurityHdrFlg\nHdrCUSIP\nHdrCUSIP9\nCUSIP\nCUSIP9\n...\nDlyVol\nDlyClose\nDlyLow\nDlyHigh\nDlyBid\nDlyAsk\nDlyOpen\nDlyNumTrd\nDlyMMCnt\nDlyPrcVol\n\n\n\n\n0\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n88291\n181.68\n181.440\n185.30\n181.67\n181.70\n185.30\n2300\n44\n16040708.9\n\n\n1\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n71463\n184.91\n180.890\n185.17\n184.90\n184.91\n180.89\n1572\n44\n13214223.3\n\n\n2\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n70308\n185.07\n183.140\n185.52\n184.77\n185.07\n184.18\n1764\n44\n13011901.6\n\n\n3\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n72267\n183.03\n182.475\n184.63\n182.90\n183.22\n184.40\n2032\n44\n13227029.0\n\n\n4\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n118592\n182.04\n181.810\n183.89\n182.04\n182.23\n182.73\n2208\n45\n21588487.7\n\n\n\n\n5 rows √ó 64 columns"
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#tip-1-keep-your-csv-files-compressed",
    "href": "posts/pandas-read-csv/index.html#tip-1-keep-your-csv-files-compressed",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Tip 1: Keep your CSV files compressed",
    "text": "Tip 1: Keep your CSV files compressed\nReading a large CSV file with pandas is slow, but it is not slower if compressed. Compressed files take less space on disk and are sometimes faster to read. In my experience, if you are using a modern computer with a fast SSD, reading compressed and uncompressed files is about the same speed, but if you are using a hard drive or other slower storage, reading compressed files will often be faster.\nThe gzip format is a good choice for compressing CSV files. It is supported by most data processing tools and is fast to read and write. I found that the xz format is even better in terms of compression ratio for relatively similar performance, but it is not as widely supported as gzip.\n\nx = time.time()\ndf = pd.read_csv(\"files/crsp_dsf.csv\")\nprint(f\"Reading uncompressed file: {time.time() - x:.4f} seconds\")\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_81162/1706856631.py:2: DtypeWarning: Columns (6,7,8,9,12,15,22,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"files/crsp_dsf.csv\")\n\n\nReading uncompressed file: 23.8861 seconds\n\n\n\nx = time.time()\ndf = pd.read_csv(\"files/crsp_dsf.csv.gz\")\nprint(f\"Reading compressed file: {time.time() - x:.4f} seconds\")\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_81162/3793337616.py:2: DtypeWarning: Columns (6,7,8,9,12,15,22,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"files/crsp_dsf.csv.gz\")\n\n\nReading compressed file: 25.3693 seconds\n\n\nThe times are very similar. Looking at the resulting DataFrame, we can see that the file contains 64 columns of different data types. The first few columns are identifiers such as the stock ticker, the date, and the exchange code. The remaining columns contain information about the stock such as prices, returns, and trading volumes. We can also see that the parsing of the file is incomplete as the dates are all of the object type. We will fix this shortly.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 64 columns):\n #   Column             Dtype  \n---  ------             -----  \n 0   PERMNO             int64  \n 1   SecInfoStartDt     object \n 2   SecInfoEndDt       object \n 3   SecurityBegDt      object \n 4   SecurityEndDt      object \n 5   SecurityHdrFlg     object \n 6   HdrCUSIP           object \n 7   HdrCUSIP9          object \n 8   CUSIP              object \n 9   CUSIP9             object \n 10  PrimaryExch        object \n 11  ConditionalType    object \n 12  ExchangeTier       object \n 13  TradingStatusFlg   object \n 14  SecurityNm         object \n 15  ShareClass         object \n 16  USIncFlg           object \n 17  IssuerType         object \n 18  SecurityType       object \n 19  SecuritySubType    object \n 20  ShareType          object \n 21  SecurityActiveFlg  object \n 22  DelActionType      object \n 23  DelStatusType      object \n 24  DelReasonType      object \n 25  DelPaymentType     object \n 26  Ticker             object \n 27  TradingSymbol      object \n 28  PERMCO             int64  \n 29  SICCD              int64  \n 30  NAICS              int64  \n 31  ICBIndustry        object \n 32  IssuerNm           object \n 33  YYYYMMDD           int64  \n 34  DlyCalDt           object \n 35  DlyDelFlg          object \n 36  DlyPrc             float64\n 37  DlyPrcFlg          object \n 38  DlyCap             float64\n 39  DlyCapFlg          object \n 40  DlyPrevPrc         float64\n 41  DlyPrevPrcFlg      object \n 42  DlyPrevDt          object \n 43  DlyPrevCap         float64\n 44  DlyPrevCapFlg      object \n 45  DlyRet             float64\n 46  DlyRetx            float64\n 47  DlyRetI            float64\n 48  DlyRetMissFlg      object \n 49  DlyRetDurFlg       object \n 50  DlyOrdDivAmt       float64\n 51  DlyNonOrdDivAmt    float64\n 52  DlyFacPrc          float64\n 53  DlyDistRetFlg      object \n 54  DlyVol             float64\n 55  DlyClose           float64\n 56  DlyLow             float64\n 57  DlyHigh            float64\n 58  DlyBid             float64\n 59  DlyAsk             float64\n 60  DlyOpen            float64\n 61  DlyNumTrd          float64\n 62  DlyMMCnt           float64\n 63  DlyPrcVol          float64\ndtypes: float64(20), int64(5), object(39)\nmemory usage: 4.2+ GB\n\n\n\ndf\n\n\n\n\n\n\n\n\nPERMNO\nSecInfoStartDt\nSecInfoEndDt\nSecurityBegDt\nSecurityEndDt\nSecurityHdrFlg\nHdrCUSIP\nHdrCUSIP9\nCUSIP\nCUSIP9\n...\nDlyVol\nDlyClose\nDlyLow\nDlyHigh\nDlyBid\nDlyAsk\nDlyOpen\nDlyNumTrd\nDlyMMCnt\nDlyPrcVol\n\n\n\n\n0\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n88291.0\n181.68\n181.440\n185.30\n181.67\n181.70\n185.30\n2300.0\n44.0\n1.604071e+07\n\n\n1\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n71463.0\n184.91\n180.890\n185.17\n184.90\n184.91\n180.89\n1572.0\n44.0\n1.321422e+07\n\n\n2\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n70308.0\n185.07\n183.140\n185.52\n184.77\n185.07\n184.18\n1764.0\n44.0\n1.301190e+07\n\n\n3\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n72267.0\n183.03\n182.475\n184.63\n182.90\n183.22\n184.40\n2032.0\n44.0\n1.322703e+07\n\n\n4\n10026\n2019-08-05\n2021-04-26\n1986-02-04\n2023-12-29\nN\n46603210\n466032109\n46603210\n466032109\n...\n118592.0\n182.04\n181.810\n183.89\n182.04\n182.23\n182.73\n2208.0\n45.0\n2.158849e+07\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8883151\n93436\n2023-10-01\n2023-12-29\n2010-06-29\n2023-12-29\nY\n88160R10\n88160R101\n88160R10\n88160R101\n...\n93148541.0\n252.54\n251.370\n258.22\n252.53\n252.54\n256.76\n972876.0\n54.0\n2.352373e+10\n\n\n8883152\n93436\n2023-10-01\n2023-12-29\n2010-06-29\n2023-12-29\nY\n88160R10\n88160R101\n88160R10\n88160R101\n...\n86700724.0\n256.61\n252.910\n257.97\n256.66\n256.71\n254.49\n954492.0\n54.0\n2.224827e+10\n\n\n8883153\n93436\n2023-10-01\n2023-12-29\n2010-06-29\n2023-12-29\nY\n88160R10\n88160R101\n88160R10\n88160R101\n...\n106250779.0\n261.44\n257.520\n263.34\n261.52\n261.56\n258.35\n1046971.0\n54.0\n2.777820e+10\n\n\n8883154\n93436\n2023-10-01\n2023-12-29\n2010-06-29\n2023-12-29\nY\n88160R10\n88160R101\n88160R10\n88160R101\n...\n113250680.0\n253.18\n252.710\n265.13\n253.08\n253.19\n263.66\n1166971.0\n54.0\n2.867281e+10\n\n\n8883155\n93436\n2023-10-01\n2023-12-29\n2010-06-29\n2023-12-29\nY\n88160R10\n88160R101\n88160R10\n88160R101\n...\n100321201.0\n248.48\n247.430\n255.19\n248.48\n248.49\n255.10\n1062240.0\n54.0\n2.492781e+10\n\n\n\n\n8883156 rows √ó 64 columns"
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#tip-2-use-the-pyarrow-library-with-pandas",
    "href": "posts/pandas-read-csv/index.html#tip-2-use-the-pyarrow-library-with-pandas",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Tip 2: Use the pyarrow library with pandas",
    "text": "Tip 2: Use the pyarrow library with pandas\nThe c engine was historically the fastest engine for reading CSV files in pandas. It is the default engine and is written in C. The python engine is the slowest engine and is written in Python, and is used when the user specifies parameters that are not supported by the c engine. Pandas 2.0 added support for the pyarrow engine is a new engine that leverages the pyarrow library. It can be faster and more memory efficient than the c engine because it supports multi-threading. Let‚Äôs compare the performance of the c engine with the pyarrow engine for reading the sample file (I tried with the python, but I got tired of waiting‚Ä¶). Note that the pyarrow engine should be faster when using the pyarrow dtype backend, so we will compare the performance of the two engines with the default, the numpy_nullable and the pyarrow dtype backends.\n\ndate_cols = [\n    \"SecInfoStartDt\",\n    \"SecInfoEndDt\",\n    \"SecurityBegDt\",\n    \"SecurityEndDt\",\n    \"YYYYMMDD\",\n    \"DlyCalDt\",\n    \"DlyPrevDt\",\n]\n\n\nx = time.time()\ndf = pd.read_csv(\"files/crsp_dsf.csv.gz\", parse_dates=date_cols, engine=\"c\")\ntime_c_default = time.time() - x\ndf.info()\nprint(f\"Reading using the c engine and default backend: {time_c_default:.4f} seconds\")\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_81162/3729689134.py:2: DtypeWarning: Columns (6,7,8,9,12,15,22,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"files/crsp_dsf.csv.gz\", parse_dates=date_cols, engine=\"c\")\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 64 columns):\n #   Column             Dtype         \n---  ------             -----         \n 0   PERMNO             int64         \n 1   SecInfoStartDt     datetime64[ns]\n 2   SecInfoEndDt       datetime64[ns]\n 3   SecurityBegDt      datetime64[ns]\n 4   SecurityEndDt      datetime64[ns]\n 5   SecurityHdrFlg     object        \n 6   HdrCUSIP           object        \n 7   HdrCUSIP9          object        \n 8   CUSIP              object        \n 9   CUSIP9             object        \n 10  PrimaryExch        object        \n 11  ConditionalType    object        \n 12  ExchangeTier       object        \n 13  TradingStatusFlg   object        \n 14  SecurityNm         object        \n 15  ShareClass         object        \n 16  USIncFlg           object        \n 17  IssuerType         object        \n 18  SecurityType       object        \n 19  SecuritySubType    object        \n 20  ShareType          object        \n 21  SecurityActiveFlg  object        \n 22  DelActionType      object        \n 23  DelStatusType      object        \n 24  DelReasonType      object        \n 25  DelPaymentType     object        \n 26  Ticker             object        \n 27  TradingSymbol      object        \n 28  PERMCO             int64         \n 29  SICCD              int64         \n 30  NAICS              int64         \n 31  ICBIndustry        object        \n 32  IssuerNm           object        \n 33  YYYYMMDD           datetime64[ns]\n 34  DlyCalDt           datetime64[ns]\n 35  DlyDelFlg          object        \n 36  DlyPrc             float64       \n 37  DlyPrcFlg          object        \n 38  DlyCap             float64       \n 39  DlyCapFlg          object        \n 40  DlyPrevPrc         float64       \n 41  DlyPrevPrcFlg      object        \n 42  DlyPrevDt          datetime64[ns]\n 43  DlyPrevCap         float64       \n 44  DlyPrevCapFlg      object        \n 45  DlyRet             float64       \n 46  DlyRetx            float64       \n 47  DlyRetI            float64       \n 48  DlyRetMissFlg      object        \n 49  DlyRetDurFlg       object        \n 50  DlyOrdDivAmt       float64       \n 51  DlyNonOrdDivAmt    float64       \n 52  DlyFacPrc          float64       \n 53  DlyDistRetFlg      object        \n 54  DlyVol             float64       \n 55  DlyClose           float64       \n 56  DlyLow             float64       \n 57  DlyHigh            float64       \n 58  DlyBid             float64       \n 59  DlyAsk             float64       \n 60  DlyOpen            float64       \n 61  DlyNumTrd          float64       \n 62  DlyMMCnt           float64       \n 63  DlyPrcVol          float64       \ndtypes: datetime64[ns](7), float64(20), int64(4), object(33)\nmemory usage: 4.2+ GB\nReading using the c engine and default backend: 28.3934 seconds\n\n\n\nx = time.time()\ndf = pd.read_csv(\n    \"files/crsp_dsf.csv.gz\",\n    parse_dates=date_cols,\n    engine=\"c\",\n    dtype_backend=\"numpy_nullable\",\n)\ntime_c_np = time.time() - x\ndf.info()\nprint(f\"Reading using the c engine and numpy_nullable backend: {time_c_np:.4f} seconds\")\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_81162/1999027842.py:2: DtypeWarning: Columns (6,7,8,9,12,15,22,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 64 columns):\n #   Column             Dtype         \n---  ------             -----         \n 0   PERMNO             Int64         \n 1   SecInfoStartDt     datetime64[ns]\n 2   SecInfoEndDt       datetime64[ns]\n 3   SecurityBegDt      datetime64[ns]\n 4   SecurityEndDt      datetime64[ns]\n 5   SecurityHdrFlg     string        \n 6   HdrCUSIP           object        \n 7   HdrCUSIP9          object        \n 8   CUSIP              object        \n 9   CUSIP9             object        \n 10  PrimaryExch        string        \n 11  ConditionalType    string        \n 12  ExchangeTier       object        \n 13  TradingStatusFlg   string        \n 14  SecurityNm         string        \n 15  ShareClass         object        \n 16  USIncFlg           string        \n 17  IssuerType         string        \n 18  SecurityType       string        \n 19  SecuritySubType    string        \n 20  ShareType          string        \n 21  SecurityActiveFlg  string        \n 22  DelActionType      object        \n 23  DelStatusType      string        \n 24  DelReasonType      string        \n 25  DelPaymentType     string        \n 26  Ticker             string        \n 27  TradingSymbol      string        \n 28  PERMCO             Int64         \n 29  SICCD              Int64         \n 30  NAICS              Int64         \n 31  ICBIndustry        string        \n 32  IssuerNm           string        \n 33  YYYYMMDD           datetime64[ns]\n 34  DlyCalDt           datetime64[ns]\n 35  DlyDelFlg          string        \n 36  DlyPrc             Float64       \n 37  DlyPrcFlg          string        \n 38  DlyCap             Float64       \n 39  DlyCapFlg          string        \n 40  DlyPrevPrc         Float64       \n 41  DlyPrevPrcFlg      string        \n 42  DlyPrevDt          object        \n 43  DlyPrevCap         Float64       \n 44  DlyPrevCapFlg      string        \n 45  DlyRet             Float64       \n 46  DlyRetx            Float64       \n 47  DlyRetI            Float64       \n 48  DlyRetMissFlg      object        \n 49  DlyRetDurFlg       string        \n 50  DlyOrdDivAmt       Float64       \n 51  DlyNonOrdDivAmt    Float64       \n 52  DlyFacPrc          Float64       \n 53  DlyDistRetFlg      string        \n 54  DlyVol             Int64         \n 55  DlyClose           Float64       \n 56  DlyLow             Float64       \n 57  DlyHigh            Float64       \n 58  DlyBid             Float64       \n 59  DlyAsk             Float64       \n 60  DlyOpen            Float64       \n 61  DlyNumTrd          Int64         \n 62  DlyMMCnt           Int64         \n 63  DlyPrcVol          Float64       \ndtypes: Float64(17), Int64(7), datetime64[ns](6), object(9), string(25)\nmemory usage: 4.4+ GB\nReading using the c engine and numpy_nullable backend: 34.7816 seconds\n\n\n\nx = time.time()\ndf = pd.read_csv(\n    \"files/crsp_dsf.csv.gz\", parse_dates=date_cols, engine=\"c\", dtype_backend=\"pyarrow\"\n)\ntime_c_arrow = time.time() - x\ndf.info()\nprint(f\"Reading using the c engine and pyarrow backend: {time_c_arrow:.4f} seconds\")\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_81162/2542695645.py:2: DtypeWarning: Columns (6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 64 columns):\n #   Column             Dtype          \n---  ------             -----          \n 0   PERMNO             int64[pyarrow] \n 1   SecInfoStartDt     datetime64[ns] \n 2   SecInfoEndDt       datetime64[ns] \n 3   SecurityBegDt      datetime64[ns] \n 4   SecurityEndDt      datetime64[ns] \n 5   SecurityHdrFlg     string[pyarrow]\n 6   HdrCUSIP           object         \n 7   HdrCUSIP9          object         \n 8   CUSIP              object         \n 9   CUSIP9             object         \n 10  PrimaryExch        string[pyarrow]\n 11  ConditionalType    string[pyarrow]\n 12  ExchangeTier       string[pyarrow]\n 13  TradingStatusFlg   string[pyarrow]\n 14  SecurityNm         string[pyarrow]\n 15  ShareClass         string[pyarrow]\n 16  USIncFlg           string[pyarrow]\n 17  IssuerType         string[pyarrow]\n 18  SecurityType       string[pyarrow]\n 19  SecuritySubType    string[pyarrow]\n 20  ShareType          string[pyarrow]\n 21  SecurityActiveFlg  string[pyarrow]\n 22  DelActionType      string[pyarrow]\n 23  DelStatusType      string[pyarrow]\n 24  DelReasonType      string[pyarrow]\n 25  DelPaymentType     string[pyarrow]\n 26  Ticker             string[pyarrow]\n 27  TradingSymbol      string[pyarrow]\n 28  PERMCO             int64[pyarrow] \n 29  SICCD              int64[pyarrow] \n 30  NAICS              int64[pyarrow] \n 31  ICBIndustry        string[pyarrow]\n 32  IssuerNm           string[pyarrow]\n 33  YYYYMMDD           datetime64[ns] \n 34  DlyCalDt           datetime64[ns] \n 35  DlyDelFlg          string[pyarrow]\n 36  DlyPrc             double[pyarrow]\n 37  DlyPrcFlg          string[pyarrow]\n 38  DlyCap             double[pyarrow]\n 39  DlyCapFlg          string[pyarrow]\n 40  DlyPrevPrc         double[pyarrow]\n 41  DlyPrevPrcFlg      string[pyarrow]\n 42  DlyPrevDt          object         \n 43  DlyPrevCap         double[pyarrow]\n 44  DlyPrevCapFlg      string[pyarrow]\n 45  DlyRet             double[pyarrow]\n 46  DlyRetx            double[pyarrow]\n 47  DlyRetI            double[pyarrow]\n 48  DlyRetMissFlg      string[pyarrow]\n 49  DlyRetDurFlg       string[pyarrow]\n 50  DlyOrdDivAmt       double[pyarrow]\n 51  DlyNonOrdDivAmt    double[pyarrow]\n 52  DlyFacPrc          double[pyarrow]\n 53  DlyDistRetFlg      string[pyarrow]\n 54  DlyVol             int64[pyarrow] \n 55  DlyClose           double[pyarrow]\n 56  DlyLow             double[pyarrow]\n 57  DlyHigh            double[pyarrow]\n 58  DlyBid             double[pyarrow]\n 59  DlyAsk             double[pyarrow]\n 60  DlyOpen            double[pyarrow]\n 61  DlyNumTrd          int64[pyarrow] \n 62  DlyMMCnt           int64[pyarrow] \n 63  DlyPrcVol          double[pyarrow]\ndtypes: datetime64[ns](6), double[pyarrow](17), int64[pyarrow](7), object(5), string[pyarrow](29)\nmemory usage: 4.2+ GB\nReading using the c engine and pyarrow backend: 51.4582 seconds\n\n\n\nx = time.time()\ndf = pd.read_csv(\"files/crsp_dsf.csv.gz\", parse_dates=date_cols, engine=\"pyarrow\")\ntime_arrow_default = time.time() - x\ndf.info()\nprint(\n    f\"Reading using the pyarrow engine and default backend: {time_arrow_default:.4f} seconds\"\n)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 64 columns):\n #   Column             Dtype         \n---  ------             -----         \n 0   PERMNO             int64         \n 1   SecInfoStartDt     datetime64[ns]\n 2   SecInfoEndDt       datetime64[ns]\n 3   SecurityBegDt      datetime64[ns]\n 4   SecurityEndDt      datetime64[ns]\n 5   SecurityHdrFlg     object        \n 6   HdrCUSIP           object        \n 7   HdrCUSIP9          object        \n 8   CUSIP              object        \n 9   CUSIP9             object        \n 10  PrimaryExch        object        \n 11  ConditionalType    object        \n 12  ExchangeTier       object        \n 13  TradingStatusFlg   object        \n 14  SecurityNm         object        \n 15  ShareClass         object        \n 16  USIncFlg           object        \n 17  IssuerType         object        \n 18  SecurityType       object        \n 19  SecuritySubType    object        \n 20  ShareType          object        \n 21  SecurityActiveFlg  object        \n 22  DelActionType      object        \n 23  DelStatusType      object        \n 24  DelReasonType      object        \n 25  DelPaymentType     object        \n 26  Ticker             object        \n 27  TradingSymbol      object        \n 28  PERMCO             int64         \n 29  SICCD              int64         \n 30  NAICS              int64         \n 31  ICBIndustry        object        \n 32  IssuerNm           object        \n 33  YYYYMMDD           datetime64[ns]\n 34  DlyCalDt           datetime64[ns]\n 35  DlyDelFlg          object        \n 36  DlyPrc             float64       \n 37  DlyPrcFlg          object        \n 38  DlyCap             float64       \n 39  DlyCapFlg          object        \n 40  DlyPrevPrc         float64       \n 41  DlyPrevPrcFlg      object        \n 42  DlyPrevDt          object        \n 43  DlyPrevCap         float64       \n 44  DlyPrevCapFlg      object        \n 45  DlyRet             float64       \n 46  DlyRetx            float64       \n 47  DlyRetI            float64       \n 48  DlyRetMissFlg      object        \n 49  DlyRetDurFlg       object        \n 50  DlyOrdDivAmt       float64       \n 51  DlyNonOrdDivAmt    float64       \n 52  DlyFacPrc          float64       \n 53  DlyDistRetFlg      object        \n 54  DlyVol             float64       \n 55  DlyClose           float64       \n 56  DlyLow             float64       \n 57  DlyHigh            float64       \n 58  DlyBid             float64       \n 59  DlyAsk             float64       \n 60  DlyOpen            float64       \n 61  DlyNumTrd          float64       \n 62  DlyMMCnt           float64       \n 63  DlyPrcVol          float64       \ndtypes: datetime64[ns](6), float64(20), int64(4), object(34)\nmemory usage: 4.2+ GB\nReading using the pyarrow engine and default backend: 21.9848 seconds\n\n\n\nx = time.time()\ndf = pd.read_csv(\n    \"files/crsp_dsf.csv.gz\",\n    parse_dates=date_cols,\n    engine=\"pyarrow\",\n    dtype_backend=\"numpy_nullable\",\n)\ntime_arrow_np = time.time() - x\ndf.info()\nprint(\n    f\"Reading using the pyarrow engine and numpy_nullable backend: {time_arrow_np:.4f} seconds\"\n)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 64 columns):\n #   Column             Dtype         \n---  ------             -----         \n 0   PERMNO             Int64         \n 1   SecInfoStartDt     datetime64[ns]\n 2   SecInfoEndDt       datetime64[ns]\n 3   SecurityBegDt      datetime64[ns]\n 4   SecurityEndDt      datetime64[ns]\n 5   SecurityHdrFlg     string        \n 6   HdrCUSIP           string        \n 7   HdrCUSIP9          string        \n 8   CUSIP              string        \n 9   CUSIP9             string        \n 10  PrimaryExch        string        \n 11  ConditionalType    string        \n 12  ExchangeTier       string        \n 13  TradingStatusFlg   string        \n 14  SecurityNm         string        \n 15  ShareClass         string        \n 16  USIncFlg           string        \n 17  IssuerType         string        \n 18  SecurityType       string        \n 19  SecuritySubType    string        \n 20  ShareType          string        \n 21  SecurityActiveFlg  string        \n 22  DelActionType      string        \n 23  DelStatusType      string        \n 24  DelReasonType      string        \n 25  DelPaymentType     string        \n 26  Ticker             string        \n 27  TradingSymbol      string        \n 28  PERMCO             Int64         \n 29  SICCD              Int64         \n 30  NAICS              Int64         \n 31  ICBIndustry        string        \n 32  IssuerNm           string        \n 33  YYYYMMDD           datetime64[ns]\n 34  DlyCalDt           datetime64[ns]\n 35  DlyDelFlg          string        \n 36  DlyPrc             Float64       \n 37  DlyPrcFlg          string        \n 38  DlyCap             Float64       \n 39  DlyCapFlg          string        \n 40  DlyPrevPrc         Float64       \n 41  DlyPrevPrcFlg      string        \n 42  DlyPrevDt          object        \n 43  DlyPrevCap         Float64       \n 44  DlyPrevCapFlg      string        \n 45  DlyRet             Float64       \n 46  DlyRetx            Float64       \n 47  DlyRetI            Float64       \n 48  DlyRetMissFlg      string        \n 49  DlyRetDurFlg       string        \n 50  DlyOrdDivAmt       Float64       \n 51  DlyNonOrdDivAmt    Float64       \n 52  DlyFacPrc          Float64       \n 53  DlyDistRetFlg      string        \n 54  DlyVol             Int64         \n 55  DlyClose           Float64       \n 56  DlyLow             Float64       \n 57  DlyHigh            Float64       \n 58  DlyBid             Float64       \n 59  DlyAsk             Float64       \n 60  DlyOpen            Float64       \n 61  DlyNumTrd          Int64         \n 62  DlyMMCnt           Int64         \n 63  DlyPrcVol          Float64       \ndtypes: Float64(17), Int64(7), datetime64[ns](6), object(1), string(33)\nmemory usage: 4.4+ GB\nReading using the pyarrow engine and numpy_nullable backend: 33.7553 seconds\n\n\n\nx = time.time()\ndf = pd.read_csv(\n    \"files/crsp_dsf.csv.gz\",\n    parse_dates=date_cols,\n    engine=\"pyarrow\",\n    dtype_backend=\"pyarrow\",\n)\ntime_arrow_arrow = time.time() - x\ndf.info()\nprint(\n    f\"Reading using the pyarrow engine and pyarrow backend: {time_arrow_arrow:.4f} seconds\"\n)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 64 columns):\n #   Column             Dtype               \n---  ------             -----               \n 0   PERMNO             int64[pyarrow]      \n 1   SecInfoStartDt     date32[day][pyarrow]\n 2   SecInfoEndDt       date32[day][pyarrow]\n 3   SecurityBegDt      date32[day][pyarrow]\n 4   SecurityEndDt      date32[day][pyarrow]\n 5   SecurityHdrFlg     string[pyarrow]     \n 6   HdrCUSIP           string[pyarrow]     \n 7   HdrCUSIP9          string[pyarrow]     \n 8   CUSIP              string[pyarrow]     \n 9   CUSIP9             string[pyarrow]     \n 10  PrimaryExch        string[pyarrow]     \n 11  ConditionalType    string[pyarrow]     \n 12  ExchangeTier       string[pyarrow]     \n 13  TradingStatusFlg   string[pyarrow]     \n 14  SecurityNm         string[pyarrow]     \n 15  ShareClass         string[pyarrow]     \n 16  USIncFlg           string[pyarrow]     \n 17  IssuerType         string[pyarrow]     \n 18  SecurityType       string[pyarrow]     \n 19  SecuritySubType    string[pyarrow]     \n 20  ShareType          string[pyarrow]     \n 21  SecurityActiveFlg  string[pyarrow]     \n 22  DelActionType      string[pyarrow]     \n 23  DelStatusType      string[pyarrow]     \n 24  DelReasonType      string[pyarrow]     \n 25  DelPaymentType     string[pyarrow]     \n 26  Ticker             string[pyarrow]     \n 27  TradingSymbol      string[pyarrow]     \n 28  PERMCO             int64[pyarrow]      \n 29  SICCD              int64[pyarrow]      \n 30  NAICS              int64[pyarrow]      \n 31  ICBIndustry        string[pyarrow]     \n 32  IssuerNm           string[pyarrow]     \n 33  YYYYMMDD           datetime64[ns]      \n 34  DlyCalDt           date32[day][pyarrow]\n 35  DlyDelFlg          string[pyarrow]     \n 36  DlyPrc             double[pyarrow]     \n 37  DlyPrcFlg          string[pyarrow]     \n 38  DlyCap             double[pyarrow]     \n 39  DlyCapFlg          string[pyarrow]     \n 40  DlyPrevPrc         double[pyarrow]     \n 41  DlyPrevPrcFlg      string[pyarrow]     \n 42  DlyPrevDt          date32[day][pyarrow]\n 43  DlyPrevCap         double[pyarrow]     \n 44  DlyPrevCapFlg      string[pyarrow]     \n 45  DlyRet             double[pyarrow]     \n 46  DlyRetx            double[pyarrow]     \n 47  DlyRetI            double[pyarrow]     \n 48  DlyRetMissFlg      string[pyarrow]     \n 49  DlyRetDurFlg       string[pyarrow]     \n 50  DlyOrdDivAmt       double[pyarrow]     \n 51  DlyNonOrdDivAmt    double[pyarrow]     \n 52  DlyFacPrc          double[pyarrow]     \n 53  DlyDistRetFlg      string[pyarrow]     \n 54  DlyVol             int64[pyarrow]      \n 55  DlyClose           double[pyarrow]     \n 56  DlyLow             double[pyarrow]     \n 57  DlyHigh            double[pyarrow]     \n 58  DlyBid             double[pyarrow]     \n 59  DlyAsk             double[pyarrow]     \n 60  DlyOpen            double[pyarrow]     \n 61  DlyNumTrd          int64[pyarrow]      \n 62  DlyMMCnt           int64[pyarrow]      \n 63  DlyPrcVol          double[pyarrow]     \ndtypes: date32[day][pyarrow](6), datetime64[ns](1), double[pyarrow](17), int64[pyarrow](7), string[pyarrow](33)\nmemory usage: 4.2 GB\nReading using the pyarrow engine and pyarrow backend: 6.4519 seconds"
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#tip-3-store-the-dataframe-in-the-parquet-format",
    "href": "posts/pandas-read-csv/index.html#tip-3-store-the-dataframe-in-the-parquet-format",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Tip 3: Store the DataFrame in the Parquet format",
    "text": "Tip 3: Store the DataFrame in the Parquet format\nAn even better way to avoid unnecessary parsing is to use a more efficient file format. CSV files are easy to create and read, but they are not the most efficient format. Because they store the data as text, pandas has to parse the data every time we read the file. The Apache Parquet file format is more efficient because it stores the data in a binary format that is optimized for efficient reading. This means that pandas does not need to parse the data again every time we read the file. In addition, Parquet files are compressed by default, so they take less space on disk.\nParquet files are columnar, which means that the data is stored by column. This is especially efficient if you only need to read a subset of the columns. Another efficient columnar file format that is supported by pandas is Apache ORC. However, it is not as widely supported as Parquet.\nParquet files are supported by pandas through the pyarrow library, so you will need to install it if you want to use Parquet files.\nFIX In the example below, we can see that both the writing and reading of the Parquet file are faster than the writing and reading of the CSV file. Reading the parquet file is almost 5 times faster than reading the CSV file, and the disk space used by the parquet file is only 73.2 MB compared to 78.1 MB for the compressed CSV file.\n\nx = time.time()\ndf.to_parquet(\"files/crsp_dsf.parquet\", engine=\"pyarrow\", index=False)\ntime_write_parquet = time.time() - x\nprint(\n    f\"Writing to Parquet using the pyarrow engine and pyarrow backend: {time_write_parquet} seconds\"\n)\n\nWriting to Parquet using the pyarrow engine and pyarrow backend: 7.636731147766113 seconds\n\n\n\nx = time.time()\ndf = pd.read_parquet(\n    \"files/crsp_dsf.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n)\ntime_parquet_arrow_arrow = time.time() - x\ndf.info()\nprint(\n    f\"Reading Parquet file using the pyarrow engine and pyarrow backend: {time_parquet_arrow_arrow} seconds\"\n)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 64 columns):\n #   Column             Dtype                 \n---  ------             -----                 \n 0   PERMNO             int64[pyarrow]        \n 1   SecInfoStartDt     date32[day][pyarrow]  \n 2   SecInfoEndDt       date32[day][pyarrow]  \n 3   SecurityBegDt      date32[day][pyarrow]  \n 4   SecurityEndDt      date32[day][pyarrow]  \n 5   SecurityHdrFlg     string[pyarrow]       \n 6   HdrCUSIP           string[pyarrow]       \n 7   HdrCUSIP9          string[pyarrow]       \n 8   CUSIP              string[pyarrow]       \n 9   CUSIP9             string[pyarrow]       \n 10  PrimaryExch        string[pyarrow]       \n 11  ConditionalType    string[pyarrow]       \n 12  ExchangeTier       string[pyarrow]       \n 13  TradingStatusFlg   string[pyarrow]       \n 14  SecurityNm         string[pyarrow]       \n 15  ShareClass         string[pyarrow]       \n 16  USIncFlg           string[pyarrow]       \n 17  IssuerType         string[pyarrow]       \n 18  SecurityType       string[pyarrow]       \n 19  SecuritySubType    string[pyarrow]       \n 20  ShareType          string[pyarrow]       \n 21  SecurityActiveFlg  string[pyarrow]       \n 22  DelActionType      string[pyarrow]       \n 23  DelStatusType      string[pyarrow]       \n 24  DelReasonType      string[pyarrow]       \n 25  DelPaymentType     string[pyarrow]       \n 26  Ticker             string[pyarrow]       \n 27  TradingSymbol      string[pyarrow]       \n 28  PERMCO             int64[pyarrow]        \n 29  SICCD              int64[pyarrow]        \n 30  NAICS              int64[pyarrow]        \n 31  ICBIndustry        string[pyarrow]       \n 32  IssuerNm           string[pyarrow]       \n 33  YYYYMMDD           timestamp[ns][pyarrow]\n 34  DlyCalDt           date32[day][pyarrow]  \n 35  DlyDelFlg          string[pyarrow]       \n 36  DlyPrc             double[pyarrow]       \n 37  DlyPrcFlg          string[pyarrow]       \n 38  DlyCap             double[pyarrow]       \n 39  DlyCapFlg          string[pyarrow]       \n 40  DlyPrevPrc         double[pyarrow]       \n 41  DlyPrevPrcFlg      string[pyarrow]       \n 42  DlyPrevDt          date32[day][pyarrow]  \n 43  DlyPrevCap         double[pyarrow]       \n 44  DlyPrevCapFlg      string[pyarrow]       \n 45  DlyRet             double[pyarrow]       \n 46  DlyRetx            double[pyarrow]       \n 47  DlyRetI            double[pyarrow]       \n 48  DlyRetMissFlg      string[pyarrow]       \n 49  DlyRetDurFlg       string[pyarrow]       \n 50  DlyOrdDivAmt       double[pyarrow]       \n 51  DlyNonOrdDivAmt    double[pyarrow]       \n 52  DlyFacPrc          double[pyarrow]       \n 53  DlyDistRetFlg      string[pyarrow]       \n 54  DlyVol             int64[pyarrow]        \n 55  DlyClose           double[pyarrow]       \n 56  DlyLow             double[pyarrow]       \n 57  DlyHigh            double[pyarrow]       \n 58  DlyBid             double[pyarrow]       \n 59  DlyAsk             double[pyarrow]       \n 60  DlyOpen            double[pyarrow]       \n 61  DlyNumTrd          int64[pyarrow]        \n 62  DlyMMCnt           int64[pyarrow]        \n 63  DlyPrcVol          double[pyarrow]       \ndtypes: date32[day][pyarrow](6), double[pyarrow](17), int64[pyarrow](7), string[pyarrow](33), timestamp[ns][pyarrow](1)\nmemory usage: 4.2 GB\nReading Parquet file using the pyarrow engine and pyarrow backend: 0.7189159393310547 seconds"
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#tip-4-only-read-the-columns-you-need",
    "href": "posts/pandas-read-csv/index.html#tip-4-only-read-the-columns-you-need",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Tip 4: Only read the columns you need",
    "text": "Tip 4: Only read the columns you need\nThe second tip is to only read the columns you need. This is especially important if you are working with large files as it will save memory and time. We can specify the columns we want with the usecols parameter of the read_csv function or the columns parameter of the read_parquet function. Note that for CSV files the saved time is mostly because we do not have to parse the columns we are not interested in. However, because CSV files are not indexed, we still have to read the entire file. Parquet files are indexed and columnar, so the `pyarrow`` engine does not have to read the entire file if we only need a subset of the columns.\n\ncols = [\n    \"DlyCalDt\",\n    \"PERMNO\",\n    \"CUSIP\",\n    \"Ticker\",\n    \"PrimaryExch\",\n    \"SecurityType\",\n    \"SICCD\",\n    \"DlyRet\",\n    \"DlyPrc\",\n]\n\n\nx = time.time()\ndf = pd.read_csv(\n    \"files/crsp_dsf.csv.gz\", usecols=cols, parse_dates=[\"DlyCalDt\"], engine=\"c\"\n)\ntime_c_default_subset = time.time() - x\ndf.info()\nprint(\n    f\"Reading a subset of columns using the c engine and default backend: {time_c_default_subset:.4f} seconds\"\n)\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_81162/3713586357.py:2: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 9 columns):\n #   Column        Dtype         \n---  ------        -----         \n 0   PERMNO        int64         \n 1   CUSIP         object        \n 2   PrimaryExch   object        \n 3   SecurityType  object        \n 4   Ticker        object        \n 5   SICCD         int64         \n 6   DlyCalDt      datetime64[ns]\n 7   DlyPrc        float64       \n 8   DlyRet        float64       \ndtypes: datetime64[ns](1), float64(2), int64(2), object(4)\nmemory usage: 610.0+ MB\nReading a subset of columns using the c engine and default backend: 11.7283 seconds\n\n\n\nx = time.time()\ndf = pd.read_csv(\n    \"files/crsp_dsf.csv.gz\",\n    usecols=cols,\n    parse_dates=[\"DlyCalDt\"],\n    engine=\"pyarrow\",\n    dtype_backend=\"pyarrow\",\n)\ntime_arrow_arrow_subset = time.time() - x\ndf.info()\nprint(\n    f\"Reading a subset of columns using the pyarrow engine and pyarrow backend: {time_arrow_arrow_subset:.4f} seconds\"\n)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 9 columns):\n #   Column        Dtype               \n---  ------        -----               \n 0   DlyCalDt      date32[day][pyarrow]\n 1   PERMNO        int64[pyarrow]      \n 2   CUSIP         string[pyarrow]     \n 3   Ticker        string[pyarrow]     \n 4   PrimaryExch   string[pyarrow]     \n 5   SecurityType  string[pyarrow]     \n 6   SICCD         int64[pyarrow]      \n 7   DlyRet        double[pyarrow]     \n 8   DlyPrc        double[pyarrow]     \ndtypes: date32[day][pyarrow](1), double[pyarrow](2), int64[pyarrow](2), string[pyarrow](4)\nmemory usage: 583.0 MB\nReading a subset of columns using the pyarrow engine and pyarrow backend: 2.9267 seconds\n\n\n\nx = time.time()\ndf = pd.read_parquet(\n    \"files/crsp_dsf.parquet\", columns=cols, engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n)\ntime_parquet_arrow_arrow_subset = time.time() - x\ndf.info()\nprint(\n    f\"Reading a subset of columns from Parquet file using the pyarrow engine and pyarrow backend: {time_parquet_arrow_arrow_subset:.4f} seconds\"\n)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8883156 entries, 0 to 8883155\nData columns (total 9 columns):\n #   Column        Dtype               \n---  ------        -----               \n 0   DlyCalDt      date32[day][pyarrow]\n 1   PERMNO        int64[pyarrow]      \n 2   CUSIP         string[pyarrow]     \n 3   Ticker        string[pyarrow]     \n 4   PrimaryExch   string[pyarrow]     \n 5   SecurityType  string[pyarrow]     \n 6   SICCD         int64[pyarrow]      \n 7   DlyRet        double[pyarrow]     \n 8   DlyPrc        double[pyarrow]     \ndtypes: date32[day][pyarrow](1), double[pyarrow](2), int64[pyarrow](2), string[pyarrow](4)\nmemory usage: 588.7 MB\nReading a subset of columns from Parquet file using the pyarrow engine and pyarrow backend: 0.0876 seconds"
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#tip-5-parquet-only-only-read-the-rows-you-need",
    "href": "posts/pandas-read-csv/index.html#tip-5-parquet-only-only-read-the-rows-you-need",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Tip 5 (Parquet-only): Only read the rows you need",
    "text": "Tip 5 (Parquet-only): Only read the rows you need\nPandas‚Äô read_parquet() function passes all the arguments to the pyarrow.parquet.read_table() function, so we can use the filters parameter to only read the rows we need. The filters parameter takes a list of tuples where each tuple contains the column name, the operator, and the value. For example, if we only want to read the rows for January 2023, we can use the following filter:\n\nfrom datetime import date\n\nfilters = [(\"DlyCalDt\", \"&gt;=\", date(2023, 1, 1)), (\"DlyCalDt\", \"&lt;\", date(2023, 2, 1))]\n\nx = time.time()\ndf = pd.read_parquet(\n    \"files/crsp_dsf.parquet\",\n    columns=cols,\n    filters=filters,\n    engine=\"pyarrow\",\n    dtype_backend=\"pyarrow\",\n)\ntime_parquet_arrow_arrow_subset = time.time() - x\ndf.info()\nprint(\n    f\"Reading a subset of rows and columns from Parquet file using the pyarrow engine and pyarrow backend: {time_parquet_arrow_arrow_subset:.4f} seconds\"\n)\ndf\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 189762 entries, 0 to 189761\nData columns (total 9 columns):\n #   Column        Non-Null Count   Dtype               \n---  ------        --------------   -----               \n 0   DlyCalDt      189762 non-null  date32[day][pyarrow]\n 1   PERMNO        189762 non-null  int64[pyarrow]      \n 2   CUSIP         189695 non-null  string[pyarrow]     \n 3   Ticker        189655 non-null  string[pyarrow]     \n 4   PrimaryExch   189762 non-null  string[pyarrow]     \n 5   SecurityType  189695 non-null  string[pyarrow]     \n 6   SICCD         189762 non-null  int64[pyarrow]      \n 7   DlyRet        189592 non-null  double[pyarrow]     \n 8   DlyPrc        189633 non-null  double[pyarrow]     \ndtypes: date32[day][pyarrow](1), double[pyarrow](2), int64[pyarrow](2), string[pyarrow](4)\nmemory usage: 12.6 MB\nReading a subset of rows and columns from Parquet file using the pyarrow engine and pyarrow backend: 0.0680 seconds\n\n\n\n\n\n\n\n\n\nDlyCalDt\nPERMNO\nCUSIP\nTicker\nPrimaryExch\nSecurityType\nSICCD\nDlyRet\nDlyPrc\n\n\n\n\n0\n2023-01-03\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n0.011823\n151.48\n\n\n1\n2023-01-04\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n-0.001716\n151.22\n\n\n2\n2023-01-05\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n-0.010713\n149.60\n\n\n3\n2023-01-06\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n0.020321\n152.64\n\n\n4\n2023-01-09\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n-0.019851\n149.61\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n189757\n2023-01-25\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n0.003753\n144.43\n\n\n189758\n2023-01-26\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n0.109673\n160.27\n\n\n189759\n2023-01-27\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n0.110002\n177.90\n\n\n189760\n2023-01-30\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n-0.063182\n166.66\n\n\n189761\n2023-01-31\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n0.039362\n173.22\n\n\n\n\n189762 rows √ó 9 columns\n\n\n\nNote that this is not necessarily faster than reading the entire file and then filtering the rows with pandas, but it can be more memory efficient because we do not have to read the entire file into memory.\n\nx = time.time()\ndf = pd.read_parquet(\n    \"files/crsp_dsf.parquet\",\n    columns=cols,\n    filters=filters,\n    engine=\"pyarrow\",\n    dtype_backend=\"pyarrow\",\n)\ndf = df[(df[\"DlyCalDt\"] &gt;= date(2023, 1, 1)) & (df[\"DlyCalDt\"] &lt; date(2023, 2, 1))]\n\ntime_parquet_arrow_arrow_subset = time.time() - x\ndf.info()\nprint(\n    f\"Reading a subset of columns from Parquet file using the pyarrow engine and pyarrow backend and filtering rows using pandas: {time_parquet_arrow_arrow_subset:.4f} seconds\"\n)\ndf\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 189762 entries, 0 to 189761\nData columns (total 9 columns):\n #   Column        Non-Null Count   Dtype               \n---  ------        --------------   -----               \n 0   DlyCalDt      189762 non-null  date32[day][pyarrow]\n 1   PERMNO        189762 non-null  int64[pyarrow]      \n 2   CUSIP         189695 non-null  string[pyarrow]     \n 3   Ticker        189655 non-null  string[pyarrow]     \n 4   PrimaryExch   189762 non-null  string[pyarrow]     \n 5   SecurityType  189695 non-null  string[pyarrow]     \n 6   SICCD         189762 non-null  int64[pyarrow]      \n 7   DlyRet        189592 non-null  double[pyarrow]     \n 8   DlyPrc        189633 non-null  double[pyarrow]     \ndtypes: date32[day][pyarrow](1), double[pyarrow](2), int64[pyarrow](2), string[pyarrow](4)\nmemory usage: 12.6 MB\nReading a subset of columns from Parquet file using the pyarrow engine and pyarrow backend and filtering rows using pandas: 0.0622 seconds\n\n\n\n\n\n\n\n\n\nDlyCalDt\nPERMNO\nCUSIP\nTicker\nPrimaryExch\nSecurityType\nSICCD\nDlyRet\nDlyPrc\n\n\n\n\n0\n2023-01-03\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n0.011823\n151.48\n\n\n1\n2023-01-04\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n-0.001716\n151.22\n\n\n2\n2023-01-05\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n-0.010713\n149.60\n\n\n3\n2023-01-06\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n0.020321\n152.64\n\n\n4\n2023-01-09\n10026\n46603210\nJJSF\nQ\nEQTY\n2052\n-0.019851\n149.61\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n189757\n2023-01-25\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n0.003753\n144.43\n\n\n189758\n2023-01-26\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n0.109673\n160.27\n\n\n189759\n2023-01-27\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n0.110002\n177.90\n\n\n189760\n2023-01-30\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n-0.063182\n166.66\n\n\n189761\n2023-01-31\n93436\n88160R10\nTSLA\nQ\nEQTY\n9999\n0.039362\n173.22\n\n\n\n\n189762 rows √ó 9 columns"
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#other-benefits-of-using-the-pyarrow-dtype-backend",
    "href": "posts/pandas-read-csv/index.html#other-benefits-of-using-the-pyarrow-dtype-backend",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Other benefits of using the pyarrow dtype backend",
    "text": "Other benefits of using the pyarrow dtype backend\nFor casual pandas users, the pyarrow dtype backend provides long-needed features such as the ability to store missing values in integer and boolean columns1 and faster reading and writing of files.\nHowever, it is meant to be more than just a replacement for the numpy dtype backend. The Apache Arrow project is a cross-language development platform for in-memory data that is designed to accelerate the performance of analytical workloads. The pyarrow library is the Python implementation of the Arrow project. One of the goals of the Arrow project is to provide a common in-memory data format for data interchange between different programming languages. This means that a pandas DataFrame with a pyarrow dtype backend can be used as an input to other Arrow-compatible libraries such as Polars and DuckDB, or even in a different programming language such as R or Julia. This lets you use the best tool for the job without having to worry about data interchange and the associated performance overhead."
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#caveats",
    "href": "posts/pandas-read-csv/index.html#caveats",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Caveats",
    "text": "Caveats\n\nExperimental Features\nThe dtype_backend parameter of the read_csv function is still labeled as experimental in the pandas documentation, so use it with caution.\n\n\nDtypes\nThe data types (dtypes), i.e.¬†the in-memory representation of the data are not the same for the default (aka numpy), the numpy_nullable, or the pyarrow dtype backends. This means that if your existing code defines the data types of the columns, you will need to update it if you switch to the pyarrow dtype backend. In most cases, all you will need to do is append [pyarrow] to the data type, e.g.¬†int64[pyarrow] instead of int64. However, you should test your code to make sure that it works as expected. However, the types aren‚Äôt all the same; for example, the default dtype backend uses the object type to represent strings while the pyarrow dtype backend uses the string[pyarrow] type. Have a look at the pandas documentation for more information."
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#conclusion",
    "href": "posts/pandas-read-csv/index.html#conclusion",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Conclusion",
    "text": "Conclusion\nI hope these tips will save you time and memory when reading and parsing large CSV files in Python! As you can see, the Arrow library and the Parquet file format are powerful tools for reading and writing data, and it is worth exploring if you are working with large datasets. This is not surprising, as the Parquet format was created as a columnar storage format for the Hadoop big data ecosystem, and the Arrow library was created to provide a standard for in-memory data representation. The Arrow library is also used by other data processing tools such as DuckDB and Polars, so it is worth learning about it if you are working with large datasets."
  },
  {
    "objectID": "posts/pandas-read-csv/index.html#footnotes",
    "href": "posts/pandas-read-csv/index.html#footnotes",
    "title": "Efficient reading and parsing of large CSV files in Python with Pandas and Arrow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the numpy_nullable backend has supported this for a while.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/github-for-research/index.html",
    "href": "posts/github-for-research/index.html",
    "title": "Using GitHub for academic research",
    "section": "",
    "text": "Git and GitHub are powerful version control tools for managing your research projects and collaborating with others while keeping a detailed record of your work. In this tutorial, we will cover the basics of Git and GitHub, and how to use them to manage your research projects.\nOver a decade ago, I started using a version control tool called Subversion with hosting on Bitbucket. I then switched to Git and GitHub, and I have been using them ever since. I use them to manage all my research projects, including my code, data, and even my academic papers. Not only does it help me keep track of my work, but it also makes it easy to collaborate with others.\nMy main motivation for starting to use a version control system was to make my research more reproducible. I wanted to be able to share my code and data with others so they could replicate my results. It turns out that not only was that desirable, but it is now a requirement for many journals and funding agencies. Git and GitHub make this process much easier."
  },
  {
    "objectID": "posts/github-for-research/index.html#video-tutorial",
    "href": "posts/github-for-research/index.html#video-tutorial",
    "title": "Using GitHub for academic research",
    "section": "Video tutorial",
    "text": "Video tutorial\nPart of this post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/github-for-research/index.html#what-is-version-control",
    "href": "posts/github-for-research/index.html#what-is-version-control",
    "title": "Using GitHub for academic research",
    "section": "What is version control?",
    "text": "What is version control?\nVersion control, also known as source control, is a system that records changes to a file or set of files over time so that you can recall specific versions later. It‚Äôs one of the most important tools in the toolkit of any developer or data scientist. It‚Äôs also very useful for researchers, especially those working with code, but in practice, it is underused in academia. The idea behind version control is quite simple: it allows you to track and manage changes to your projects. Think of it as the ‚Äútrack changes‚Äù feature in Microsoft Word, but for all your files and turbocharged with features that make it easy to collaborate with others.\nImagine you‚Äôre working on a research paper and decide to delete a section. A few days later, you realize that section was crucial. Without version control, you‚Äôd have to rewrite that entire section. With version control, you can simply look at your previous versions, find the one that includes the section you need, and restore it. Most of us have some kind of version control in our lives. For example, when you write a paper, you might save different versions of the document as you work on it. This way, if you make a mistake or delete something important, you can go back to a previous version. However, this approach has limitations, and there are better ways to manage versions of your work.\nIn the context of coding, version control is even more important. As you add new features to your code or fix bugs, it‚Äôs essential to be able to track these changes. If something breaks, you need to know what was changed so you can figure out what went wrong and how to fix it. Additionally, version control systems allow multiple people to work on the same project simultaneously, making collaboration easier and more efficient while keeping a detailed record of who made what changes and when."
  },
  {
    "objectID": "posts/github-for-research/index.html#what-is-git",
    "href": "posts/github-for-research/index.html#what-is-git",
    "title": "Using GitHub for academic research",
    "section": "What is Git?",
    "text": "What is Git?\nGit is the most widely used version control system in the world. It was created in 2005 by Linus Torvalds, the creator of the Linux operating system. Torvalds wanted a version control system that was fast, efficient, and capable of handling small to very large projects with ease. Unlike its predecessors, Git was designed to be decentralized, allowing multiple developers to work on the same project simultaneously without stepping on each other‚Äôs toes. Like Linux, Git is free and distributed under an open-source license.\nAt its core, Git allows users to keep a complete history of their project, noting every change made to every file. This feature is akin to having a detailed logbook that captures the evolution of a project over time. With Git, users can branch off from the main project to experiment or work on new features without disrupting the core project. Later, these branches can be merged back into the main project seamlessly. This ability to branch and merge is particularly powerful, preventing conflicts and maintaining the integrity of the original project. Git is also incredibly robust in managing project history, enabling users to revert to previous versions if needed, offering a safety net against errors or unintended consequences of new changes.\n\n\n\n\n\n\nTipNot only for code\n\n\n\nGit is a great tool for version control of any kind of file, especially text files. It turns out that if you mainly use LaTeX or Markdown for writing and presentations, you can use Git to track changes in your documents and collaborate with others. Gone are the days of sending around files with names like paper_v1_final_final_really_final.tex and paper_v1_final_final_really_final_revised.tex!"
  },
  {
    "objectID": "posts/github-for-research/index.html#what-is-github",
    "href": "posts/github-for-research/index.html#what-is-github",
    "title": "Using GitHub for academic research",
    "section": "What is GitHub?",
    "text": "What is GitHub?\nGitHub, launched in 2008 and acquired by Microsoft in 2018, quickly rose to become the de facto online platform for code management and collaboration. While Git is the engine, GitHub can be thought of as the sleek, user-friendly vehicle that houses this engine. It takes the core functionalities of Git and provides a web-based graphical interface that is intuitive and accessible. GitHub‚Äôs rise is not just due to its user-friendly nature but also because it functions like a social network for developers and researchers. Users can host their Git repositories, share their work with others, collaborate on projects, and even contribute to others‚Äô projects.\n\n\n\n\n\n\nTipNot the only game in town.\n\n\n\nWhile GitHub is the most popular platform for code management and collaboration, it is not the only one. Two other popular platforms are GitLab and Bitbucket. Cloud providers like AWS and Azure also offer Git hosting services."
  },
  {
    "objectID": "posts/github-for-research/index.html#why-use-git-and-github-for-research",
    "href": "posts/github-for-research/index.html#why-use-git-and-github-for-research",
    "title": "Using GitHub for academic research",
    "section": "Why use Git and GitHub for research?",
    "text": "Why use Git and GitHub for research?\nFor finance researchers, Git and GitHub offer a multitude of benefits. Git is an excellent tool for managing complex research projects. It allows researchers to track changes in their data analysis scripts, models, and even research papers, ensuring a clear audit trail of how the analysis was conducted and conclusions were reached. This level of transparency is crucial not just for personal record-keeping but also for collaborative projects where multiple researchers contribute to a single body of work. In a field where reputation is everything, Git can help researchers maintain a high level of integrity and accountability. The pull request system of GitHub is particularly beneficial for collaborative projects. It enables researchers to propose, discuss, and review changes before they are integrated into the main project. This not only ensures that every change is scrutinized for accuracy and relevance but also fosters a culture of peer review and collective improvement among collaborators as the project progresses. Furthermore, GitHub‚Äôs issue-tracking and project management features help researchers organize their tasks, track bugs, and manage project progress transparently."
  },
  {
    "objectID": "posts/github-for-research/index.html#setup-and-installation",
    "href": "posts/github-for-research/index.html#setup-and-installation",
    "title": "Using GitHub for academic research",
    "section": "Setup and installation",
    "text": "Setup and installation\nIf you followed my tutorial on installing Python then you already have Git installed on your computer.\nGit is available for Windows, Mac, and Linux operating systems. Git will be installed by default on most Linux distributions. If you are using a Mac, you can install Git using Homebrew. If you are using Windows, or are on Mac and prefer not to use Homebrew, you can download Git from the Git website.\nYou will also need to create a GitHub account. You can do so by visiting the GitHub website. GitHub offers free accounts for individuals and paid plans for teams and organizations. For most researchers, the free plan will be sufficient. If you are a student or educator, I recommend that you sign up for the GitHub Education program, which offers additional benefits including free access to GitHub Copilot, a ChatGPT-powered coding assistant.\nGit is a command-line tool that you use in the terminal, but there are many graphical user interfaces available that make it easier to use, including GitHub Desktop. GitHub Desktop makes it easy to clone repositories, create branches, commit changes, and push your changes to GitHub. In this tutorial, I will use the built-in Git and GitHub integration in Visual Studio Code, but I still keep GitHub Desktop installed because it lets you easily clone repositories from GitHub in one click.\nVS Code‚Äôs Git integration is available in the Source Control tab on the left sidebar. To enable GitHub integration, you will need to sign in to your GitHub account. You should also install the GitHub Pull Requests and Issues extension to make it easier to work with pull requests and issues.\nAdditional GitHub features such as Copilot, Codespaces, and Actions are available in VS Code by installing the relevant extensions. You can also install the GitLens extension to get additional features for working with Git."
  },
  {
    "objectID": "posts/github-for-research/index.html#git-workflow",
    "href": "posts/github-for-research/index.html#git-workflow",
    "title": "Using GitHub for academic research",
    "section": "Git workflow",
    "text": "Git workflow\n\nUnderstanding the Core Concepts of Git\nGit‚Äôs power lies in its ability to manage and track changes in your projects, and this is achieved through a set of core functionalities. Let‚Äôs demystify these key terms:\n1. Repository (Repo): The heart of any Git project, a repository is like a project folder but with superpowers. It contains all of your project files along with each file‚Äôs revision history. You can have local repositories on your computer and synchronize them with remote repositories on GitHub to share and collaborate.\n2. Staging: Think of staging as a prep area. When you make changes to files, they don‚Äôt automatically get saved into your repository. Instead, you selectively add these changes to the staging area, indicating that you‚Äôve marked these modifications for your next commit.\n3. Commits: Committing is the act of saving your staged changes to the project‚Äôs history. A commit is like a snapshot of your repository at a particular point in time. Each commit has a unique ID and includes a message describing the changes, aiding future you or collaborators in understanding what was modified and why.\n4. Push and Pull: These are the methods by which you interact with a remote repository. When you push, you are sending your committed changes to a remote repo. Conversely, when you pull, you are fetching the latest changes from the remote repo to your local machine.\n5. Branching: Branching allows you to diverge from the main line of development and work independently without affecting the main project, often referred to as the main branch.1 It‚Äôs perfect for developing new features or experimenting.\n6. Merging: After you‚Äôve finished working in a branch, you merge those changes back into the main project. Merging combines the changes in your branch with those in the main branch, creating a single, unified history.\n7. Conflicts: Sometimes, when merging branches, Git encounters conflicts - changes that contradict each other. This can happen when two people make changes to the same file. These conflicts need to be manually resolved before completing the merge process.\nWe will explore these concepts in more detail in the next sections.\n\n\nCreating a repository\nA repository (or repo) is where all the magic happens ‚Äì it‚Äôs where your code, documentation, and all other project-related files reside. To create a new repo, simply log into your GitHub account, click on the + icon in the top right corner, and select New repository.\n\nNaming and Describing Your Repository\nChoose a name that succinctly reflects your project. Keep in mind that this name will be part of the URL for your repository, and that it will be used as the default name for the folder when you clone the repository (make a local copy of the repository on your computer). The description field is an opportunity to briefly outline your project‚Äôs objective. This helps others understand the purpose of your repo at a glance.\n\n\nSelecting a License\nYou can also define the license for your project. It is not necessary if you don‚Äôt intend on sharing this code publicly, but it is a good practice to include a license. When it comes to research code, transparency and accessibility are key. I recommend opting for a permissive license, like the MIT License. This license allows others to freely use, modify, and distribute your work ‚Äì perfect for fostering open-source collaboration in the research community. GitHub makes it easy to include a license; just select the MIT License from the dropdown menu when creating your repository. Other permissive licenses include the BSD License and the Apache License.\n\n\nAdding a .gitignore file\nBefore you start adding files to your repo, consider setting up a .gitignore file. This file tells Git which files or folders to ignore in a project. Typically, you‚Äôll want to exclude certain files from being tracked ‚Äì like temporary files, local configuration files, files containing sensitive information, or large data files. GitHub offers templates for .gitignore files tailored to various programming languages and frameworks, which can be a great starting point. It is available as a dropdown menu option when creating the repository. gitignore.io is another useful resource for generating .gitignore files.\n\n\nAdding a README file\nFinally, you‚Äôll want to add a README.md file to your repository. This file is the first thing visitors will see when they visit your repository on GitHub. It‚Äôs an essential component of your project, acting as the introduction and guide. Use the README to explain what your project does, how to set it up, and how to use it. This is important even if your project is not public, as it will help you remember how to use your project in the future and facilitate onboarding new collaborators. This file can be written in plain text or formatted using Markdown, a lightweight markup language that is easy to learn and use. GitHub automatically renders Markdown files, making them easy to read and navigate. You can also include images, links, and code snippets in Markdown files. GitHub offers a handy guide to help you get started with Markdown.\n\n\n\nCloning a repository\nCloning a repository creates a local copy of the remote repository on your computer. This allows you to work on the project locally and push your changes to the remote repository when you‚Äôre ready to share them with others. To clone a repository, you‚Äôll need the URL of the remote repository. You can find this by clicking on the green Code button on the repository‚Äôs homepage. If you are using GitHub Desktop, you can clone the repository by selecting Open with GitHub Desktop, which will open the repository in GitHub Desktop. You can then select the location where you want to store the repository on your computer and click Clone.\nIf you are not using GitHub Desktop, you can clone the repository using the command line. First, copy the URL of the repository from the repository‚Äôs homepage by clicking on the green Code button, then copying the URL by clicking on the clipboard icon next to the URL. To clone the repository, open the terminal and navigate to the directory where you want to store the repository. Then, run the following command:\ngit clone &lt;url&gt;\nThis will create a new directory with the same name as the repository and download all the files from the remote repository into this directory. You can then open this directory in VS Code and start working on the project. From GitHub Desktop, you can open the repository in VS Code by selecting Open in Visual Studio Code from the Repository menu.\n\n\nTracking changes\nOnce you have cloned the repository, you can start making changes to the files in the repository. You can create new files, edit existing files, or delete files. You can also move files around or rename them. You can see all your changes in the Source Control tab in VS Code. Files will be listed under Changes with a U if they are new (untracked), a M if they have been modified, or a D if they have been deleted. You can also see the changes you have made to each file by clicking on the file name.\nWhen you create a new file, it will not be tracked by Git until you add it to the staging area. To add a file to the staging area, you use the Stage Changes button in the Source Control tab in VS Code (the little + sign next to a file when you hover over it). You need to do this not only for new files, but for all files that you have modified or deleted since the last commit. Files in the staging area be included in the next commit.\nOnce you have added one or many changed files to the staging area, you can commit those changes to the repository. To commit changes, you need to enter a commit message describing the changes you have made and then click on the Commit button in the Source Control tab in VS Code (the checkmark icon). You can also use the keyboard shortcut (Command+Enter on Mac or Ctrl+Enter on Windows or Linux) to commit your changes. This will create a new commit, i.e., a new snapshot, with the changes you have staged.\nYou can see all your commits in the Source Control tab under Commits. You can click on a commit to see the changes that were made in that commit. You can also right-click on the commit to access the commit details, including the commit message, the author, and the date and time of the commit.\n\n\nSyncing with the remote repository\nAfter committing your changes locally in Visual Studio Code, the next step is to synchronize these changes with your remote repository on GitHub. This process involves two main actions: pulling changes from the remote repository and pushing your local changes to the remote.\n\nPulling changes from the remote repository\nBefore you push your changes, it‚Äôs a good practice to pull any updates that others might have made to the remote repository. This ensures that your local repository is up-to-date. In VS Code, you can pull changes by clicking on the ... (more actions) button in the Source Control tab and selecting Pull. Alternatively, you can use the keyboard shortcut (Command+Shift+P on Mac or Ctrl+Shift+P on Windows/Linux) and type Git: Pull in the command palette. Pulling changes will merge updates from the remote repository into your local branch. If there are no conflicts, the merge will happen automatically.\n\n\nPushing changes to the remote repository\nOnce your local branch is up-to-date and you‚Äôve committed your changes, you‚Äôre ready to push these changes to the remote repository. In the Source Control tab, click on the ... button and select Push. This will upload your commits to the remote repository on GitHub. You can also use the keyboard shortcut (Command+Shift+P on Mac or Ctrl+Shift+P on Windows/Linux) and type Git: Push in the command palette. If you‚Äôre pushing to a branch that doesn‚Äôt exist on the remote, VS Code will automatically create this branch in the remote repository.\n\n\nResolving merge conflicts\nOccasionally, when you pull changes from the remote repository, you may encounter merge conflicts. These occur when changes in the remote repository overlap with your local changes in a way that Git can‚Äôt automatically resolve. VS Code provides tools to help resolve these conflicts. Conflicted files will be marked in the Source Control tab. You can open these files and choose which changes to keep. After resolving conflicts, you‚Äôll need to stage and commit the merged files before pushing.\nRegularly pulling and pushing changes will keep your local and remote repositories synchronized. This is crucial in collaborative projects to ensure everyone is working with the most current version of the project.\n\n\n\nBranching and merging\nBefore using Git, whenever I wanted to try something new in my code, I would make a copy of the entire project folder and work on that copy. This was a tedious process, and it was easy to lose track of which version was the most recent. With Git, branching makes this process much easier. Branching allows you to create a copy of your project, called a branch, and work on that branch without affecting the main project. Once you‚Äôre satisfied with the changes you‚Äôve made in your branch, you can merge those changes back into the main project. This process is much more efficient and less error-prone than manually copying and pasting files.\n\nCreating a New Branch\nIn VS Code, you can create a new branch by clicking on the branch name in the bottom left corner, then selecting Create new branch.... Give your branch a descriptive name that reflects its purpose. You can switch between branches by clicking on the branch name in the bottom left corner and selecting the branch you want to work on.\nAfter creating and switching to your new branch, any changes you make are confined to that branch. You can stage and commit changes in this branch as you would in the main branch.\nYou can also choose to publish your branch to the remote repository. This will create a copy of your branch on GitHub. This is useful if you want to collaborate with others on this branch, or to use GitHub to backup the branch. Note that once the branch is published, others who have access to the repository will be able to see that branch. To publish your branch, click on the ... button in the Source Control tab and select Publish Branch....\n\n\nMerging Branches\nOnce you‚Äôve completed the work in your branch and you‚Äôre satisfied with the changes, you‚Äôll want to merge these changes back into the main branch. Before merging, ensure your branch is up-to-date with the main branch. You can do this by checking out the main branch and pulling the latest changes, then switching back to your branch and merging the main branch into it. After that, you are ready to merge your branch into the main branch. After merging, you can delete your branch if you no longer need it. This avoids cluttering the repository with branches that are no longer needed.\nIn the Source Control tab, click on the ... button, select Merge Branch..., and choose the branch you want to merge into your current branch. If there are no conflicts, VS Code will complete the merge. VS Code will also ask you if you want to delete the merged branch.\nMerge conflicts happen when the same lines of code have been changed differently in both branches. VS Code will notify you if there are conflicts that need resolution. The first time, Git will also need you to confiure how you want to handle merge conflicts by entering one of the following commands in the terminal:\n\ngit config pull.rebase false: This command sets the pull behavior to merge. When you pull from a remote repository, Git will merge any incoming commits with your current branch. This is the one I usually use.\ngit config pull.rebase true: This command sets the pull behavior to rebase. Instead of merging incoming commits, Git will reapply your local commits on top of the incoming commits, creating a linear commit history.\ngit config pull.ff only: This command sets the pull behavior to fast-forward only. Git will only update your branch if it can fast-forward, meaning the main branch has not changed since you created your branch. If the main branch has new commits, Git will not pull the changes and you‚Äôll need to manually merge or rebase.\n\nConflicted files will be marked in the Source Control tab. Open these files, and VS Code will highlight the conflicting changes. Choose which changes to keep, then save the file, stage, and commit the resolved files. Once all conflicts are resolved and changes are committed, the branches are successfully merged. If you‚Äôve merged into your local main branch, don‚Äôt forget to push these changes to the remote repository to keep everything synchronized.\n\n\n\nPull requests and code reviews\nA pull request (PR) is a method in GitHub to propose changes from one branch to another, typically from a feature into the main branch. It‚Äôs a request to pull in your changes. The name is a bit misleading because it‚Äôs not related to the pull command in Git. You can think of it as a ‚Äúmerge request‚Äù instead. When you create a PR, you‚Äôre initiating a discussion about your proposed changes. Your collaborators can review the code, leave comments, request changes, or approve the PR.\n\nCreating a Pull Request in GitHub\nOnce you have pushed your branch to the remote repository, you can create a PR. Navigate to the repository on GitHub.com. GitHub often shows a prompt to create a PR for recent branches. If not, go to the Pull Requests tab and click New pull request. Select your branch and the branch you want to merge into (usually the main branch). When creating a PR, include a clear title and a detailed description of the changes. This helps reviewers understand the context and purpose of the changes. You can also assign reviewers to the PR, add labels, and set a milestone. Once you‚Äôre satisfied with the PR, click Create pull request. Any assigned reviewers will be notified of the PR and can begin reviewing it.\n\n\nCode Reviews\nCollaborators can review the changes in a PR by navigating to the Files changed tab within the PR. Reviewers can leave comments on specific lines of code, general comments on the PR, and suggest changes. They can also pull the branch locally and test the changes themselves. Once the review is complete, the reviewer can approve the PR, request changes, or leave a comment. If changes are requested, the PR author can make the requested changes and push them to the branch. The PR will be automatically updated with the new changes. Once the PR is approved, it can be merged into the target branch. Based on the feedback, you might need to make additional commits to your branch. These updates will automatically appear in the PR. This back-and-forth can continue until the changes are satisfactory.\n\n\nMerging the Pull Request\nOnce the PR is approved and any conflicts are resolved, you can merge it into the target branch. This is typically done via the ‚ÄòMerge pull request‚Äô button on GitHub. After merging, it‚Äôs a good practice to delete the feature branch from the remote repository to keep the branch list tidy.\n\n\nBest Practices for Pull Requests and Code Reviews\n\nSmall, Focused Changes: Aim for smaller, manageable PRs that focus on a specific feature or fix. This makes code reviews more efficient and less overwhelming.\nClear Communication: Use clear, descriptive messages in both your PRs and commits. This helps reviewers understand your thought process and the changes made.\nConstructive Feedback: When reviewing, offer constructive and respectful feedback. Code reviews are not just about finding mistakes but also about sharing knowledge and improving the codebase collaboratively.\n\nPull requests and code reviews are vital for maintaining high-quality code and fostering collaboration in your finance research projects. While not yet commonly used in academia, I have found them the perfect tools for collaborating on research projects. They ensure that every change is scrutinized and understood by all collaborators, and they foster a culture of peer review and collective improvement as the project progresses.\nGitHub offers many other features that can be useful for research projects. I list them at the end of this post and will cover them in a future post."
  },
  {
    "objectID": "posts/github-for-research/index.html#github-for-research-code",
    "href": "posts/github-for-research/index.html#github-for-research-code",
    "title": "Using GitHub for academic research",
    "section": "GitHub for research code",
    "text": "GitHub for research code\nIn empirical finance research, the ability to reproduce results is more important now than ever, especially that most top journals require authors to share their code and data. In this section, I will discuss some best practices I have adopted for using GitHub to manage research code, with an emphasis on reproducibility, documentation, and effective use of GitHub‚Äôs features.\nThe first thing to consider after creating a new repository is the structure of your project. A well-organized project is easier to navigate and understand, and it makes it easier for others to reproduce your work. There is no one-size-fits-all approach to organizing a project, but the following project structure is a good starting point:\nproject\n‚îú‚îÄ‚îÄ data/\n‚îú‚îÄ‚îÄ docs/\n‚îú‚îÄ‚îÄ output/\n    ‚îú‚îÄ‚îÄ figures/\n    ‚îî‚îÄ‚îÄ tables/\n‚îú‚îÄ‚îÄ src/\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ .env\n‚îú‚îÄ‚îÄ .env-example\n‚îú‚îÄ‚îÄ conf.yaml\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ poetry.lock\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ requirements.txt\nSo, which files should you commit to your repository? Here are some guidelines:\nYou should include:\n\nConfiguration files: Files like .json, .yml, or .ini are crucial for ensuring that your project can be set up and run by others with the exact same parameters you used. In my example the conf.yaml file contains the configuration parameters for the project and should be included in the repository.\nSource code: Include all scripts and code files that are essential for your analysis or model. In my example, the src directory contains all the Python scripts used in the project.\nDocumentation: Any files that help explain your project, especially markdown files with notes. In my example, the docs directory contains the documentation for the project and should be included in the repository. If your documentation is generated from source files, such as Markdown or Latex, then you should include the source files in the repository, not the generated files. Your repository should also include a README file at the root of the repository that provides an overview of your project, its purpose, and how to use it.\nDependencies: For projects in languages like Python, a file listing the dependencies is essential. This file lists all the external libraries and their specific versions needed for your project. This ensures that anyone cloning your repository can easily install the necessary dependencies and run your code in an environment identical to yours. In my example, I use Poetry to manage dependencies, so I include the pyproject.toml. The Poetry documentation recommends also including the poetry.lock file in the repository, unless you are writing a library for ditribution. I also include a requirements.txt file for users who prefer to install dependencies using pip instead of Poetry.\n.gitignore file: This file tells Git which files or folders to ignore in a project. Typically, you‚Äôll want to exclude certain files from being tracked ‚Äì like temporary files, local configuration files, files containing sensitive information, or large data files. GitHub offer templates, but they seem to be missing a few things. For example, if you are on Mac you will want to add .DS_Store to your ignore file. gitignore.io is another useful resource for generating .gitignore files that are much more comprehensive. Make sure to also add the .gitignore file to your repository.\n\nFinally, make sure that you include in the .gitignore file all the files and directories that you should not include in the repository.\nYou should not include:\n\nData files: While large datasets might not be feasible to store on GitHub, even small datasets can be problematic if they are updated often. Instead, consider including sample datasets or scripts that automatically fetch or generate data, or sharing your data among collaborators using a cloud storage service like Dropbox or Google Drive. There exist tools like DVC that can help you manage large datasets with version control, but I have not used them myself.\nSensitive data and local configuration files: Do not include sensitive information like passwords or API keys, or computer-specific configuration parameters such as local paths. Instead, you should include an example file with the expected parameters that need to be set in the configuration file. In my example, I use a .env file to store sensitive and local information, and I include a .env-example file that contains the name of the environment variable that needs to be set in the .env file. I would then include the .env-example file in the repository, but not the .env file. I also include the .env file in the .gitignore file so that it is not included in the repository.\nOutput: You should not include output files in the repository. Instead, you should include the code that generates the output files. Every collaborator should be able to generate the results in his environment. In my example, the output directory contains the figures and tables generated by the code in the src directory."
  },
  {
    "objectID": "posts/github-for-research/index.html#git-and-jupyter-notebooks",
    "href": "posts/github-for-research/index.html#git-and-jupyter-notebooks",
    "title": "Using GitHub for academic research",
    "section": "Git and Jupyter notebooks",
    "text": "Git and Jupyter notebooks\nJupyter Notebooks are a popular tool for data analysis and visualization. They allow users to combine code, text, and visuals in one document, making it easy to share and collaborate on data science projects. However, Jupyter Notebooks have many shortcomings when it comes to replicability and using them with Git can be challenging.\n\nChallenges with Git and Jupyter notebooks\nJupyter Notebooks, while an excellent tool for data analysis and visualization, present unique challenges when used with Git. The core issue lies in their format: Notebooks save both the input (code) and the output (results, graphs, etc.) in a single JSON file. This means that even small changes in the code can lead to large changes in the file, making it difficult for Git to handle diffs and merges effectively. The output sections, especially those with visual content, can create ‚Äúnoise‚Äù in version control. When different users run the same notebook, slight differences in output can appear, leading to unnecessary conflicts. Because Git keeps track of the full history of the notebook, the size of the repository can grow quickly, especially if the notebook contains large outputs such as images. This can make it difficult to share and collaborate on notebooks.\n\n\nVS Code Notebook Diff Viewer\nRecognizing these challenges, tools like Visual Studio Code have introduced features to help. The VS Code diff viewer (the tool that shows differences in files due to changes) supports Jupyter notebooks, allowing users to compare and understand changes between notebook versions more easily. This tool provides a clearer visualization of differences in the code, reducing the complexity involved in tracking changes in notebooks in Git.\n\n\nReplicability concerns with notebooks\nReplicability in research is crucial, and Jupyter Notebooks can sometimes hinder this. The linear, state-dependent nature of notebooks can lead to scenarios where code runs successfully in one instance but not in another, due to differences in the execution order or environment. Ensuring that notebooks are run from a clean state and in the right order is essential for replicability. Another concern is that code in notebooks is harder to test and debug than code in scripts. This can lead to errors that are difficult to detect and fix, especially in large notebooks. Finally, notebooks are not ideal for large-scale projects. As projects grow in size and complexity, notebooks can become unwieldy and difficult to manage. In these cases, it is better to use scripts or modules instead.\n\n\nUsing Notebooks with Online Platforms\nDespite these challenges, Jupyter Notebooks remain a popular and powerful tool for data analysis and research. Their interactive nature and the ability to combine code, text, and visuals in one document make them invaluable.\nPlatforms like Binder and Google Colab integrate well with Jupyter Notebooks hosted on GitHub. These platforms can automatically create interactive, shareable environments from notebooks, making them more accessible for collaborative work and education. By using these platforms, researchers can share their notebooks in a more user-friendly and interactive format, ensuring that others can easily replicate and experiment with their findings."
  },
  {
    "objectID": "posts/github-for-research/index.html#github-for-writing",
    "href": "posts/github-for-research/index.html#github-for-writing",
    "title": "Using GitHub for academic research",
    "section": "GitHub for writing",
    "text": "GitHub for writing\nGitHub is not just for code; it‚Äôs also an excellent platform for tracking your writing, especially if you are using formats based on plain-text files such as Markdown (like the Quarto publishing system) and LaTeX.\nThe same principles for organizing code projects apply to writing projects. You should include all the files that are essential to generate the output of your project, such as the source (e.g.¬†.md, .tex, and .bib) files, configuration files, and tables and figures. You should avoid including the output files, such as PDFs, or HTML files."
  },
  {
    "objectID": "posts/github-for-research/index.html#tagging-releases-for-milestones",
    "href": "posts/github-for-research/index.html#tagging-releases-for-milestones",
    "title": "Using GitHub for academic research",
    "section": "Tagging releases for milestones",
    "text": "Tagging releases for milestones\nThere are times when you want to create a snapshot of your project, including the output, at a specific point in time. For example, when you submit a paper to a journal, you want to create a snapshot of the project at that point in time. This allows you to keep track of the changes made in between revisions. GitHub provides a way to do this using tags and releases.\nWhen you reach a significant milestone in your writing ‚Äì such as the completion of a draft, submission to a journal, or final revisions ‚Äì you can create a tag and a corresponding release.\nTo create a tag and release, head to the repository on GitHub.com and click on Create a new release under Releases in the right sidebar. Enter a tag version number and a title for the release. You can also add release notes summarizing the changes or updates in this version. Finally, attach the output files (e.g.¬†PDFs) to the release. Click Publish release to create the release. You can then download the release files or share the release link with others."
  },
  {
    "objectID": "posts/github-for-research/index.html#publishing-your-code-on-github",
    "href": "posts/github-for-research/index.html#publishing-your-code-on-github",
    "title": "Using GitHub for academic research",
    "section": "Publishing your code on GitHub",
    "text": "Publishing your code on GitHub\nIn empirical finance academic research, sharing your code has become increasingly important. Publishing your code enhances the transparency and reproducibility of your research. It allows peers to review, replicate, and build upon your work, contributing to the collective knowledge of the field. Making your code available can also increase the citation and impact of your research, as it provides tangible artifacts that others can use and reference. Finally, it is also a requirement for publishing in many journals, including the top ones.\nJournals will publish your code alongside your paper, so why should you also publish it on GitHub?\nFor me, the main reason is to keep control over my code. By publishing your code on GitHub, you retain control over it. You can continue to make changes to it, and update it as needed. Other researchers who visit your GitHub repository can also be exposed to your other work, increasing the visibility of your research. Finally, GitHub offers a platform for collaboration and feedback, allowing others to flag issues, contribute to your work, and build upon it.\nTo publish your code on GitHub, all you need to do is set the visibility of your repository to public. If you don‚Äôt want to share the full history of your code, you can create a new repository and upload the latest version of your code.\nMake sure to include a README file that explains what your project does, how to set it up, and how to use it. Documentation is key to making your code accessible to others (and to reducing the number of questions you get about your code). You can also include instructions on how to cite your code in the README file. Finally, you should also include a LICENSE file to clearly state how others can use your code.\nOnce you have completed these steps, your code is published! If you want a DOI for your repository, you can use Zenodo, which allows you to mint a DOI for your GitHub repository."
  },
  {
    "objectID": "posts/github-for-research/index.html#other-github-features-for-academic-researchers",
    "href": "posts/github-for-research/index.html#other-github-features-for-academic-researchers",
    "title": "Using GitHub for academic research",
    "section": "Other GitHub features for academic researchers",
    "text": "Other GitHub features for academic researchers\nIn addition to the core features of GitHub, many other tools and functionalities can be useful for academic researchers. I plan on covering most of them in future posts. Here are the ones that I use the most:\n\nProject Management Tools\nGitHub offers several tools to help you manage your projects, including Projects, Issues, Discussions, and Wikis. These tools can be used to organize your work, track tasks, and collaborate with others.\n\n\nGitHub Copilot\nGitHub Copilot is an AI coding assistant. It can do code completion, suggest functions, and even generate code based on comments. There is also a Copilot Chat powered by GPT-4 that can answer questions about code while being aware of the context. When you allow it, it can consult your private repositories to provide more relevant suggestions.\nSeriously, if you haven‚Äôt tried it yet, you should. It‚Äôs a game-changer, and new features are being added all the time. And it will work for text too if you write your Markdown or LaTeX files in VS Code.\n\n\nGitHub Pages\nGitHub Pages is a free service that allows you to host static websites directly from GitHub. This can be useful for hosting project websites, blogs, or personal websites. My personal website and this blog are both hosted on GitHub Pages.\nHere are some tools that can be useful for generating static websites suitable for GitHub Pages:\n\nQuarto: Quarto is an open-source scientific and technical publishing system built on Pandoc. It is excellent for publishing computational notebooks, including Jupyter and Markdown notebooks. This blog and my personal website are built using Quarto.\nAcademic Pages Template (Jekyll): This Jekyll-based template is designed for academic personal websites and can be hosted on GitHub Pages. It‚Äôs a great way to create a professional online presence showcasing your research work. It‚Äôs what I used for my personal website before switching to Quarto.\nNikola and Pelican: If you want something all Python-based, Nikola and Pelican are two good options. I have used Nikola in the past and found it to be a great tool for generating static websites, but I found it simpler to consolidate everything in Quarto.\n\n\n\nGitHub Classroom\nGitHub Classroom simplifies the use of GitHub in classroom settings. It‚Äôs a toolset that automates the repetitive tasks involved in grading and feedback, making it easier to use GitHub for coursework and assignments in a research or academic context. I have been using it for three years and it has been a game-changer for me. While it‚Äôs not bug-free, it has saved me countless hours of grading and feedback. Automated grading has a monthly limit after which you need to pay, but the cost is minimal and well worth it.\n\n\nGitHub Actions\nGitHub Actions is a powerful tool that allows you to automate workflows. You can set up CI/CD pipelines2 to automate testing, building, and deploying your applications or research code. GitHub Actions are small scripts that run in response to events in your repository, such as commit or pull requests. For researchers, it can be used to automate the testing of code or even automate routine data processing tasks.\n\n\nGitHub Codespaces\nGitHub Codespaces provide a fully featured cloud development environment accessible directly from GitHub. Your code lives in a remote server, and you get a complete VS Code environment in your browser. This can be particularly useful for researchers who want to quickly experiment with code or collaborate without the need to set up a local development environment. It is also great to ensure maximum replicability of the code you distribute, as the environment is identical for everyone."
  },
  {
    "objectID": "posts/github-for-research/index.html#footnotes",
    "href": "posts/github-for-research/index.html#footnotes",
    "title": "Using GitHub for academic research",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHistorically, this branch was called master, but GitHub has recently changed the default branch name to main to avoid the racially charged connotations of the word master.‚Ü©Ô∏é\nContinuous integration and continuous development‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "New Year, new blog",
    "section": "",
    "text": "Happy New Year and welcome to the first post of Vincent Codes Finance! As we step into a fresh year, I‚Äôm thrilled to launch this new blog and companion YouTube channel dedicated to Python and coding in empirical finance research. I‚Äôm excited to share my knowledge and experience with you."
  },
  {
    "objectID": "posts/welcome/index.html#about-me",
    "href": "posts/welcome/index.html#about-me",
    "title": "New Year, new blog",
    "section": "About me",
    "text": "About me\nI‚Äôm an Associate Professor of Finance at HEC Montr√©al where I have been teaching Empirical Finance at the Master‚Äôs level since 2020. My research interests include information economics, market microstructure, big data and machine learning applications in finance, fintech, and cybersecurity in finance. I‚Äôm also a Python enthusiast and curious about anything related to coding, data science, and machine learning. See my academic page for more details."
  },
  {
    "objectID": "posts/welcome/index.html#why-this-blog",
    "href": "posts/welcome/index.html#why-this-blog",
    "title": "New Year, new blog",
    "section": "Why this blog?",
    "text": "Why this blog?\nVincent Codes Finance is more than just a blog; it‚Äôs my contribution to enhancing empirical finance research through proficient coding practices. Here, I‚Äôll share my insights, experiences, and love for Python, aiming to elevate the quality and efficiency of research. Whether you‚Äôre a student, a fellow researcher, or an enthusiast, there‚Äôs something here for you.\nMy motivation for creating Vincent Codes Finance is driven by a blend of professional ambition and personal passion:\n\nEnhancing Communication and Presentation Skills: This is one of my New Year‚Äôs resolutions. By regularly writing and producing content, I aim to refine my ability to present complex ideas clearly and engagingly.\nCreating a Valuable Resource: I‚Äôve witnessed firsthand the challenges students and co-authors face when dealing with empirical finance research, especially in coding and data analysis. Vincent Codes Finance is my channel to provide a structured, accessible resource to elevate the quality of their work and mine.\nSharing a Passion for Coding and Tools: Coding is not just about algorithms and syntax; it‚Äôs a way of thinking and problem-solving. I‚Äôm excited to share how powerful coding can be in the realm of finance research and to explore and introduce various computer tools that can transform our research methodologies.\nContributing to the Field: With years of experience under my belt, I believe I have unique insights and skills that can benefit others. By sharing my knowledge, I aim to contribute to the broader community of finance researchers and practitioners.\nPromoting High-Quality, Replicable Research: The landscape of empirical finance is rapidly changing with the increasing importance of data and computational tools. I am committed to promoting practices that ensure research is not only innovative but also replicable and robust, contributing to the credibility and reliability of financial research."
  },
  {
    "objectID": "posts/welcome/index.html#about-vcf",
    "href": "posts/welcome/index.html#about-vcf",
    "title": "New Year, new blog",
    "section": "About VCF",
    "text": "About VCF\nWe‚Äôll start with the basics, setting a strong foundation for everyone. The initial posts will cover installing Python, setting up your development environment, understanding Python fundamentals, and introducing data analysis using pandas. These topics are essential for anyone who wants to get started with Python and coding in empirical finance research and will be released throughout January to align with my current teaching.\nCome February, we‚Äôll dive into more advanced topics such as handling dates and timestamps efficiently, diverse file formats for research data, using git and GitHub for collaboration, and techniques to speed up your computations.\nYour input is invaluable! I encourage you to send in your topic suggestions, questions, or any insights you might have. For all blog-related inquiries, please reach out to me at vincent@codes.finance.\nFollow Along! Don‚Äôt miss out on any updates! Subscribe to the newsletter, the YouTube channel, the blog‚Äôs RSS feed, and follow me on X. Let‚Äôs raise the bar for empirical finance research together.\nI look forward to sharing this journey with you and seeing where it takes us.\nCheers to a year of coding, discovery, and breakthroughs in empirical finance research!\n- Vincent"
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html",
    "href": "posts/timeseries-standard-errors/index.html",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "",
    "text": "In this post, I show how to estimate standard errors in OLS regressions of time series data with Python and the statsmodels library.\nMore specifically, I show how to estimate OLS models with:\nIf you just want the code examples with no explanations, jump to the cheat sheet at the end of the post.\nFor panel data models, see my previous post on estimating standard errors in panel data models with Python and linearmodels."
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#video-tutorial",
    "href": "posts/timeseries-standard-errors/index.html#video-tutorial",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "Video tutorial",
    "text": "Video tutorial\nPart of this post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#time-series-data",
    "href": "posts/timeseries-standard-errors/index.html#time-series-data",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "Time series data",
    "text": "Time series data\nTime series data is ubiquitous in finance research, but it can present challenges when using it in OLS regressions because it can violate the assumptions of OLS. The main challenge is that the error term can be correlated across time periods and can be heteroskedastic, meaning that the variance of the error term is not constant. This can lead to biased standard errors and invalid hypothesis tests. Time series data models have the following form:\n\\[\ny_{t} =  x_{t}\\beta  + \\epsilon_{t},\n\\]\nwhere \\(y_{t}\\) is the dependent variable at time \\(t\\in [1,T]\\), \\(x_{t}\\) is a vector of independent variables, \\(\\beta\\) is a vector of coefficients, and \\(\\epsilon_{t}\\) is the error term. If the model includes an intercept, then the first element of \\(x_{t}\\) is 1 and the first element of \\(\\beta\\) is \\(\\alpha\\).\nWhen we get to the data, we write the model in matrix form:\n\\[\n\\mathbf{y} = \\mathbf{X}\\beta + \\mathbf{\\epsilon},\n\\]\nwhere \\(\\mathbf{y}\\) is a \\(T \\times 1\\) vector of dependent variables, \\(\\mathbf{X}\\) is a \\(T \\times K\\) matrix of independent variables, \\(\\beta\\) is a \\(K \\times 1\\) vector of coefficients, and \\(\\mathbf{\\epsilon}\\) is a \\(T \\times 1\\) vector of error terms. The number of observations is \\(T\\).\n\n\n\n\n\n\nTipMore on matrix form representation\n\n\n\n\n\nThe observations are then stacked in their temporal order. For example, the vector \\(\\mathbf{y}\\) of elements \\(y_{t}\\) is defined as:\n\\[\n\\mathbf{y} = \\left[\\begin{array}{c} y_{1}\\\\ y_{2}\\\\ \\vdots  \\\\  y_{T-1} \\\\ y_{T} \\end{array} \\right],\n\\]\nThe matrix \\(\\mathbf{X}\\) formed with vectors \\(x_{t}\\) is defined as:\n\\[\n\\mathbf{X} = \\left[\\begin{array}{ccccc}\nx_{1}^{(1)} & x_{1}^{(2)} & \\cdots & x_{1}^{(K-1)} & x_{1}^{(K)} \\\\\nx_{2}^{(1)} & x_{2}^{(2)} & \\cdots & x_{2}^{(K-1)} & x_{2}^{(K)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nx_{T-1}^{(1)} & x_{T-1}^{(2)} & \\cdots & x_{T-1}^{(K-1)} & x_{T-1}^{(K)}\\\\\nx_{T}^{(1)} & x_{T}^{(2)} & \\cdots & x_{T}^{(K-1)} & x_{T}^{(K)}\n\\end{array} \\right],\n\\]\nwhere \\(x_{t}^{(j)}\\) is the \\(j\\)-th element of the vector \\(x_{t}\\).\nFinally, the \\(\\beta\\) vector is defined as usual:\n\\[\n\\beta = \\left[\\begin{array}{c}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_{K-1} \\\\ \\beta_{K} \\end{array} \\right].\n\\]\nIf the model includes an intercept, then the first column of \\(\\mathbf{X}\\) is a column of ones (i.e.¬†all the \\(x_{t}^{(1)}\\equiv 1\\)) and the first element of \\(\\beta\\) is \\(\\alpha\\):\n\\[\n\\beta = \\left[\\begin{array}{c}\\alpha \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{K-2} \\\\ \\beta_{K-1} \\end{array} \\right].\n\\]\n\n\n\nThe goal is then to estimate the coefficients \\(\\beta\\) and their associated standard errors. The standard errors are important because they allow us to perform statistical inference. In the context of financial anomalies, or more generally trading strategies, we can test whether the \\(\\alpha\\) is statistically different from zero."
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#test-data",
    "href": "posts/timeseries-standard-errors/index.html#test-data",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "Test data",
    "text": "Test data\nIn this post, I explain how to estimate standard errors in time series regressions with Python and the statsmodels library. I do so using the anomalies data set available on Open Source Asset Pricing, which contains monthly returns for the anomalies discussed in Chen and Zimmermann (2022). More specifically, I use daily returns of the 12-month momentum anomaly in the PredictorVW.zip file available at this link.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"Mom12m_ret.csv\", parse_dates=[\"date\"], index_col=[\"date\"])\ndf\n\n\n\n\n\n\n\n\nport01\nport02\nport03\nport04\nport05\nport06\nport07\nport08\nport09\nport10\nportLS\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1927-01-03\n-0.312789\n-1.315867\n-0.361545\n-0.557918\n-1.252321\n-0.310728\n-0.767765\n-0.796250\n-0.649846\n-1.267020\n-0.954230\n\n\n1927-01-04\n1.845338\n0.659064\n0.828047\n0.102919\n0.213863\n-0.011713\n0.127618\n0.102198\n0.659057\n0.874677\n-0.970662\n\n\n1927-01-05\n1.012461\n0.724596\n0.581067\n-0.152049\n0.079294\n0.288886\n0.283217\n-0.080991\n0.110790\n0.048784\n-0.963677\n\n\n1927-01-06\n-0.902011\n0.289840\n-0.462931\n-1.100983\n-0.223404\n-0.010662\n-0.121442\n-0.095102\n-0.142538\n-0.014550\n0.887461\n\n\n1927-01-07\n-0.332090\n0.286702\n0.062731\n0.221905\n0.152737\n0.093068\n0.456891\n0.672505\n0.225237\n0.209930\n0.542021\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-12-23\n-1.492861\n-0.940321\n-0.087377\n0.254381\n0.580743\n0.502287\n0.482743\n0.588520\n0.295648\n1.433918\n2.926779\n\n\n2022-12-27\n-3.014401\n-3.667398\n-1.638701\n-0.776100\n-0.984944\n-0.153435\n-0.000316\n0.146955\n-0.746434\n0.281178\n3.295579\n\n\n2022-12-28\n-0.475730\n-1.287840\n-0.931057\n-1.367397\n-1.121403\n-1.162237\n-1.097983\n-0.922127\n-1.433636\n-1.478748\n-1.003018\n\n\n2022-12-29\n6.153138\n4.639327\n3.960907\n3.063374\n2.489143\n2.013535\n1.798595\n1.207053\n1.708395\n0.742981\n-5.410157\n\n\n2022-12-30\n0.607938\n0.718418\n0.117090\n-0.167288\n-0.194521\n-0.461471\n-0.395566\n-0.365650\n-0.181250\n-0.002331\n-0.610269\n\n\n\n\n25249 rows √ó 11 columns\n\n\n\nI complement the momentum strategy with the daily Fama-French 5 factors available from Ken French‚Äôs data library.\n\nff5_df = pd.read_csv(\n    \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_daily_CSV.zip\",\n    skiprows=2,\n    parse_dates=[0],\n    index_col=[0],\n)\n# Clean column names for formula api\nff5_df.columns = [col.replace(\"-\", \"\") for col in ff5_df.columns]\nff5_df.index.name = \"date\"\nff5_df\n\n\n\n\n\n\n\n\nMktRF\nSMB\nHML\nRMW\nCMA\nRF\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n1963-07-01\n-0.67\n0.02\n-0.35\n0.03\n0.13\n0.012\n\n\n1963-07-02\n0.79\n-0.28\n0.28\n-0.08\n-0.21\n0.012\n\n\n1963-07-03\n0.63\n-0.18\n-0.10\n0.13\n-0.25\n0.012\n\n\n1963-07-05\n0.40\n0.09\n-0.28\n0.07\n-0.30\n0.012\n\n\n1963-07-08\n-0.63\n0.07\n-0.20\n-0.27\n0.06\n0.012\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-22\n0.21\n0.61\n0.09\n-0.64\n0.19\n0.021\n\n\n2023-12-26\n0.48\n0.81\n0.46\n-0.34\n-0.15\n0.021\n\n\n2023-12-27\n0.16\n0.16\n0.12\n-0.31\n-0.14\n0.021\n\n\n2023-12-28\n-0.01\n-0.38\n0.03\n-0.32\n0.15\n0.021\n\n\n2023-12-29\n-0.43\n-1.13\n-0.37\n0.67\n-0.07\n0.021\n\n\n\n\n15229 rows √ó 6 columns\n\n\n\nFinally, I join the two data sets to create a single data set with the following columns:\n\ndf = df[[\"portLS\"]].join(ff5_df[[\"MktRF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]], how=\"inner\")\ndf\n\n\n\n\n\n\n\n\nportLS\nMktRF\nSMB\nHML\nRMW\nCMA\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n1963-07-01\n0.127573\n-0.67\n0.02\n-0.35\n0.03\n0.13\n\n\n1963-07-02\n0.680630\n0.79\n-0.28\n0.28\n-0.08\n-0.21\n\n\n1963-07-03\n0.184325\n0.63\n-0.18\n-0.10\n0.13\n-0.25\n\n\n1963-07-05\n0.304217\n0.40\n0.09\n-0.28\n0.07\n-0.30\n\n\n1963-07-08\n-0.216363\n-0.63\n0.07\n-0.20\n-0.27\n0.06\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-12-23\n2.926779\n0.51\n-0.34\n1.16\n0.86\n0.46\n\n\n2022-12-27\n3.295579\n-0.51\n-0.44\n1.42\n1.16\n1.22\n\n\n2022-12-28\n-1.003018\n-1.23\n-0.30\n-0.29\n-0.96\n-0.04\n\n\n2022-12-29\n-5.410157\n1.87\n1.03\n-1.07\n-1.02\n-0.82\n\n\n2022-12-30\n-0.610269\n-0.22\n0.14\n-0.03\n-0.54\n0.00\n\n\n\n\n14979 rows √ó 6 columns"
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#statsmodels",
    "href": "posts/timeseries-standard-errors/index.html#statsmodels",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "statsmodels",
    "text": "statsmodels\nThe statsmodels library is a Python package that provides many functions for estimating statistical models and conducting statistical tests.\n\nInstallation\nYou can install the library with your package manager of choice:\npoetry:\npoetry add statsmodels\npip:\npip install statsmodels\nconda:\nconda install statsmodels\n\n\nUsage\nThe library is easy to use for anyone familiar with the models used. However, reference to the documentation is a must to understand how to use all the available options. In this post, I will use the linear regression model estimated by ordinary least squares (OLS). Statmodels provides two APIs: their usual1 API and the formula API that allows you to specify the model using a formula string similar to R. I will use the formula API in this post. The two APIs are imported as follows:\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nYou define model a model by passing a string with the formula and a pandas DataFrame with the data to the from_formula method of the model class. For example, to create our OLS model that regresses the momentum strategy on the Fama-French 5 factors, we use the following code:\n\nmod = smf.ols(\"portLS ~ MktRF + SMB + HML + RMW + CMA\", data=df)"
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#ols-coefficients-and-standard-errors",
    "href": "posts/timeseries-standard-errors/index.html#ols-coefficients-and-standard-errors",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "OLS coefficients and standard errors",
    "text": "OLS coefficients and standard errors\nThe first model I estimate is a vanilla OLS model. In our example with the momentum strategy, the \\(\\beta\\) vector is \\([ \\alpha\\ \\beta_{MktRF}\\ \\beta_{SMB}\\ \\beta_{HML}\\ \\beta_{RMW}\\ \\beta_{CMA}]'\\).\nThe OLS estimator is the most common in finance research. It is also the most efficient estimator when the errors are homoskedastic and uncorrelated across time. The OLS estimator is defined as:\n\\[\n\\hat{\\beta}_{OLS} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y},\n\\]\nwhere \\(\\mathbf{X}\\) is the matrix of exogenous variables and \\(\\mathbf{y}\\) is the vector of dependent variables. The covariance matrix \\(\\Sigma\\) of the estimator is defined as:\n\\[\n\\Sigma_{OLS}  =\\sigma_{\\mathbf{\\epsilon}}^{2}\\cdot\\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1},\n\\]\nwhere \\(\\sigma_{\\mathbf{\\epsilon}}^{2}\\) is usually estimated with the residuals \\(\\mathbf{\\hat{e}}=( \\mathbf{y}-\\mathbf{X}\\hat{\\beta}_{OLS})\\) from the OLS regression:\n\\[\n\\hat{\\sigma}_{\\mathbf{\\epsilon}}^{2} =\\frac{1}{N-K}\\mathbf{\\hat{e}}'\\mathbf{\\hat{e}}.\n\\]\nThe estimate of the covariance matrix is then:\n\\[\n\\hat{\\Sigma}_{OLS}  =\\frac{1}{N-K}\\mathbf{\\hat{e}}'\\mathbf{\\hat{e}}\\cdot\\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1},\n\\]\n\\[\n\\hat{\\Sigma}_{OLS}  =\\left[\\begin{array}{ccccc}\n\\hat{\\sigma}_{\\beta_1}^2 & \\hat{\\sigma}_{\\beta_1,\\beta_2} & \\cdots & \\hat{\\sigma}_{\\beta_1,\\beta_{K-1}} & \\hat{\\sigma}_{\\beta_1,\\beta_K} \\\\\n\\hat{\\sigma}_{\\beta_2,\\beta_1} & \\hat{\\sigma}_{\\beta_2}^2 & \\cdots & \\hat{\\sigma}_{\\beta_2,\\beta_{K-1}} & \\hat{\\sigma}_{\\beta_2,\\beta_K} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\hat{\\sigma}_{\\beta_{K-1},\\beta_1} & \\hat{\\sigma}_{\\beta_{K-1},\\beta_2} & \\cdots & \\hat{\\sigma}_{\\beta_{K-1}}^2 & \\hat{\\sigma}_{\\beta_{K-1},\\beta_K} \\\\\n\\hat{\\sigma}_{\\beta_K,\\beta_1} & \\hat{\\sigma}_{\\beta_K,\\beta_2} & \\cdots & \\hat{\\sigma}_{\\beta_K,\\beta_{K-1}} & \\hat{\\sigma}_{\\beta_K}^2\n\\end{array} \\right].\n\\]\nTo estimate the OLS model with statsmodels, we call the fit() method. The fit() method returns a regression results object, which contains the estimated coefficients, standard errors, and other statistics. The results object has a summary() Depending on the parameters passed to the fit() method, we can get z-statistics or t-statistics. I always use the use_t=True option to make sure I get t-statistics in the summary table.\n\nmod.fit(use_t=True).summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nportLS\nR-squared:\n0.128\n\n\nModel:\nOLS\nAdj. R-squared:\n0.128\n\n\nMethod:\nLeast Squares\nF-statistic:\n439.1\n\n\nDate:\nThu, 01 Feb 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n05:38:44\nLog-Likelihood:\n-26910.\n\n\nNo. Observations:\n14979\nAIC:\n5.383e+04\n\n\nDf Residuals:\n14973\nBIC:\n5.388e+04\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0928\n0.012\n7.767\n0.000\n0.069\n0.116\n\n\nMktRF\n-0.0635\n0.013\n-4.980\n0.000\n-0.089\n-0.039\n\n\nSMB\n-0.2919\n0.023\n-12.510\n0.000\n-0.338\n-0.246\n\n\nHML\n-0.9909\n0.025\n-39.510\n0.000\n-1.040\n-0.942\n\n\nRMW\n0.3269\n0.032\n10.179\n0.000\n0.264\n0.390\n\n\nCMA\n0.6359\n0.040\n15.712\n0.000\n0.557\n0.715\n\n\n\n\n\n\n\n\nOmnibus:\n2945.727\nDurbin-Watson:\n1.724\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n43704.781\n\n\nSkew:\n-0.523\nProb(JB):\n0.00\n\n\nKurtosis:\n11.302\nCond. No.\n3.84\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#white-standard-errors",
    "href": "posts/timeseries-standard-errors/index.html#white-standard-errors",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "White standard errors",
    "text": "White standard errors\nWith time series data, the errors are often heteroskedastic and correlated across time. In this case, the OLS standard errors are incorrect.\nA common solution to the problem of heteroskedasticity, i.e.¬†that the variance of the error term is not constant, is to use heteroskedasticity-robust standard errors. The most common heteroskedasticity-robust standard errors estimator is the White (1980) estimator, which is defined as\n\\[\n\\hat{\\Sigma}_{White} = (\\mathbf{X}' \\mathbf{X})^{-1}\\cdot\\left[\\mathbf{X}' \\cdot \\hat{\\Omega}_0  \\cdot \\mathbf{X}\\right] \\cdot\\left(\\mathbf{X}' \\mathbf{X}\\right)^{-1},\n\\]\nwhere \\(\\hat{\\Omega}_0\\) is a diagonal matrix with the squared residuals \\(\\mathbf{\\hat{e}}^{2}\\) on the diagonal, i.e., with the square of the j-th residual \\(\\hat{e}_{j}^{2}\\) at position \\((j,j)\\) and \\(0\\) elsewhere. The matrix \\(\\hat{\\Omega}_0\\) is defined as:\n\\[\n\\hat{\\Omega}_0= \\left[\\begin{array}{cccccccc}\n\\hat{e}_{1}^{2} & 0 &  \\cdots & 0 & 0 \\\\\n0 & \\hat{e}_{2}^{2} &  \\cdots & 0 & 0\\\\\n\\vdots & \\vdots & \\ddots  & \\vdots & \\vdots\\\\\n0 & 0 &  \\cdots & \\hat{e}_{T-1}^{2} & 0 \\\\\n0 & 0 &  \\cdots & 0 & \\hat{e}_{T}^{2}\n\\end{array} \\right].\n\\]\nThis is referred to as the HC0 estimator in statsmodels. MacKinnon and White (1985) introduced three new estimators, HC1, HC2, and HC3, which are also available in statsmodels. The HC1 estimator is the same as the HC0 estimator with \\(\\hat{\\Omega}_0\\) normalized by a factor of \\(T/(T-K)\\) to make the estimator unbiased, where \\(T\\) is the number of observations and \\(K\\) is the number of independent variables.\nThe ols model we created can be used to estimate OLS models with White standard errors. The following code estimates the OLS coefficients and White standard errors:\n\nmod.fit(cov_type=\"HC0\", use_t=True).summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nportLS\nR-squared:\n0.128\n\n\nModel:\nOLS\nAdj. R-squared:\n0.128\n\n\nMethod:\nLeast Squares\nF-statistic:\n94.22\n\n\nDate:\nThu, 01 Feb 2024\nProb (F-statistic):\n4.93e-98\n\n\nTime:\n05:38:28\nLog-Likelihood:\n-26910.\n\n\nNo. Observations:\n14979\nAIC:\n5.383e+04\n\n\nDf Residuals:\n14973\nBIC:\n5.388e+04\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nHC0\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0928\n0.012\n7.675\n0.000\n0.069\n0.117\n\n\nMktRF\n-0.0635\n0.020\n-3.168\n0.002\n-0.103\n-0.024\n\n\nSMB\n-0.2919\n0.038\n-7.741\n0.000\n-0.366\n-0.218\n\n\nHML\n-0.9909\n0.056\n-17.570\n0.000\n-1.101\n-0.880\n\n\nRMW\n0.3269\n0.069\n4.724\n0.000\n0.191\n0.463\n\n\nCMA\n0.6359\n0.081\n7.820\n0.000\n0.477\n0.795\n\n\n\n\n\n\n\n\nOmnibus:\n2945.727\nDurbin-Watson:\n1.724\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n43704.781\n\n\nSkew:\n-0.523\nProb(JB):\n0.00\n\n\nKurtosis:\n11.302\nCond. No.\n3.84\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity robust (HC0)\n\n\nThe estimated coefficients are the same as with the OLS standard errors, but the standard errors are different. In turn, all statistical measures that depend on the standard errors, such as t-statistics and p-values, are different."
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#newey-west-standard-errors",
    "href": "posts/timeseries-standard-errors/index.html#newey-west-standard-errors",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "Newey-West standard errors",
    "text": "Newey-West standard errors\nThe White estimator takes into account only heteroskedasticity, but not autocorrelation. For that, we can use a hetoroskedasticity and autocorrelation robust (HAR)2 standard errors estimator.\nIn empirical finance research, it is common in practice to use Newey-West standard errors (introduced in Newey and West 1987) to account for both heteroskedasticity and autocorrelation. The Newey-West estimator is defined as:\n\\[\n\\hat{\\Sigma}_{NW} = (\\mathbf{X}' \\mathbf{X})^{-1}\\cdot \\hat{S}_{HAR} \\cdot\\left(\\mathbf{X}' \\mathbf{X}\\right)^{-1},\n\\]\nwhere \\(\\hat{S}_{HAR}\\) is a matrix that takes into account both heteroskedasticity and autocorrelation:\n\\[\n\\hat{S}_{HAR} = \\left[\\mathbf{X}' \\cdot \\hat{\\Omega}_0  \\cdot \\mathbf{X}\\right] +\\sum_{j=1}^{l}\\omega(j,l)  \\cdot\\sum_{t=j+1}^{T}\\left(\nx_{t}\\hat{e}_{t}\\hat{e}_{t-j}x_{t-j}^{\\prime}+x_{t-j}\\hat{e}%\n_{t-j}\\hat{e}_{t}x_{t}^{\\prime}\\right),\n\\]\nwhere \\(\\hat{e}_{i}\\) is the \\(i\\)-th element of the vector \\(\\mathbf{\\hat{e}}\\). The matrix \\(\\hat{S}_{HAR}\\) is a weighted sum of the autocovariances of the residuals \\(\\mathbf{\\hat{e}}\\), where the kernel \\(\\omega(j,l)\\) provides the weight for lag \\(j\\) with a bandwith (maximum lag) of \\(l\\). The Newey-West estimator uses a Bartlett kernel, which is defined as:\n\\[\n\\omega(j,l) = \\left\\{ \\begin{array}{ll}\n1 - \\frac{|j|}{l+1} & \\text{if } |j| \\leq l \\\\\n0 & \\text{if } |j| &gt; l\n\\end{array} \\right.\n\\]\nThe truncated uniform kernel used by Hansen and Hodrick (1980) is also available in statsmodels.\nTo use Newey-West standard errors, you pass the following arguments to the fit() method:\n\ncov_type=\"HAC\": To use HAC standard errors. See the documentation on Kernel (HAC) for more details.\ncov_kwds={\"kernel\": \"bartlett\", \"maxlags\": 5}: A dictionary with the kernel and the number of lags to use (maxlags). Use the uniform kernel for the truncated uniform kernel of Hansen and Hodrick (1980).\n\n\nmod.fit(\n    cov_type=\"HAC\", cov_kwds={\"kernel\": \"bartlett\", \"maxlags\": 5}, use_t=True\n).summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nportLS\nR-squared:\n0.128\n\n\nModel:\nOLS\nAdj. R-squared:\n0.128\n\n\nMethod:\nLeast Squares\nF-statistic:\n58.10\n\n\nDate:\nThu, 01 Feb 2024\nProb (F-statistic):\n4.28e-60\n\n\nTime:\n05:38:28\nLog-Likelihood:\n-26910.\n\n\nNo. Observations:\n14979\nAIC:\n5.383e+04\n\n\nDf Residuals:\n14973\nBIC:\n5.388e+04\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nHAC\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0928\n0.014\n6.837\n0.000\n0.066\n0.119\n\n\nMktRF\n-0.0635\n0.025\n-2.532\n0.011\n-0.113\n-0.014\n\n\nSMB\n-0.2919\n0.047\n-6.275\n0.000\n-0.383\n-0.201\n\n\nHML\n-0.9909\n0.073\n-13.596\n0.000\n-1.134\n-0.848\n\n\nRMW\n0.3269\n0.100\n3.261\n0.001\n0.130\n0.523\n\n\nCMA\n0.6359\n0.103\n6.191\n0.000\n0.435\n0.837\n\n\n\n\n\n\n\n\nOmnibus:\n2945.727\nDurbin-Watson:\n1.724\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n43704.781\n\n\nSkew:\n-0.523\nProb(JB):\n0.00\n\n\nKurtosis:\n11.302\nCond. No.\n3.84\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 5 lags and without small sample correction"
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#cheat-sheet",
    "href": "posts/timeseries-standard-errors/index.html#cheat-sheet",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "Cheat sheet",
    "text": "Cheat sheet\nThe following table summarizes the standard errors estimators in statsmodels and their corresponding methods:\n# Import the formula API\nimport statsmodels.formula.api as smf\n\n# Define OLS model\nmod = smf.ols(\"portLS ~ MktRF + SMB + HML + RMW + CMA\", data=df)\n\n# Estimate OLS model with OLS standard errors\nres = mod.fit(use_t=True)\n\n# Estimate OLS model with White standard errors\nres = mod.fit(cov_type=\"HC0\", use_t=True)\n\n# Estimate OLS model with Newey-West standard errors\nres = mod.fit(\n    cov_type=\"HAC\", cov_kwds={\"kernel\": \"bartlett\", \"maxlags\": 5}, use_t=True\n)\n\n# Estimate OLS model with Hansen-Hodrick standard errors\nres = mod.fit(\n    cov_type=\"HAC\", cov_kwds={\"kernel\": \"uniform\", \"maxlags\": 5}, use_t=True\n)"
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#learn-more",
    "href": "posts/timeseries-standard-errors/index.html#learn-more",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "Learn more",
    "text": "Learn more\nTo learn more about the statsmodels library, I recommend reading the documentation, especially the section on linear regression.\nUsing widely-used software tools like statsmodels is a good way to make your research more reproducible and transparent. However, like any tool, it is important to understand the underlying econometric theory. If you are interested in learning more about the standard errors discussed in this post, I recommend reading White (1980), Newey and West (1987), MacKinnon and White (1985), and Hansen and Hodrick (1980)."
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#references",
    "href": "posts/timeseries-standard-errors/index.html#references",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "References",
    "text": "References\n\n\nChen, Andrew Y., and Tom Zimmermann. 2022. ‚ÄúOpen Source Cross-Sectional Asset Pricing.‚Äù Critical Finance Review 27 (2): 207‚Äì64. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3604626.\n\n\nHansen, Lars Peter, and Robert J Hodrick. 1980. ‚ÄúForward Exchange Rates as Optimal Predictors of Future Spot Rates: An Econometric Analysis.‚Äù Journal of Political Economy 88 (5): 829‚Äì53. https://www.journals.uchicago.edu/doi/abs/10.1086/260910.\n\n\nMacKinnon, James G, and Halbert White. 1985. ‚ÄúSome Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.‚Äù Journal of Econometrics 29 (3): 305‚Äì25. https://doi.org/10.1016/0304-4076(85)90158-7.\n\n\nNewey, Whitney K, and Kenneth D West. 1987. ‚ÄúA Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.‚Äù Econometrica 55 (3): 703‚Äì8. https://www.jstor.org/stable/1913610.\n\n\nWhite, Halbert. 1980. ‚ÄúA Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.‚Äù Econometrica, 817‚Äì38. https://doi.org/10.2307/1912934."
  },
  {
    "objectID": "posts/timeseries-standard-errors/index.html#footnotes",
    "href": "posts/timeseries-standard-errors/index.html#footnotes",
    "title": "Estimating standard errors in time series data with Python and statsmodels",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is how it is called in the documentation.‚Ü©Ô∏é\nAlso called heteroskedasticity and autocorrelation consistent (HAC) standard errors.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/devcontainers/index.html",
    "href": "posts/devcontainers/index.html",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "",
    "text": "In empirical finance research, the tools and methodologies we employ play a crucial role in ensuring the integrity and reproducibility of our findings. One such powerful tool is containerization, which allows you to encapsulate your code and its dependencies into a standardized unit. Development containers, or devcontainers, provide a convenient way to create isolated environments for your data analysis tasks, ensuring that your code runs consistently across different systems.\nContainers are essentially lightweight, portable environments that package up code and all its dependencies, ensuring that the software runs consistently across different computing environments. This consistency is particularly valuable in research settings where the reproducibility of results is paramount. By containerizing their code, researchers can avoid the ‚Äúit works on my machine‚Äù problem, ensuring that their analyses can be replicated by others, regardless of the underlying system configurations. You might be familiar with the use of tools like poetry or pyenv for managing Python environments to separate dependencies for different projects. Containers take this concept a step further by encapsulating the entire environment, including the operating system, runtime, libraries, and configurations. This ensures that the code runs identically on any machine, making it easier for researchers to share their work and for others to verify their findings.\nThis encapsulation also provides an added level of safety and security that is essential in research settings. By isolating the code and its dependencies from the host system, containers protect the environment from potential security vulnerabilities or malware disguised as python librairies. This isolation ensures that the code runs in a controlled environment, reducing the risk of unintended interactions with the host system.\nFinally, containers facilitate collaboration among researchers. When a project is containerized, collaborators can easily set up their environment by simply running the container, eliminating the often cumbersome process of manually installing and configuring dependencies. This ease of setup promotes a more efficient workflow and reduces the likelihood of errors, making collaborative research more streamlined and productive. Even if you work solo, containers improve the resiliency of your research workflows, making it easy to recover from a broken or stolen computer by reinstalling your project on a new computer without worrying about compatibility issues. You can even run your code in the cloud with services like GitHub Codespaces, ensuring that your analyses are not tied to a specific machine or operating system.\nIn this post, I provide a step-by-step guide on setting up devcontainers in VS Code, with a focus on supporting Python Poetry and mounting local directories for file storage.\nAll the code and configurations used in this tutorial are available in the GitHub repository."
  },
  {
    "objectID": "posts/devcontainers/index.html#introduction",
    "href": "posts/devcontainers/index.html#introduction",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "",
    "text": "In empirical finance research, the tools and methodologies we employ play a crucial role in ensuring the integrity and reproducibility of our findings. One such powerful tool is containerization, which allows you to encapsulate your code and its dependencies into a standardized unit. Development containers, or devcontainers, provide a convenient way to create isolated environments for your data analysis tasks, ensuring that your code runs consistently across different systems.\nContainers are essentially lightweight, portable environments that package up code and all its dependencies, ensuring that the software runs consistently across different computing environments. This consistency is particularly valuable in research settings where the reproducibility of results is paramount. By containerizing their code, researchers can avoid the ‚Äúit works on my machine‚Äù problem, ensuring that their analyses can be replicated by others, regardless of the underlying system configurations. You might be familiar with the use of tools like poetry or pyenv for managing Python environments to separate dependencies for different projects. Containers take this concept a step further by encapsulating the entire environment, including the operating system, runtime, libraries, and configurations. This ensures that the code runs identically on any machine, making it easier for researchers to share their work and for others to verify their findings.\nThis encapsulation also provides an added level of safety and security that is essential in research settings. By isolating the code and its dependencies from the host system, containers protect the environment from potential security vulnerabilities or malware disguised as python librairies. This isolation ensures that the code runs in a controlled environment, reducing the risk of unintended interactions with the host system.\nFinally, containers facilitate collaboration among researchers. When a project is containerized, collaborators can easily set up their environment by simply running the container, eliminating the often cumbersome process of manually installing and configuring dependencies. This ease of setup promotes a more efficient workflow and reduces the likelihood of errors, making collaborative research more streamlined and productive. Even if you work solo, containers improve the resiliency of your research workflows, making it easy to recover from a broken or stolen computer by reinstalling your project on a new computer without worrying about compatibility issues. You can even run your code in the cloud with services like GitHub Codespaces, ensuring that your analyses are not tied to a specific machine or operating system.\nIn this post, I provide a step-by-step guide on setting up devcontainers in VS Code, with a focus on supporting Python Poetry and mounting local directories for file storage.\nAll the code and configurations used in this tutorial are available in the GitHub repository."
  },
  {
    "objectID": "posts/devcontainers/index.html#video-tutorial",
    "href": "posts/devcontainers/index.html#video-tutorial",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "Video Tutorial",
    "text": "Video Tutorial\nThis post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/devcontainers/index.html#setting-up-devcontainers-in-vs-code",
    "href": "posts/devcontainers/index.html#setting-up-devcontainers-in-vs-code",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "Setting Up Devcontainers in VS Code",
    "text": "Setting Up Devcontainers in VS Code\nDevelopment containers are a feature in VS Code that leverages Docker1 to create and manage containers specifically for development purposes. This allows developers and researchers to work within a consistent environment, which is crucial for complex data analysis tasks. Setting up devcontainers in VS Code is straightforward, but there are a few prerequisites you need to have in place: Docker, Visual Studio Code, and the Dev Containers extension. If you‚Äôre on macOS, you can install Docker using Homebrew:\nbrew install --cask docker\nTo begin, you‚Äôll need to create a .devcontainer folder in your project directory. Inside this folder, you should create a devcontainer.json file, which will define the configuration for your development container. This file specifies the base image for the container, any additional tools or libraries that need to be installed, and other settings related to the development environment. By configuring this file, you can tailor the container to meet the specific needs of your data analysis tasks. This can be done using the Dev Containers extension in VS Code, which provides a user-friendly interface for creating and managing devcontainers. To get started, simply invoke Dev Containers: Add Development Container Configuration Files... from the command palette in VS Code and follow the prompts to create your devcontainer.json file.\nThis will prompt you to select a base image for your container, configure any additional tools or libraries, and set up other environment settings. As default options, I like to use the following:\n\nSetup location: in workspace\nBase image: Latest Python 3.x image from Microsoft\nFeatures: Poetry with pipx (more on this later)\n\nOnce you have configured your devcontainer.json file, you should get a prompt to reopen the project in the container. This will build the container based on the specified configuration and open your project within the containerized environment. You can verify that the container is running by checking the status bar in VS Code, which should indicate that you are working in a containerized environment.\nYou can always rebuild the container by invoking the Dev Containers: Rebuild and Reopen in Container command from the command palette. This will recreate the container based on the latest configuration settings, ensuring that your development environment is up-to-date and consistent with your project requirements.\nHere is what a simple devcontainer.json file (minus the comments) looks like:\n{\n  \"name\": \"Python 3\",\n  \"image\": \"mcr.microsoft.com/devcontainers/python:1-3.12-bullseye\",\n  \"features\": {\n    \"ghcr.io/devcontainers-contrib/features/poetry:2\": {}\n  }\n}"
  },
  {
    "objectID": "posts/devcontainers/index.html#images",
    "href": "posts/devcontainers/index.html#images",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "Images",
    "text": "Images\nImages are a crucial aspect of containerization, as they define the base environment for your development container. When setting up a devcontainer in VS Code, you can choose from a variety of pre-built images that provide different programming languages, tools, and libraries. These images serve as the foundation for your development environment, ensuring that the necessary dependencies are available for your data analysis tasks. By tying your devcontainer to a specific image, you can guarantee that your code runs consistently across different systems, making it easier to share and replicate your work.\nNote: Most images are based on Linux distributions, so you may need to adjust your code or configurations if you are used to working on Windows or macOS. However, the differences are usually minimal and can be easily managed within the container.\nNote 2: Most images specify the environment (i.e.¬†Linux version, Python version, etc.), but not the architecture, which makes them compatible with most systems. For example, most PCs and older Macs use Intel or AMD CPUs with the x86_64 architecture, while newer Macs with Apple Silicon have the arm64 architecture. The images are usually compatible with both architectures, but that means that the environment will not be 100% identical if you run the container on different architectures. This is usually not a problem for data analysis tasks, but it‚Äôs something to keep in mind if you are having issues with your code running differently on different systems."
  },
  {
    "objectID": "posts/devcontainers/index.html#features",
    "href": "posts/devcontainers/index.html#features",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "Features",
    "text": "Features\nFeatures are additional tools or libraries that can be installed in your development container to enhance its functionality. These features can include language-specific tools, package managers, or development environments that are tailored to your project requirements. By specifying features in your devcontainer.json file, you can extend the capabilities of your container and ensure that it is well-suited for your data analysis tasks. You can find a list of available features at containers.dev/features. In the example above, we are using the poetry feature to install the Python dependency manager Poetry in our development container."
  },
  {
    "objectID": "posts/devcontainers/index.html#poetry",
    "href": "posts/devcontainers/index.html#poetry",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "Poetry",
    "text": "Poetry\nPoetry is a powerful tool for managing Python dependencies and project configurations. It simplifies the process of creating, managing, and sharing Python projects by providing a unified interface for dependency management, packaging, and publishing. Poetry uses a pyproject.toml file to define project dependencies, scripts, and configurations, making it easy to manage project settings and requirements. By integrating Poetry into your devcontainer setup, you can streamline your development workflow and ensure that your Python projects are well-organized and reproducible.\nFor example here is a pyproject.toml file that defines the dependencies for a Python project:\n[tool.poetry]\nname = \"vcf-sample\"\nversion = \"0.1.0\"\ndescription = \"Sample code for Vincent Codes Finance\"\nauthors = [\"Vincent Codes Finance &lt;vincent@codes.finance&gt;\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\npackage-mode = false\n\n[tool.poetry.dependencies]\npython = \"^3.12\"\njupyter = \"^1.0.0\"\npandas = \"^2.2.2\"\n\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\nThe package-mode = false line tells Poetry to only install the dependencies, that the project is not a package itself.\nTypically, we would run poetry install to install the dependencies in the pyproject.toml file and then activate that environment with poetry shell (this is what I do in the video tutorial). However, since we only have one project in a devcontainer, we can simplify this by installing the dependencies directly in the base environment by setting the following poetry config:\npoetry config virtualenvs.create false\nWe can automate this process by adding a postCreateCommand to our devcontainer.json file to ensure that all dependencies are installed when the container is created:\n  \"postCreateCommand\": \"poetry config virtualenvs.create false; poetry install\""
  },
  {
    "objectID": "posts/devcontainers/index.html#mounting-local-directories",
    "href": "posts/devcontainers/index.html#mounting-local-directories",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "Mounting Local Directories",
    "text": "Mounting Local Directories\nBy default, the development container is isolated from the host system, except for the workspace directory, which is mounted into the container. This ensures that your project files are accessible within the container, allowing you to work on your code seamlessly. However, there are cases where you may need to access files or directories outside the workspace, such as large data files. To achieve this, you can mount local directories into the development container, making them available within the container environment.\nTo mount a local directory, you can add the mounts property to your devcontainer.json file, specifying the source path on the host and the target path in the container.\nHere is an example configuration for mounting a local directory:\n  \"mounts\": [\"source=/path/to/local/directory,target=/workspace/data,type=bind,consistency=cached\"]\nThis setup ensures that the directory /path/to/local/directory on your host machine is accessible within the container at /workspace/data. This approach provides the flexibility to work with local files while benefiting from the isolated environment of the container, except for the mounted directories.\nNote: This mounting process limits the flexibility of the container, as the source directory must be available on the host system. If you are working on a shared project or need to access files from different locations, you may need to consider alternative approaches, such as using a networked file system or cloud storage. As far as I know, there is no way to specify the source directory using an environment variable, so each collaborator would need to update the devcontainer.json file with their local path."
  },
  {
    "objectID": "posts/devcontainers/index.html#vs-code-extensions",
    "href": "posts/devcontainers/index.html#vs-code-extensions",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "VS Code Extensions",
    "text": "VS Code Extensions\nYou can also use the devcontainer.json file to configure default VS Code seettings and install VS Code extensions in your development container. When working with devcontainers, you will notice that not all the extensions you have installed on your host system are available in the container environment. Specifically, extensions that are mostly used for the host system (think UI), such as themes or language packs, will still be available. However, extensions that require access to the container environment, such as language servers or debuggers, will need to be installed. For example, the python image will install the Python extension, but you may need to install additional extensions for specific tasks, such as the Jupyter extension for working with Jupyter notebooks or with the interactive window. It can also be useful to make sure that all collaborators use the same formatting tools, such as Ruff.\nTo address this, you can specify the extensions and settings you want to install in the devcontainer.json file. For example, this will install Jupyter and Ruff, and configure Ruff as the default formatter:\n    \"customizations\": {\n        \"vscode\": {\n            \"extensions\": [\n                \"ms-toolsai.jupyter\",\n                \"charliermarsh.ruff\"\n            ],\n            \"vscode\": {\n                \"settings\": {\n                    \"[python]\": {\n                        \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n                        \"editor.codeActionsOnSave\": {\n                            \"source.fixAll\": \"explicit\",\n                            \"source.organizeImports\": \"explicit\"\n                        }\n                    },\n                    \"python.analysis.fixAll\": [\n                        \"source.unusedImports\"\n                    ],\n                    \"editor.formatOnSave\": true\n                }\n            }\n        }\n    }\nYou can find the extension IDs by looking at the extension in VS Code, then clicking on the gear icon and selecting ‚ÄúCopy Extension ID‚Äù."
  },
  {
    "objectID": "posts/devcontainers/index.html#limitations",
    "href": "posts/devcontainers/index.html#limitations",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "Limitations",
    "text": "Limitations\nDevcontainers are a powerful tool for creating isolated development environments, but they do have some limitations. One of the main drawbacks is the overhead associated with running containers, which can slow down the development process, especially for large projects or resource-intensive tasks. It‚Äôs not an issue that I have found to be significant, but it‚Äôs something to keep in mind if you are working on a particularly demanding project or have limited system resources.\nAdditionally, the isolation provided by containers can make it challenging to use some resources from the host system, such as GPUs or hardware peripherals. While it is possible to pass through devices to the container using Docker, this process can be complex and may not be suitable for all use cases. For example, on Apple Silicon Macs, even if the Linux container could access the GPU, there are no Linux drivers available for the GPU, so it would not be able to use it.\n\nAdditional Resources\n\nDevelopment Containers\nVS Code Documentation"
  },
  {
    "objectID": "posts/devcontainers/index.html#footnotes",
    "href": "posts/devcontainers/index.html#footnotes",
    "title": "Use Devcontainers for safe and replicable research",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther alternatives such as Podman and Colima can be used, but the official documentation is centered around Docker. See the VS Code documentation for supported alternatives.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/install-python-312/index.html",
    "href": "posts/install-python-312/index.html",
    "title": "Installing Python 3.12",
    "section": "",
    "text": "In this short tutorial, I cover installing Python 3.12 and the related tools for a complete coding environment."
  },
  {
    "objectID": "posts/install-python-312/index.html#what-you-need-for-a-complete-python-environment",
    "href": "posts/install-python-312/index.html#what-you-need-for-a-complete-python-environment",
    "title": "Installing Python 3.12",
    "section": "What you need for a complete Python environment",
    "text": "What you need for a complete Python environment\nThe most common way to use Python is to install it locally on your computer. The instructions below will guide you through the process of installing the following tools:\n\nPython: The Python interpreter, which allows you to run Python code.\nPoetry: A package manager for Python. I use it to manage the external libraries used in projects. Poetry makes it easy to install and update libraries on a per-project basis, and to make sure all collaborators use the same library version.\nVisual Studio Code: Visual Studio Code is a free source code editor made by Microsoft. Features include support for debugging, syntax highlighting and intelligent code completion. Users can install extensions that add additional functionality.\n\nWe will also install the following tools that are not required to run Python code, but are useful when working on projects with code:\n\nGit and GitHub: I use Git to manage my code and GitHub to host my code online and collaborate with others. Git is a version control system that tracks code changes and keeps a full history of changes. GitHub is a website that hosts Git repositories and provides additional features for collaboration such as issue tracking and pull requests.\n\n\n\n\n\n\n\nNotePoetry vs Anaconda\n\n\n\n\n\nMost Python projects use external libraries. For example, we use the pandas library for data analysis. To manage these libraries, we need a package manager. I recommend using Poetry. Anaconda was my package manager of choice for many years and it remains very popular, but like many I recently switched to Poetry. Here is my view of the pros and cons of each:\n\nPoetry\n\nPros\n\nFlexibility with Python version It is very easy to specify which version of Python an environment should use and to change it later.\nDependency management Poetry makes it easy to install and update libraries on a per-project basis, and to make sure all collaborators use the same library version.\nPerformance Poetry is much faster than Anaconda.\nSimple to update Poetry is easy to update, and it is easy to update the libraries in a project.\n\n\n\nCons\n\nInitial setup The initial setup is a bit more complex than Anaconda, but if you follow the instructions below, it should be easy.\nLearning curve Poetry is a new tool, so there is a learning curve. However, it is not that difficult to learn.\nNot as widely used (newer) Poetry is a new tool, so it is not as widely used as Anaconda. However, it is gaining popularity very quickly.\n\n\n\n\nAnaconda\n\nPros\n\nEase of use Anaconda is very easy to install and use.\nWide adoption in science and data analysis Anaconda is very popular in the scientific community, so it is easy to find help online.\n\n\n\nCons\n\nBloat and performance Anaconda is very bloated and slow. It is much slower than Poetry.\nUpdates Updating Anaconda is a pain. It is also difficult to update the libraries in a project when a conflict arises.\nComplexity when there are issues Anaconda is a complex tool, so when there are issues, it can be difficult to troubleshoot.\n\nAnother important difference is that Anaconda is a complete environment. Anaconda comes with many libraries pre-installed, so you can start working on projects right away. This might seem like a good thing, but I find that it is better to start with a clean environment and install only the libraries you need for each project. This way, you know exactly which libraries are used in each project, which is part of the best practices for reproducible research."
  },
  {
    "objectID": "posts/install-python-312/index.html#video-tutorial",
    "href": "posts/install-python-312/index.html#video-tutorial",
    "title": "Installing Python 3.12",
    "section": "Video tutorial",
    "text": "Video tutorial\nThe video below shows how to install Python 3.12 and the related tools on macOS using Homebrew, which is my preferred method for macOS. The instructions for a manual installation on macOS or for other platforms are provided below the video."
  },
  {
    "objectID": "posts/install-python-312/index.html#installation",
    "href": "posts/install-python-312/index.html#installation",
    "title": "Installing Python 3.12",
    "section": "Installation",
    "text": "Installation\n\n macOS (preferred) macOS (manual) Linux Windows\n\n\nIf you are using macOS, I recommend using Homebrew to install Python and the other tools. Homebrew is a package manager for macOS that allows you to install and update software from the command line. It simplifies the installation process and makes it easy to keep your software up-to-date. If you prefer not to use Homebrew, you can install Python and the other tools manually using the official installers.\nFirst, you need to open the Terminal app. You can find it in the Applications/Utilities folder, or by using Spotlight (press ‚åò + Space and type Terminal).\n\nXcode command-line tools\nHomebrew requires the Xcode command-line tools from Apple. If so, you can install them from the terminal:\nxcode-select --install\n\n\nHomebrew\nTo install Homebrew (brew for short), run the following command in the Terminal app:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nOnce brew is installed, look at the output of the previous command. It should tell you if you need to add Homebrew to your path. If so, copy the command it suggests and run it in the Terminal app.\n\n\nPython\n\n\n\n\n\n\nCautionIf you have Anaconda installed\n\n\n\nIf you have Anaconda installed, you should uninstall it before installing Python with brew. Follow the instructions here to uninstall Anaconda.\n\n\n\n:::\nTo install Python with brew, run the following command in the Terminal app:\n\n```bash\nbrew install python@3.12 python@3.11 python@3.10\nThis will install the latest version of Python (3.12), as well as the previous two versions. This is useful because some libraries are not compatible with the latest version of Python, so you might need to use an older version for some projects.\nTo make sure that the latest version of Python is used by default, run the following command in the Terminal app:\necho 'export PATH=\"$HOMEBREW_PREFIX/opt/python@3.12/libexec/bin:$PATH\"' &gt;&gt; ~/.zshrc\nYou should now have Python 3.12 installed. To check, run the following command in the Terminal app:\npython3 --version\nIt should also work with the python command:\npython --version\nIf the two versions are different, it means that python still links to the system Python. To fix this, run the following command in the Terminal app:\necho 'alias python=\"python3\"' &gt;&gt; ~/.zshrc\n\n\n\n\n\n\nImportantHomebrew and the base environment\n\n\n\n\n\nBy default, Homebrew will lock the base Python environment, which means that you cannot install additional libraries in the base environment. This means that you cannot easily install pandas or other libraries in the base environment.\nI consider this a good thing because it prevents you from accidentally breaking your Python installation and it forces you to use virtual environments, which is a good practice. However, if you want to change this behavior, there are a few workarounds.\nIsn‚Äôt it more convenient to have a base environment with all the libraries you need? Maybe, but eventually, it can lead to conflicts, difficult updates, and other annoying issues. My approach is to keep a sandbox environment with all the libraries I use regularly and to create a new environment for each project. This way, I can easily update the libraries in the sandbox environment, and I can delete the project environments when I am done with the project. This prevents accumulating virtual environments for discarded projects. If there is a conflict between two libraries in my sandbox environment, I can easily create a new one without reinstalling everything.\n\n\n\n\n\nPoetry\nTo install Poetry with brew, run the following command in the Terminal app:\nbrew install poetry\nBy default, Poetry installs Python for each project in the ~/Library/Caches/pypoetry/virtualenvs/ directory. I prefer to have it in the project directory. That way if, I delete the directory, then the environment is deleted as well, which prevents accumulating virtual environments for discarded projects. To enable this, run the following command:\npoetry config virtualenvs.in-project true\n\n\nVisual Studio Code\nTo install Visual Studio Code with brew, run the following command in the Terminal app:\nbrew install --cask visual-studio-code\n\n\n\n\n\n\nNote--cask option\n\n\n\nYou might have noticed that we used the --cask option to install Visual Studio Code.\nBrew offers two types of packages: formulae and casks. Formulae are used to install command-line tools, while casks are used to install graphical applications that go in the Applications folder.\nMost Mac applications can be installed using brew, just search for them on the homepage to find the name of the cask. For example, to install Google Chrome, you would run brew install --cask google-chrome.\n\n\n\n\nGit and GitHub\nGit is already installed on your Mac as a command-line tool because you have installed the Xcode tools. Still, I recommend installing the latest version using brew:\nbrew install git\nTo follow along with the upcoming tutorials, or to collaborate efficiently with others, you will also need to use GitHub. For this, you will want to use the GitHub CLI (gh for short) or the GitHub Desktop app. Both can be installed using brew:\nbrew install gh\nbrew install --cask github\n\n\n\n\n\n\n\n\n\nCautionYou should consider Homebrew\n\n\n\nI know that using command-line instructions to install software can be intimidating at first, but it is not that difficult. In the long run, it is much easier to use Homebrew than to install everything manually. If you are not convinced, you can follow the manual installation instructions below, but I recommend that you come back later and try Homebrew.\n\n\n\nPython\nmacOS comes with Python pre-installed. However, it might not be the latest version of Python, so I recommend installing a fresh version from the official distribution at python.org.\nTo complete the installation, go to Applications/Python 3.12 and double-click Install Certificates.command.\nSee this thread on StackOverflow for more details.\n\n\nPoetry\nInstallation instructions can be found here.\nNote: If you get an SSH-related issue, go back up to the Python installation instruction and run Install Certificates.command.\nBy default, Poetry installs Python for each project in the ~/Library/Caches/pypoetry/virtualenvs/ directory. I prefer to have it in the project directory, that way if, I delete the directory, then the environment is deleted as well, which prevents accumulating virtual environments for discarded projects. To enable this, run the following command in the Terminal app. You can find it in the Applications/Utilities folder, or by using Spotlight (press ‚åò + Space and type Terminal).\npoetry config virtualenvs.in-project true\nIf you get an error message that poetry is not found, you need to add it to your path. To do so, open the Terminal app and run the following command:\nexport PATH=\"/Users/{user}/.local/bin:$PATH\"\nwhere {user} is your username. You can find it by running the following command in the Terminal app:\nwhoami\n\n\nVisual Studio Code\nDownload Visual Studio Code from code.visualstudio.com.\n\n\nGit and GitHub\nGit might already be installed on your Mac as a command-line tool if you have installed the Xcode tools. If not, you can get the official installer. You can also use Git directly in VS Code, or using a GUI client such as GitHub Desktop. I prefer to use the VS Code integration or the command-line tool, but many beginners prefer to use GitHub Desktop.\n\n\n\nTo be honest, if you‚Äôre using Linux, you probably already know how to install Python and other tools. The instructions below are for manual installation, but you probably want to use your distribution‚Äôs package manager instead.\n\nPython\nMost Linux distributions come with Python pre-installed. However, it might not be the latest version of Python, so I recommend installing a fresh version from the official distribution at python.org.\n\n\nPoetry\nInstallation instructions can be found here.\nBy default, Poetry installs Python for each project in the ~/.cache/pypoetry/virtualenvs/ directory. I prefer to have it in the project directory, that way if, I delete the directory, then the environment is deleted as well, which prevents accumulating virtual environments for discarded projects. To enable this, run the following command in the terminal:\npoetry config virtualenvs.in-project true\n\n\nVisual Studio Code\nDownload Visual Studio Code from code.visualstudio.com.\n\n\nGit and GitHub\nGit is probably already installed on Linux as a command-line tool. You can also use Git directly in VS Code, or using a GUI client such as GitHub Desktop. I prefer to use the VS Code integration or the command-line tool, but many beginners prefer to use GitHub Desktop.\n\n\n\n\nPython\nWindows does not have a built-in Python interpreter, so you will need to install it. I recommend installing a fresh version from the Microsoft Store. You can also install it from the official distribution at python.org, but the Microsoft Store version is easier to install and is more consistent in setting up the environment variables correctly.\n\n\nPoetry\nInstallation instructions can be found here.\nNote: Unless you are using the Windows Subsystem for Linux (WSL), you should follow the instructions for Windows PowerShell.\nBy default, Poetry installs Python for each project in a global directory. I prefer to have it in the project directory, that way if, I delete the directory, then the environment is deleted as well, which prevents accumulating virtual environments for discarded projects. To enable this, run the following command in Powershell:\npoetry config virtualenvs.in-project true\n\n\nVisual Studio Code\nDownload Visual Studio Code from code.visualstudio.com.\n\n\nGit and GitHub\nTo use Git on Windows, you need to install the Git client, which is a command-line tool.\nYou can also use Git directly in VS Code, or using a GUI client such as GitHub Desktop, but you need to first install the Git client. I prefer to use the VS Code integration or the command-line tool, but many beginners prefer to use GitHub Desktop."
  },
  {
    "objectID": "posts/install-python-312/index.html#github.com-optional",
    "href": "posts/install-python-312/index.html#github.com-optional",
    "title": "Installing Python 3.12",
    "section": "GitHub.com (optional)",
    "text": "GitHub.com (optional)\nYou do not need a GitHub account to have a complete Python environment. However, I recommend creating one because it will be useful later when we start working on projects.\nTo follow the upcoming tutorials, you will need to create a GitHub account. You can create a free account at https://github.com/.\nGitHub offers many benefits to students and educators, including free access to GitHub Copilot and extra free hours for GitHub Codespaces. I highly recommend applying at GitHub Education if you are eligible."
  },
  {
    "objectID": "posts/install-python-312/index.html#creating-a-sandbox-environment",
    "href": "posts/install-python-312/index.html#creating-a-sandbox-environment",
    "title": "Installing Python 3.12",
    "section": "Creating a sandbox environment",
    "text": "Creating a sandbox environment\nI like to have a sandbox environment with all the libraries I use regularly that are not tied to a project. That way, if I want to try something quickly like reading a CSV file to look at it, I can do it without creating a new project. It is common to use the default (or base) environment for this, but I prefer to create a separate environment. This way, I can easily update the libraries in the sandbox environment. If there is a conflict between two libraries in my sandbox environment, I can easily create a new one without reinstalling everything.\nFor my sandbox environment, I will want at least the following libraries:\n\npandas: Data analysis library\nnumpy: Numerical computing library\nscipy: Scientific computing library\nmatplotlib: Plotting library\nseaborn: Plotting library\nstatsmodels: Statistical models\nscikit-learn: Machine learning library\nlinearmodels: Linear models for Python\npyarrow: Library for working with parquet files\njupyter: for Jupyter notebooks and the VS Code Python interactive window\npytest: Testing framework\n\nTo create this sandbox environment, I will use Poetry. First, I need to create a new directory for the environment. I will call it sandbox. Then, I need to create a new project in this directory:\nmkdir ~/Documents/sandbox\ncd ~/Documents/sandbox\npoetry init\nFollow the instructions to create the project. You can leave the default values for most questions, but do not add any dependencies (it‚Äôs simpler to add them after).\nThis creates a pyproject.toml file in the sandbox directory. This file contains the list of dependencies for the project (which will be empty for now).\nOnce the project is created, you can add the dependencies:\npoetry add pandas numpy scipy matplotlib seaborn statsmodels scikit-learn linearmodels pyarrow jupyter pytest\nThis step updates the pyproject.toml file and creates a poetry.lock file, which contains the exact version of each dependency. This file is used to make sure that all collaborators use the same version of each library. Note that because our dependencies are built on top of other libraries, Poetry will also install the dependencies of our dependencies.\nTo activate the environment in the terminal, run the following command:\npoetry shell\n\n\n\n\n\n\nNote\n\n\n\nIf at some point you get the following error message, you can safely ignore it:\nThe current project could not be installed: No file/folder found for package sandbox\nIt is because Poetry is trying to install the sandbox package, which does not exist. This does not affect the creation of the environment and the installation of the dependencies. I will cover the intricacies of Poetry in a future post."
  },
  {
    "objectID": "posts/install-python-312/index.html#configuring-visual-studio-code",
    "href": "posts/install-python-312/index.html#configuring-visual-studio-code",
    "title": "Installing Python 3.12",
    "section": "Configuring Visual Studio Code",
    "text": "Configuring Visual Studio Code\nVisual Studio Code is a free source code editor made by Microsoft. Features include support for debugging, syntax highlighting and intelligent code completion. While there are some built-in features for Python, most of the functionality comes from extensions. What I recommend is to use the profile feature of VS Code, which lets you define a set of extensions for each use case. For example, you can have a profile for Python development, another for R development, and another for LaTeX editing. This way, you can have a clean installation of VS Code and only install the extensions you need for each profile. Furthermore, each profile can have its specific settings and theming options.\nTo create a profile, click on the profile icon in the bottom left corner of the VS Code window. Then, under the Profiles section, click on Create Profile.\n\n\n\n\n\nGive the profile a name and select a distinctive icon. Make sure to copy from the Data Science template, which will install all the extensions you need for data analysis with Python.\n\n\n\n\n\nVS Code works best when you have a project (directory) open. To open a project, select Open Folder from the File menu and select the folder you want to open, for example, the sandbox folder we created earlier.\nTo open an interactive window, bring up the command palette by pressing ‚åò + Shift + P (or Ctrl + Shift + P on Windows and Linux) and type Python: Create Interactive Window.\nAt this point, VS Code should have detected the virtual environment created by Poetry and should have asked you if you want to use it. If not, you can select it manually by clicking on the Python version in the top right corner of the interactive window."
  },
  {
    "objectID": "posts/install-python-312/index.html#github-codespaces",
    "href": "posts/install-python-312/index.html#github-codespaces",
    "title": "Installing Python 3.12",
    "section": "Python in the cloud using Github Codespaces",
    "text": "Python in the cloud using Github Codespaces\nMany online platforms allow you to develop and run Python code without installing anything on your computer. If you want to use a cloud-based solution, I recommend using GitHub Codespaces.\nAll you need is a GitHub account. However, note that GitHub Codespaces is not free. At the time of this writing, you get 60 hours per month for free, or 90 hours if you signed up for the GitHub Student Developer Pack (this is for a 2-core machine, which is the smallest machine available). After that, you have to pay for it (the current rate is USD 0.18 per hour).\nMake sure to shut down your Codespace when you are not using it, otherwise you will run out of free hours very quickly.\n\nOther cloud alternatives\nThere are many other cloud-based alternatives. However, most are based on Jupyter notebooks, which can be interesting when you are learning Python, but are not ideal for robust, replicable research (watch out in the future for a video rant on why I don‚Äôt like Juptyer notebooks). Some of the most popular alternatives are:\n\nGoogle Colab\nCocalc\nWRDS Jupyter Hub (requires a WRDS subscription through your institution)"
  },
  {
    "objectID": "posts/install-python-312/index.html#whats-next",
    "href": "posts/install-python-312/index.html#whats-next",
    "title": "Installing Python 3.12",
    "section": "What‚Äôs next?",
    "text": "What‚Äôs next?\nNow that you have a complete Python environment, you can start learning Python. Watch out for my upcoming tutorials on Python for finance research in the coming days."
  },
  {
    "objectID": "posts/claude-code-data-analysis/index.html",
    "href": "posts/claude-code-data-analysis/index.html",
    "title": "Claude Code for Data Analysis",
    "section": "",
    "text": "Claude Code is a powerful new way to work with coding agents. In this post, I‚Äôll walk through how to get started with Claude Code for data analysis, the workflow I recommend, and some of the more advanced features you might want to explore once you‚Äôre comfortable."
  },
  {
    "objectID": "posts/claude-code-data-analysis/index.html#what-is-claude-code",
    "href": "posts/claude-code-data-analysis/index.html#what-is-claude-code",
    "title": "Claude Code for Data Analysis",
    "section": "What is Claude Code?",
    "text": "What is Claude Code?\nClaude Code is a command-line interface (CLI) tool designed for agentic coding. Instead of just giving you autocomplete suggestions like early versions of GitHub Copilot, Claude Code allows you to interact with an AI model in a structured, project-aware way.\nWhile tools like GitHub Copilot and Cursor also offer ‚Äúagentic modes,‚Äù in practice Claude Code has consistently given me the best results and smoothest experience when using coding agents. It‚Äôs designed not just to generate code, but to collaborate with you across an entire workflow: planning, executing, and reviewing tasks with awareness of your project context.\nAnother practical benefit is pricing. Claude Code isn‚Äôt free, but you don‚Äôt need a special expensive developer plan to try it out. If you already have a Claude Pro subscription (the entry-level paid plan for Claude), you can start using Claude Code right away for casual coding or data analysis‚Äîno extra cost.\nNote that Claude Code is not the only agentic coding tool available. Other good options include OpenAI‚Äôs Codex CLI, Google‚Äôs Gemini CLI, Cursor CLI, and open-source alternatives such as Open Code. Each has its strengths and weaknesses, but Claude Code is my personal favorite for data analysis (for now!)"
  },
  {
    "objectID": "posts/claude-code-data-analysis/index.html#video-tutorial",
    "href": "posts/claude-code-data-analysis/index.html#video-tutorial",
    "title": "Claude Code for Data Analysis",
    "section": "Video tutorial",
    "text": "Video tutorial\nThis post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/claude-code-data-analysis/index.html#getting-started-with-claude-code",
    "href": "posts/claude-code-data-analysis/index.html#getting-started-with-claude-code",
    "title": "Claude Code for Data Analysis",
    "section": "Getting Started with Claude Code",
    "text": "Getting Started with Claude Code\nClaude Code runs as a command-line interface (CLI), so setup is done from your terminal. Once installed, you can use it directly in your projects or alongside your existing workflow tools like Visual Studio Code and GitHub.\n\nInstalling Claude Code\nBefore installing Claude Code, make sure you have Node.js installed on your computer:\n\nYou can download it directly from the official Node website [Node.js link], or\nIf you‚Äôre on macOS and use Homebrew, run:\nbrew install node\n\nOnce Node is installed, you can install Claude Code globally with:\nnpm install -g @anthropic-ai/claude-code\nAfter installation, you‚Äôll have access to the claude command in your terminal.\nTwo optional but very convenient tools to add at this point:\n\n*Visual Studio Code and JetBrains Extensions** ‚Äì lets you see Claude‚Äôs updates and outputs directly in your IDE. [Claude Code VS Code extension link]\nGitHub CLI (gh) ‚Äì this allows Claude Code to interact with GitHub repositories. On macOS with Homebrew, install it with:\nbrew install gh\n\n\n\nInitializing a Project\nIn my typical workflow, I like to set up the project repository first, before asking Claude to do anything. This way, Claude Code starts with a clean, structured environment that reflects how I want the project organized.\n1. Create the repository and environment If I‚Äôm working in Python, I usually initialize the project with uv a fast package/dependency manager:\nuv init my-project\ncd my-project\nThen I‚Äôll add the core dependencies I know I‚Äôll need (e.g.¬†pandas, numpy, matplotlib). Claude can install more later if required, but giving it a solid baseline helps it recognize what‚Äôs already available. Note that Claude will usually try to install packages by editing pyproject.toml directly. I prefer to instruct it to use uv add &lt;package&gt; commands instead, so the latest versions are always installed.\n2. Define a project structure\nAt this stage, I‚Äôll set up directories for different types of files. For example:\nsrc/         # source code  \ndata/        # raw datasets  \nfigures/     # plots and charts  \ntables/      # analysis outputs  \nClaude Code can ‚Äúsee‚Äù this structure and will naturally start saving outputs in the right places.\n3. Add a short project description I usually write a short markdown file describing what I‚Äôm doing‚Äîeither a README.md (if the project will be hosted on GitHub) or a simple notes.md. This provides Claude with immediate context about the project‚Äôs purpose.\n4. Initialize Claude in the project\nOnce the project is set up, it‚Äôs time to bring Claude Code into the picture. You can eitheir run claude in the terminal or use the VS Code extension.\nThe first time you launch, Claude will ask you to either sign in (if you‚Äôre on a Claude Pro or Max subscription) or provide an API key (for pay-as-you-go).\n\nWith Claude Pro, you‚Äôll automatically use the Sonnet model, which is more than sufficient for casual coding and data analysis.\nOn Claude Max, you can also select the model you want with the /model command.\n\nOnce logged in, run the /init command inside Claude. This generates a CLAUDE.md file, which contains a set of instructions for your project. Claude builds this file by looking at your project‚Äôs structure and any descriptions you‚Äôve provided.\n5. Review the CLAUDE.md file\nIt‚Äôs good practice to open the CLAUDE.md right after it‚Äôs created. This file is included in every prompt Claude sends while working on your project, so you want to be sure it reflects the right context and conventions, and that any special instructions are clear.\n\n\nMaking Your Data Accessible\nFor data analysis projects, one of the most important steps is making sure Claude Code knows where your data is and how to access it. Depending on your workflow, there are several approaches:\n1. Keep data directly in the project folder\nIf your dataset is small, the simplest option is to drop it into a dedicated folder inside your project (e.g.¬†data/). Claude Code will automatically see the files in this directory and can use them in analysis.\n2. Reference external data locations\nIf your data lives outside the project folder‚Äîsay, in a Dropbox directory you share with collaborators‚ÄîClaude won‚Äôt automatically know where to find it. You have two good options here:\n\nReference the path in CLAUDE.md ‚Äì add a note in your project description telling Claude where to look for the dataset.\nUse environment variables ‚Äì if collaborators have different setups, it‚Äôs cleaner to define the data path in a .env file (e.g.¬†DATA_PATH=/Users/yourname/Dropbox/finance-data). Then, in your CLAUDE.md, simply tell Claude to read the path from the .env file. In your code, you can use the python-dotenv package to load the variables from .env, and os.getenv(\"DATA_PATH\") to access them.\n\n3. Fetch data programmatically For datasets stored online (e.g.¬†academic repositories, APIs, or financial data feeds), you can also have Claude write the code to fetch the data directly as part of the workflow. This is especially useful when you want reproducibility.\nHave Claude describe the data\nBefore you start the analysis, a good idea is to have Claude read part of the data to infer its schema. This way, it will understand what‚Äôs in your dataset.\nHave Claude write this information to a Markdown file so you can review it and make any required corrections. You can then direct Claude to look at this file for reference. This ensures that Claude will always have a clear idea of what each of these columns means.\n\n\nExtending Claude with MCPs\nClaude Code is only as effective as the information it has access to. By default, it relies on its training data and the context you provide in your project. But as with any LLM, relying solely on training knowledge has two big limitations:\n\nKnowledge cutoff: the model will not know about updates or changes that happened after its training point.\nHallucination risk: without reliable references, it may generate outdated or incorrect code. Claude is pretty good at recovering from mistakes, but they waste time and tokens so it‚Äôs always better to avoid them in the first place.\n\nThis is where Model Context Protocols (MCPs) come in. MCPs are connectors that allow Claude to query external tools, APIs, or datasets, enriching its context with up-to-date, authoritative information.\nOne MCP I consider essential is context7. This tool gives Claude access to package documentation on demand. For example:\n\nIf Claude needs to use pandas, it can pull the latest pandas documentation instead of relying on memory from an older version. It can even query specific functions or classes to ensure correct usage.\nIf you are running a regression in statsmodels, Claude can check the official API docs to confirm argument names and recommended usage.\n\nIn practice, Context-7 makes a big difference because the agent can rely on current documentation instead of outdated training knowledge, which means you get to the right results more quickly.\n\nInstalling and Using context7\nYou can install context7 with the following command in the terminal.:\nclaude mcp add --transport http context7 https://mcp.context7.com/mcp\nIf you have an instance of Claude Code running already (i.e.¬†if the VS Code extension is opened), you may need to restart it for the MCP to be recognized. Once installed, you can make Claude use it by simply adding a note in your prompt, for example:\nPlease use context7 to get the latest documentation before writing the code."
  },
  {
    "objectID": "posts/claude-code-data-analysis/index.html#plan-review-execute-review",
    "href": "posts/claude-code-data-analysis/index.html#plan-review-execute-review",
    "title": "Claude Code for Data Analysis",
    "section": "Plan, Review, Execute, Review",
    "text": "Plan, Review, Execute, Review\nOne of the biggest advantages of Claude Code compared to autocomplete-style tools is that it can work with you in a structured, iterative way. To get the most out of it, it helps to think of your interaction with Claude as a collaborative workflow rather than a one-shot prompt.\nThe way I like to think of Claude Code is as a research assistant who works hard but needs guidance to stay on track. The best results come from a cycle of:\n\nStep 1: Plan\nStart by giving Claude a clear, specific task with as much detail as you can provide. Then, ask Claude to outline a plan of how it intends to complete the task. This gives you visibility into its approach before any code is written.\nFor a more complicated task, it is a good idea to have Claude write a to-do list of the plan as a markdown file. This way, it can keep track of its progress. If you have to stop the task in the middle, you will always know where you are and what‚Äôs left to do.\n\n\nStep 2: Review the Plan\nGo through Claude‚Äôs proposed steps and make adjustments if needed. This is very important because everything that follows will be based on this plan.\n\n\nStep 3: Execute Step by Step\nAsk Claude to execute the plan one step at a time. This way, you can monitor progress, catch mistakes early, and keep the workflow under control. If you see something that doesn‚Äôt look right, you can pause and correct it before it goes too far using the escape key.\n\n\nStep 4: Review and Adjust\nOnce the task is complete, review the results with Claude. If something looks off, provide feedback and ask it to refine the code or analysis.\nIn practice, this cycle‚Äîplan, review, execute, review‚Äîturns Claude into a true coding partner/research assistant.\nJust like any human collaborator, Claude benefits from clear instructions, regular check-ins, and constructive feedback. And just like any research assistant, it is your responsibility to ensure the final results are accurate and meaningful."
  },
  {
    "objectID": "posts/claude-code-data-analysis/index.html#analysis-figures-and-jupyter-notebooks",
    "href": "posts/claude-code-data-analysis/index.html#analysis-figures-and-jupyter-notebooks",
    "title": "Claude Code for Data Analysis",
    "section": "Analysis, Figures, and Jupyter Notebooks",
    "text": "Analysis, Figures, and Jupyter Notebooks\nOnce Claude Code begins working through your tasks, you can treat it like a research assistant who generates both results and visual outputs. The key is to ask explicitly for the kinds of outputs you want at each stage.\n\nSummary Statistics and Results\nIt is always helpful to request summary statistics and intermediate results before diving into more advanced analysis. I also like to ask Claude to comment on these results.\nThis has two benefits:\n\nClaude can flag cases where numbers seem inconsistent or unexpected, which can save time.\nSometimes Claude will even realize that results don‚Äôt make sense and adjust its own analysis automatically.\n\nThat said, I do not rely on Claude‚Äôs interpretation. As the domain expert, I am the one responsible for evaluating whether results are meaningful.\n\n\nFigures and Visualizations\nFigures are an essential part of understanding data, and Claude can generate them directly in your workflow. A useful practice is to:\n\nAsk Claude to save plots into a dedicated figures/ directory.\nProvide detailed instructions on formatting, labeling, and what you expect to see.\nLet Claude inspect its own output.\n\nSince Claude is multimodal, it can actually look at the figures it generates and check whether they match your requirements. If something is missing, for example a mislabeled axis or an incorrect chart type, Claude often corrects it automatically.\nThat said, you should still review the figures yourself. Most of the time Claude gets close to what you want, but your detailed feedback ensures the final results are presentation-ready.\n\n\nJupyter Notebooks\nClaude Code can also generate Jupyter Notebooks that capture the workflow in a transparent, reproducible format. This has two main advantages:\n\nYou can open the notebook and run the code directly, verifying outputs line by line.\nYou get a structured record of the analysis steps, which makes it easier to tweak or extend the work later.\n\nIt is best practice to ask Claude to save notebooks in a dedicated folder (e.g.¬†notebooks/). And if you have existing notebooks, you can ask Claude to edit or extend them as part of the workflow. It can read notebooks directly, including looking at figures and outputs."
  },
  {
    "objectID": "posts/claude-code-data-analysis/index.html#what-to-explore-next",
    "href": "posts/claude-code-data-analysis/index.html#what-to-explore-next",
    "title": "Claude Code for Data Analysis",
    "section": "What to Explore Next",
    "text": "What to Explore Next\nOnce you are comfortable using Claude Code for basic analysis, there are some more advanced features worth keeping in mind.\n\nManaging Context\nClaude relies heavily on context to perform well. This includes:\n\nYour project files and structure\nThe CLAUDE.md file you defined earlier\nAny MCPs you have enabled\nThe outputs it has generated so far\nThe running history of your conversation\n\nMore context generally means better results, but there are trade-offs:\n\nQuota usage: larger contexts consume more tokens, so you may hit your plan limits faster.\nNoise: as the session history grows, older details may crowd the context and reduce clarity for current tasks.\n\nClaude does will automatically ‚Äúcompact‚Äù the context, i.e.¬†condense it into a smaller summary, when it gets too large. But there is always information loss when this happens, and in my experience the performance degrades noticeably right after a compaction, especially if it happens in the middle of a complex task.\nThere are a few strategies to manage this:\n\nUse /clear to reset context when you are done with a line of work.\nUse /compact to manually trigger compaction at logical transition points.\n\n\n\nSub-agents\nAnother advanced feature is the ability to define sub-agents. These are specialized agents you configure for particular types of work, such as generating figures, running statistical analyses, or cleaning datasets.\nEach sub-agent comes with its own set of instructions and only receives the context it needs for its specific task. The main agent can then call the sub-agent when appropriate. This has three main benefits:\n\nTasks run faster because the context is smaller.\nYou save on quota since fewer tokens are consumed running the sub-agent.\nThe context of the main agents stays smaller for longer as it only includes the final outputs from sub-agents, not all the steps they took.\n\nSub-agents require a bit more setup, but they are a powerful way to keep Claude focused and efficient in larger projects."
  },
  {
    "objectID": "posts/claude-code-data-analysis/index.html#wrapping-up",
    "href": "posts/claude-code-data-analysis/index.html#wrapping-up",
    "title": "Claude Code for Data Analysis",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nClaude Code is more than just a coding assistant‚Äîit is a full workflow partner that can plan, execute, review, and document your analysis. With tools like MCPs, multimodal feedback, and context management, it can streamline even complex projects.\nIf you are just starting out, focus on the basics: setting up your project, making your data accessible, and working in an iterative plan-review-execute loop. As you grow more comfortable, you can explore context management and sub-agents to push your workflow further.\nI will also be publishing a companion YouTube walkthrough that shows these ideas in action with a real data analysis example, so stay tuned for that if you‚Äôd like to see Claude Code at work step by step."
  },
  {
    "objectID": "posts/documents-llm/index.html",
    "href": "posts/documents-llm/index.html",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "",
    "text": "Large language models (LLMs) have revolutionized the way we interact with text data, enabling us to generate, summarize, and query information with unprecedented accuracy and efficiency. In this tutorial, we‚Äôll explore how to leverage the power of LLMs to process and analyze PDF documents using Ollama, an open-source tool that manages and runs local LLMs. By combining Ollama with LangChain, we‚Äôll build an application that can summarize and query PDFs using AI, all from the comfort and privacy of your computer. Utilizing Ollama to serve the models, along with LangChain for its extensive library of convenience tools for accessing and interacting with large language models, we‚Äôll construct an app that operates entirely locally on your machine. We‚Äôll then use Streamlit to build an interactive dashboard, enhancing the usability of our application.\nAll the code for this tutorial is available on GitHub, so you can follow along and experiment with the application yourself. If you‚Äôre ready to enhance your research process with a powerful, AI-driven tool for summarizing and querying PDF documents, then you‚Äôve come to the right place. Let‚Äôs get started!"
  },
  {
    "objectID": "posts/documents-llm/index.html#video-tutorial",
    "href": "posts/documents-llm/index.html#video-tutorial",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "Video tutorial",
    "text": "Video tutorial\nThis post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/documents-llm/index.html#ollama",
    "href": "posts/documents-llm/index.html#ollama",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "Ollama",
    "text": "Ollama\nOllama is a tool to manage and run local LLMs, such as Meta‚Äôs Llama2 and Mistral‚Äôs Mixtral. I discussed how to use Ollama as a private, local ChatGPT replacement in a previous post.\nThe first step in setting up Ollama is to download and install the tool on your local machine. The installation process is straightforward and involves running a few commands in your terminal. Ollama‚Äôs download page provides installers for macOS and Windows, as well as instructions for Linux users. Once you‚Äôve downloaded the installer, follow the installation instructions to set up Ollama on your machine.\nIf you‚Äôre using a Mac, you can install Ollama using Homebrew by running the following command in your terminal:\nbrew install ollama\nThe benefit of using Homebrew is that it simplifies the installation process and also sets up Ollama as a service, allowing it to run in the background and manage the LLM models you download.\nAt the moment, the most popular code models on Ollama are:\nAfter installing Ollama, you can install a model from the command line using the pull command:\nollama pull mixtral"
  },
  {
    "objectID": "posts/documents-llm/index.html#dependencies",
    "href": "posts/documents-llm/index.html#dependencies",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "Dependencies",
    "text": "Dependencies\nAlongside Ollama, our project leverages several key Python libraries to enhance its functionality and ease of use:\n\nLangChain is our primary tool for interacting with large language models programmatically, offering a streamlined approach to processing and querying text data.\nPyPDF is instrumental in handling PDF files, enabling us to read and extract text from documents, which is the first step in our summarization and querying process.\nlangchain_openai and the openai modules are used to access the OpenAI API-compatible API of Ollama. The added benefit is that it allows for a seamless transition to compatible cloud-based LLMs such as OpenAI or Groq.\ntiktoken assists in token counting within queries, ensuring we optimize the model‚Äôs performance by staying within limits.\npython-dotenv is used for environment management, allowing us to store and access API keys and other sensitive information securely. To construct our interactive dashboard, we employ Streamlit, which significantly simplifies the development of user-friendly interfaces.\nRich (optional) is a library that enhances command-line outputs with rich text and formatting, useful for developers who prefer CLI tools. I won‚Äôt be using Rich in this tutorial, but I also have a CLI tool in the repository that uses Rich for formatting the output.\n\n\nSetting up the python project\nEasiest: If you are using poetry to manage your Python dependencies, you can get the pyproject.toml file from the repository and run poetry install to install the dependencies.\n\nUsing pip\nTo begin, create a dedicated project directory to house all your files and dependencies. Open your terminal or command prompt, navigate to your project directory, and initiate a Python virtual environment and activate it by running:\n\n macOS/  Linux Windows\n\n\npython -m venv venv\nsource venv/bin/activate\n\n\npython -m venv venv\n.\\venv\\Scripts\\activate\n\n\n\nThis ensures that all the dependencies installed are confined to this project, avoiding conflicts with other Python projects. Once your environment is active, install the aforementioned dependencies using pip, Python‚Äôs package installer. You can do this by running:\npip install langchain pypdf langchain-openai openai tiktoken python-dotenv streamlit rich\nWith your environment set and dependencies installed, you‚Äôre well-prepared to dive into the development of your AI-powered PDF processing app."
  },
  {
    "objectID": "posts/documents-llm/index.html#building-the-pdf-processing-app",
    "href": "posts/documents-llm/index.html#building-the-pdf-processing-app",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "Building the PDF processing app",
    "text": "Building the PDF processing app\nFor this app, I will be showcasing two methods for exploring documents: the stuffing method for document summarization and the map-reduce method for targeted document querying. It is adapted from the Summarization example in the LangChain documentation.\nThe stuffing method involves condensing the entire content of a PDF into a single, comprehensive query that the LLM can interpret and summarize. This technique is particularly useful for generating succinct overviews of documents, allowing users to grasp the core essence without reading the entire text. It‚Äôs a straightforward approach that mimics how one might ask a colleague to summarize a report they‚Äôve read, providing us with a distilled version of the document‚Äôs contents. It works best for shorter documents that can fit within the model‚Äôs context window, ensuring the summary remains concise and informative.\nOn the other hand, the map-reduce method takes a more granular approach, dissecting the document into manageable pieces and applying specific queries (mapping) to each segment. This method is akin to conducting a thorough examination of each page of a document to answer a particular question, with the ‚Äúreduce‚Äù phase aggregating these individual insights into a cohesive answer. It‚Äôs especially powerful for extracting specific information from documents, enabling precise and targeted queries across the entire text. The map-reduce method is ideal for longer documents or those with complex structures, allowing users to pinpoint and extract the data they need efficiently. It is, however, more computationally intensive than the stuffing method, as it requires processing each segment individually before combining the results.\nAnother popular method for querying documents is RAG (Retrieval-Augmented Generation), which involves first retrieving and filtering relevant information from a database or document corpus before generating a response with an LLM. While I won‚Äôt be covering RAG in this tutorial, it‚Äôs a powerful technique that can significantly enhance the quality of responses generated by LLMs, especially when dealing with very large or diverse datasets.\nIn the following sections, I‚Äôll cover the implementation details of these methods, guiding you through the process of building a fully functioning PDF processing app. From loading and processing documents to interfacing with LLMs and designing a user-friendly dashboard, I‚Äôll cover all the bases, ensuring you have the knowledge and tools to replicate and customize this solution for your own needs."
  },
  {
    "objectID": "posts/documents-llm/index.html#loading-pdf-documents-with-langchain",
    "href": "posts/documents-llm/index.html#loading-pdf-documents-with-langchain",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "Loading PDF Documents with LangChain",
    "text": "Loading PDF Documents with LangChain\nThe initial step in creating our PDF processing app involves efficiently loading and preparing the PDF documents for further analysis. To facilitate this, we will use LangChain, a comprehensive library designed to streamline the interaction with large language models and various document types, including PDFs, CSV files, and more.\n\nWhy Choose LangChain?\nLangChain stands out for its flexibility and robustness in handling different document formats, making it an ideal choice for our project. Not only does it support local files like PDFs, which are our primary focus, but it also offers compatibility with multiple other file types and integrates seamlessly with third-party data providers. After loading, the documents are transformed into a structured format that can be easily processed by the other components of our app, such as the AI models responsible for summarization and querying.\n\n\nLoading PDFs\nWe begin by importing the PDF document loader provided by LangChain, specifically designed for handling PDF files. For the examples in this tutorial, I‚Äôll be using my paper titled Price revelation from insider trading: Evidence from hacked earnings news (üîì open access).\nWith the file path specified, we proceed to create an instance of the PDF loader. Upon initiating the loader with our document, it parses the PDF, generating a list of documents where each entry corresponds to a page in the PDF. This structured approach ensures that each page is individually accessible for detailed analysis.\nfrom langchain_community.document_loaders.pdf import PyPDFLoader\n\nfile_path = \"hacking_prices.pdf\"\nloader = PyPDFLoader(file_path)\ndocs = loader.load()"
  },
  {
    "objectID": "posts/documents-llm/index.html#summarizing-pdf-documents-using-the-stuffing-method",
    "href": "posts/documents-llm/index.html#summarizing-pdf-documents-using-the-stuffing-method",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "Summarizing PDF Documents Using the Stuffing Method",
    "text": "Summarizing PDF Documents Using the Stuffing Method\nAfter successfully loading our PDF into ‚Äúdocuments‚Äù (one for each page), our next objective is to use a LLM to summarize these documents. The process we will use, known as the ‚Äústuffing method,‚Äù involves feeding the entire text of the document into a large language model (LLM) to generate a concise summary. The beauty of this method lies in its ability to produce an overview that captures the essence of the document, making it invaluable for quick insights into extensive research papers or reports.\n\nCrafting the Prompt\nThe first step in this process involves crafting a prompt that will guide the LLM in summarizing the document. For our application, we employ a template that instructs the model to focus exclusively on the content provided, excluding any external opinions or analysis. Here‚Äôs the structure of our prompt:\n# Prompt\nfrom langchain_core.prompts import PromptTemplate\n\nprompt_template = \"\"\"Write a long summary of the following document. \nOnly include information that is part of the document. \nDo not include your own opinion or analysis.\n\nDocument:\n\"{document}\"\nSummary:\"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\nThe prompt is defined as a template that specifies the desired output format and the content to be summarized. When invoking the LLM, LangChain will replace the {document} placeholder with the actual text from the PDF document.\n\n\nSetting up the LLM chain\nTo execute our summarization task, we utilize Ollama, specifically its OpenAI-compatible API, which allows for a seamless transition to GPT-4 or similar models in the future. Configuring the model involves setting parameters such as the temperature, which controls the creativity of the responses, and specifying the model name. In our case, I chose ‚ÄúMixtral‚Äù for its balance of speed and performance. Additionally, an API key is required for model authentication, and the base URL is adjusted to point to our local machine where Ollama is running. Note that even though we are using a local model, we must still provide an API key, which will be ignored by Ollama.\n# Define LLM Chain\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains.llm import LLMChain\n\nllm = ChatOpenAI(\n    temperature=0.1,\n    model_name=\"mixtral:latest\",\n    api_key=\"ollama\",\n    base_url=\"http://localhost:11434/v1\",\n)\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\n\nInvoking the stuff documents chain\nWith our language model chain configured, we proceed to create a ‚ÄúStuff Documents‚Äù chain using LangChain. This specialized chain will take our prompt templates and LLM configuration to generate summaries for each page of the PDF document.\n# Create full chain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\n\nstuff_chain = StuffDocumentsChain(\n    llm_chain=llm_chain, document_variable_name=\"document\"\n)\nOnce set up, we can invoke this chain with our loaded documents to generate the summary:\nresult = stuff_chain.invoke(docs)\nThe result will contain the summarized text. With my paper, the summary was:\n\nThis paper examines whether informed trading activities, such as insider trading and institutional trading, can predict post-earnings announcement drift (PEAD). The authors find that informed trading activities are positively related to PEAD, suggesting that these trades contain valuable information about future stock returns. They also find that the relation between informed trading and PEAD is stronger for stocks with higher levels of information asymmetry, such as those with lower institutional ownership or higher bid-ask spreads. The results suggest that informed traders are able to extract private information from earnings announcements and use it to earn abnormal returns.\n\nThis summary is not very good, as it takes the information from references in the paper‚Äôs bibliography and includes them in the summary. We can refine the summary by selecting only the pages that contain relevant information and omitting those that are primarily references or irrelevant details.\n# Invoke with limited pages\nresult = stuff_chain.invoke(docs[:-3])\nWith this, I get a much more accurate summary that focuses on the content of the paper itself, excluding the references:\n\nThe paper examines the impact of hacked newswire services on informed trading and stock prices. It finds that hacked firms experience a significant increase in effective spreads, which is driven by an increase in realized spreads rather than price impacts. However, there is no evidence of higher absolute order imbalance or quoted spreads for these firms. The paper also suggests that liquidity providers may be adjusting quotes to manage inventory risk associated with large buy/sell pressure. A placebo test using morning trades shows no significant differences in informed trading measures, further supporting the findings."
  },
  {
    "objectID": "posts/documents-llm/index.html#querying-pdf-documents-using-the-map-reduce-approach",
    "href": "posts/documents-llm/index.html#querying-pdf-documents-using-the-map-reduce-approach",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "Querying PDF documents using the map-reduce approach",
    "text": "Querying PDF documents using the map-reduce approach\nAfter exploring document summarization, we will see how we can query PDF documents for specific information. This includes a summary, but it can be much more. This capability is particularly useful when looking for particular data or answers within extensive documents. Unlike the stuffing method used for summarization, the map-reduce method involves dissecting and analyzing documents at a granular level.\nFor this example, I will use the following user query:\nuser_query = \"What is the data used in this analysis?\"\n\nThe map phase: document-specific queries\nIn the map phase, we apply a unique query to each page of the document, treating each page as a separate document within a larger set. This approach ensures that no detail is overlooked in our search for answers. To implement this, we craft a prompt that instructs the large language model to identify information relevant to a specific query from the text of each page. If a page is deemed irrelevant, the model is instructed to note this, ensuring only pertinent information is processed further.\nmap_template = \"\"\"The following is a set of documents\n{docs}\nBased on this list of documents, please identify the information that is most relevant to the following query:\n{user_query} \nIf the document is not relevant, please write \"not relevant\".\nHelpful Answer:\"\"\"\nmap_prompt = PromptTemplate.from_template(map_template)\nmap_prompt = map_prompt.partial(user_query=user_query)\nmap_chain = LLMChain(llm=llm, prompt=map_prompt)\n\n\nThe reduce phase: aggregating answers\nAfter mapping, we move to the reduce phase, where the outputs from the map phase are consolidated into a final, comprehensive answer. This step involves another carefully designed prompt that guides the model to distill the collected answers into a singular, coherent response to the original query. This process not only synthesizes the information gathered from each page but also ensures the final answer is succinct and directly addresses the query.\nreduce_template = \"\"\"The following is set of partial answers to a user query:\n{docs}\nTake these and distill it into a final, consolidated answer to the following query:\n{user_query} \nComplete Answer:\"\"\"\nreduce_prompt = PromptTemplate.from_template(reduce_template)\nreduce_prompt = reduce_prompt.partial(user_query=user_query)\n\n\nConstructing the full chain\nTo bring our querying process to life, we construct a full chain that encompasses both the map and reduce phases. This includes setting up separate LLM chains for mapping and reducing, ensuring each is tailored to its specific task within the overall process. The MapReduce Documents chain then binds these components together, managing the flow of information and ensuring the efficiency of the query process. This comprehensive setup guarantees that our queries are not only accurate but also optimized for performance, avoiding unnecessary processing and token usage.\nfrom langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n\n\nreduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n\n# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\ncombine_documents_chain = StuffDocumentsChain(\n    llm_chain=reduce_chain, document_variable_name=\"docs\"\n)\n\n# Combines and iteratively reduces the mapped documents\nreduce_documents_chain = ReduceDocumentsChain(\n    combine_documents_chain=combine_documents_chain,\n    collapse_documents_chain=combine_documents_chain,\n    # The maximum number of tokens to group documents into.\n    token_max=4000,\n)\n\n# Combining documents by mapping a chain over them, then combining results\nmap_reduce_chain = MapReduceDocumentsChain(\n    llm_chain=map_chain,\n    reduce_documents_chain=reduce_documents_chain,\n    document_variable_name=\"docs\",\n    return_intermediate_steps=False,\n)\n\n\nExecution and results\nExecuting this map-reduce process on a document will be more time-consuming than summarization due to the complexity and number of queries involved. However, the results are worth the wait, providing precise answers to specific questions, which, in our case, included a detailed summary of the data used in a research project. It will also work for longer documents that will not fit within the LLM‚Äôs context window and for which the stuffing method would not be suitable.\nresult = map_reduce_chain.invoke(docs[:-3])\nAfter about 5 minutes, the result was:\n\nThe data used in this analysis includes stock prices, trading volume, and order imbalance measures for a sample of U.S. firms that experienced a newswire hack between 2010 and 2014. Specifically, the authors use minute-level trade and quote data from the Trade and Quote (TAQ) database, which is maintained by the New York Stock Exchange. They also use firm-level financial data from Compustat and measures of media coverage from Factiva. The sample includes all U.S. common stocks listed on the NYSE, NASDAQ, or AMEX exchanges during the study period. Not all variables are available for all firms and time periods, resulting in an unbalanced panel. Additionally, the analysis uses information from legal documents of SEC prosecutions, newswire servers, and a set of control variables such as log market capitalization, fraction of shares held by institutional investors, natural logarithm of number of analysts, natural logarithm of newswire news in the quarter leading to the announcement, daily cost of borrowing from Markit, and the inverse of the stock price. The data is used to examine the impact of hackers‚Äô trading on volume, spreads, and order flow measures.\n\nOverall, this is a pretty decent answer to the query, providing a detailed overview of the data used in the analysis. The map-reduce method is a powerful tool for extracting specific information from documents, enabling targeted queries and detailed responses that address user queries effectively."
  },
  {
    "objectID": "posts/documents-llm/index.html#creating-a-ui-with-streamlit",
    "href": "posts/documents-llm/index.html#creating-a-ui-with-streamlit",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "Creating a UI with Streamlit",
    "text": "Creating a UI with Streamlit\nTransforming our PDF summarization and querying capabilities into a user-friendly application enhances accessibility and utility, making these powerful functions easier to use and understand. For this purpose, I use Streamlit, a dynamic Python framework that simplifies the creation of interactive web applications. Streamlit‚Äôs intuitive design and extensive features allow us to build a sleek UI without the need for complex web development skills. This approach enables the user to interact with our local AI models through a browser interface, providing a seamless experience for summarizing and querying documents. Streamlit apps are easy to deploy and share, but in this case, it would not be possible to share the app with others, as it requires access to the local LLM.\n\nStructuring the Streamlit application\nOur Streamlit application, encapsulated within doc_app.py, serves as the gateway for users to access the summarization and querying functionalities. The application‚Äôs architecture is straightforward yet efficient, integrating various components that facilitate user interaction and display results.\nHere is the basic structure of the files for this application:\n.\n‚îú‚îÄ‚îÄ doc_app.py : Streamlit application\n‚îú‚îÄ‚îÄ documents_llm\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ document.py : Document loading and processing\n‚îÇ   ‚îú‚îÄ‚îÄ query.py : Querying operations\n‚îÇ   ‚îú‚îÄ‚îÄ st_helpers.py : Streamlit helper functions\n‚îÇ   ‚îî‚îÄ‚îÄ summarize.py: Summarization operations\n‚îú‚îÄ‚îÄ poetry.lock : Poetry lock file\n‚îî‚îÄ‚îÄ pyproject.toml : Poetry project file\nThe files document.py, query.py and summarize.py contain the logic for loading and processing documents, querying operations, and summarization tasks, respectively. These files encapsulate the core functionality of our application, abstracting the underlying operations into modular components for enhanced readability and maintainability.\nThey provide the following functions that encapsulate what we have done in the previous sections:\ndef load_pdf(\n    file_path: Path | str, start_page: int = 0, end_page: int = -1\n) -&gt; list[Document]\n\ndef summarize_document(\n    docs: list[Document],\n    model_name: str,\n    openai_api_key: str,\n    base_url: str,\n    temperature: float = 0.1,\n) -&gt; str\n\ndef query_document(\n    docs: list[Document],\n    user_query: str,\n    model_name: str,\n    openai_api_key: str,\n    base_url: str,\n    temperature: float = 0.3,\n) -&gt; str\nThe Streamlit application is defined in doc_app.py. Here is what the header of the file looks like:\nimport os\nimport time\n\nimport streamlit as st\nfrom dotenv import load_dotenv\n\nfrom documents_llm.st_helpers import run_query\n\n# Load environment variables\nload_dotenv()\n\n# Load model parameters\nMODEL_NAME = os.getenv(\"MODEL_NAME\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nOPENAI_URL = os.getenv(\"OPENAI_URL\")\n\n\nst.title(\"üêç VCF Document Analyzer\")\nst.write(\n    \"This is a simple document analyzer that uses LLM models to summarize and answer questions about documents. \"\n    \"You can upload a PDF or text file and the model will summarize the document and answer questions about it.\"\n)\nWe first import dependencies, load environment variables, and define the title and introductory text for our application. The st. functions are used to create various UI elements, such as titles, text, and file upload buttons, making it easy to design a user-friendly interface.\n\n\nUser input\nThe application‚Äôs sidebar is designed to collect user inputs, such as the model name, temperature settings, and the document to be analyzed. This design allows for a customizable experience, where users can adjust parameters according to their needs and upload PDF files directly into the application.\nOnce a document is uploaded, users can specify the range of pages they wish to include in their analysis, further refining the scope of the summarization or query. This functionality ensures that the application‚Äôs output is tailored to the user‚Äôs precise requirements.\nwith st.sidebar:\n    st.header(\"Model\")\n\n    model_name = st.text_input(\"Model name\", value=MODEL_NAME)\n\n    temperature = st.slider(\"Temperature\", value=0.1, min_value=0.0, max_value=1.0)\n\n    st.header(\"Document\")\n    st.subheader(\"Upload a PDF file\")\n    file = st.file_uploader(\"Upload a PDF file\", type=[\"pdf\"])\n    if file:\n        st.write(\"File uploaded successfully!\")\n\n    st.subheader(\"Page range\")\n\n    st.write(\n        \"Select page range. Pages are numbered starting at 0. For end page, you can also use negative numbers to count from the end, e.g., -1 is the last page, -2 is the second to last page, etc.\"\n    )\n    col1, col2 = st.columns(2)\n    with col1:\n        start_page = st.number_input(\"Start page:\", value=0, min_value=0)\n    with col2:\n        end_page = st.number_input(\"End page:\", value=-1)\n\n    st.subheader(\"Query type\")\n\n    query_type = st.radio(\"Select the query type\", [\"Summarize\", \"Query\"])\nIf the user selects the ‚ÄúQuery‚Äù option, we also need to get the query from the user. We want this in the main body of the page, so it will be displayed outside of the with st.sidebar block.\nif query_type == \"Query\":\n    user_query = st.text_area(\n        \"User query\", value=\"What is the data used in this analysis?\"\n    )\n\n\nHelper functions\nThe core of our application lies in its ability to perform summarization and querying tasks based on the user‚Äôs inputs. To this end, we have abstracted the logic into helper functions that manage the loading of PDF files, the construction of prompts, and the invocation of the appropriate LangChain processes. These functions are defined in st_helpers.py and are imported into the Streamlit application.\nThe first helper function, save_uploaded_file, handles the file upload process, saving the uploaded PDF file to a temporary location for processing. Streamlit keeps uploaded files in memory, but we need to save them to disk to work with LangChain.\ndef save_uploaded_file(\n    uploaded_file: \"UploadedFile\", output_dir: Path = Path(\"/tmp\")\n) -&gt; Path:\n    output_path = Path(output_dir) / uploaded_file.name\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(output_path, \"wb\") as f:\n        f.write(uploaded_file.getbuffer())\n    return output_path\nWith this function in place, we can now see how the application‚Äôs main logic is structured to handle document loading, summarization, and querying based on user inputs:\ndef run_query(\n    uploaded_file: \"UploadedFile\",\n    summarize: bool,\n    user_query: str,\n    start_page: int,\n    end_page: int,\n    model_name: str,\n    openai_api_key: str,\n    openai_url: str,\n    temperature: float,\n) -&gt; str:\n    # Saves the uploaded file to a temporary location, loads the PDF, and deletes the file\n    st.write(\"Saving the uploaded file...\")\n    file_path = save_uploaded_file(uploaded_file, output_dir=Path(\"/tmp\"))\n    st.write(\"Loading the document...\")\n    docs = load_pdf(file_path, start_page=start_page, end_page=end_page)\n    file_path.unlink()\n\n    if summarize:\n        st.write(\"Summarizing the document...\")\n        return summarize_document(\n            docs,\n            model_name=model_name,\n            openai_api_key=openai_api_key,\n            base_url=openai_url,\n            temperature=temperature,\n        )\n    st.write(\"Querying the document...\")\n    return query_document(\n        docs,\n        user_query=user_query,\n        model_name=model_name,\n        openai_api_key=openai_api_key,\n        base_url=openai_url,\n        temperature=temperature,\n    )\nNote that the st.write statements are used to provide feedback to the user during the processing of the document. Because this function is called inside a with st.status block (see below), the user will see these messages inside the status widget as the document is being processed.\n\n\nExecuting summarization and querying operations\nFinally, the rest of the Streamlit application is dedicated to executing the summarization and querying operations based on the user‚Äôs inputs. We have one button for running the query and displaying the results or an error message:\nif st.button(\"Run\"):\n    result = None\n    start = time.time()\n    if file is None:\n        st.error(\"Please upload a file.\")\n    else:\n        with st.status(\"Running...\", expanded=True) as status:\n            try:\n                result = run_query(\n                    uploaded_file=file,\n                    summarize=query_type == \"Summarize\",\n                    user_query=user_query if query_type == \"Query\" else \"\",\n                    start_page=start_page,\n                    end_page=end_page,\n                    model_name=model_name,\n                    openai_api_key=OPENAI_API_KEY,\n                    openai_url=OPENAI_URL,\n                    temperature=temperature,\n                )\n                status.update(label=\"Done!\", state=\"complete\", expanded=False)\n\n            except Exception as e:\n                status.update(label=\"Error\", state=\"error\", expanded=False)\n                st.error(f\"An error occurred: {e}\")\n                result = \"\"\n\n        if result:\n            with st.container(border=True):\n                st.header(\"Result\")\n                st.markdown(result)\n                st.info(f\"Time taken: {time.time() - start:.2f} seconds\", icon=\"‚è±Ô∏è\")\nOnce the app is complete, you can run it using the following command:\nstreamlit run doc_app.py"
  },
  {
    "objectID": "posts/documents-llm/index.html#final-thoughts",
    "href": "posts/documents-llm/index.html#final-thoughts",
    "title": "Summarize and query PDFs with AI using Ollama",
    "section": "Final thoughts",
    "text": "Final thoughts\nBy integrating Streamlit into our project, we‚Äôve created an accessible and powerful tool that bridges the gap between complex AI models and end-users seeking to extract valuable insights from PDF documents.\nThis application demonstrates the practical application of AI in document analysis, but it remains very simple. I hope it inspires you to explore the possibilities of AI-driven document processing further, whether for research, business, or personal use. The combination of LangChain, Ollama, and Streamlit provides a robust foundation for building sophisticated applications that leverage the power of large language models to enhance productivity and efficiency."
  },
  {
    "objectID": "posts/ollama-chatgpt/index.html",
    "href": "posts/ollama-chatgpt/index.html",
    "title": "Using Ollama as a ChatGPT Replacement",
    "section": "",
    "text": "Local Large Language Models (LLMs) like Ollama offer a powerful alternative to cloud-based solutions like ChatGPT. In this post, explore the benefits of using Ollama as a ChatGPT replacement for empirical finance research, focusing on privacy, customization, and computational flexibility. ChatGPT has become a daily driver for many researchers and professionals, including myself, offering a powerful tool for generating text, code, and insights across a wide range of domains. However, the reliance on closed, cloud-based solutions for leveraging Large Language Models (LLMs) like ChatGPT comes with inherent privacy, data security, and replicability concerns, especially in fields like empirical finance research.\nEnter Ollama, a tool that allows researchers and professionals to manage and run open-source LLMs such as Meta‚Äôs Llamma 2 and Mistral AI‚Äôs Mistral, bypassing the need for cloud-based solutions. This post, the first in a series of three, aims to demystify the process of installing and using Ollama as a ChatGPT replacement. We‚Äôll delve into the advantages of running LLMs on your own machines, offering you full control over your data while catering to the specific needs of finance-related research. Subsequent posts will explore Ollama‚Äôs prowess as a replacement for GitHub Copilot, enhancing coding efficiency, and its capabilities as a Python API for advanced text processing, opening new doors for empirical finance analysis and beyond."
  },
  {
    "objectID": "posts/ollama-chatgpt/index.html#video-tutorial",
    "href": "posts/ollama-chatgpt/index.html#video-tutorial",
    "title": "Using Ollama as a ChatGPT Replacement",
    "section": "Video tutorial",
    "text": "Video tutorial\nThis post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/ollama-chatgpt/index.html#ollama",
    "href": "posts/ollama-chatgpt/index.html#ollama",
    "title": "Using Ollama as a ChatGPT Replacement",
    "section": "Ollama",
    "text": "Ollama\nThe open-source community was set ablaze with excitement following Meta‚Äôs release of LLaMA, the first ‚Äúbig‚Äù open-source Large Language Model (LLM). This pivotal moment marked a shift towards democratizing access to powerful AI tools, sparking a fervent race to discover the most efficient ways to harness such models on consumer hardware. Amidst this bustling innovation, one solution emerged as a beacon for those seeking to leverage the immense potential of LLMs within their own computing environments: Ollama.\n\nWhat is Ollama?\nOllama is a cutting-edge software designed to simplify the process of downloading, managing, and running open-source LLMs directly on your computer. Recognizing the complexities and technical challenges involved in setting up and deploying LLMs, Ollama offers a streamlined solution that makes these powerful tools more accessible to a wider audience. Whether you‚Äôre a researcher, developer, or enthusiast, Ollama provides the necessary ‚Äúback-end‚Äù infrastructure to run these models smoothly on your local machine.\n\n\nThe Power of Local Processing\nAt its core, Ollama harnesses the power of local processing, offering users complete control over their data and the AI models they interact with. This approach not only enhances privacy and security but also allows for greater flexibility and customization to meet specific needs or research goals. With Ollama, the complexities of running LLMs are abstracted away, leaving users free to focus on their work without worrying about the underlying technicalities.\n\n\nLimitations of Local LLMs\nWhile the benefits of local LLMs are clear, it‚Äôs important to acknowledge that running these models on consumer hardware comes with its own set of limitations. The computational resources required to run LLMs are substantial, and while Ollama provides a robust solution for managing these resources, users should be mindful of the hardware and memory constraints of their local machines. For best results, it‚Äôs recommended to run Ollama on a machine with a powerful GPU and ample memory to ensure smooth and efficient operation. Apple‚Äôs M-series chips, because they share memory between the CPU and GPU, are particularly well-suited for running Ollama and LLMs.\n\n\nFront-End Applications for Enhanced Interaction\nWhile Ollama serves as the robust engine running the models, most users seek a more intuitive way to interact with their LLMs beyond the command line. To address this, Ollama supports the integration of various ‚Äúfront-end‚Äù applications, each offering a unique interface and set of features tailored to different user preferences and use cases. In this post, we will explore two such front-end applications that serve as excellent ChatGPT replacements:\n\nOllama-UI: A Chrome extension that enables users to access and interact with their LLMs directly from their web browser, offering convenience and flexibility for those who prefer web-based tools. Easy to install but limited in features.\nOpen WebUI: A comprehensive ChatGPT replacement that offers a full-featured web interface, catering to users looking for a robust and feature-rich platform to leverage the capabilities of their local LLMs.\n\nAs we delve into the specifics of these front-end applications, it‚Äôs clear that Ollama is not just a tool but a gateway to unlocking the full potential of open-source LLMs. By bridging the gap between powerful AI models and everyday users, Ollama is setting the stage for a new era of innovation and accessibility in the world of artificial intelligence.\n\n\nOther Use Cases\nWhile this post focuses on using Ollama as a ChatGPT replacement, it‚Äôs important to note that Ollama‚Äôs capabilities extend far beyond text generation. In subsequent posts, we will explore two additional use cases for Ollama:\n\nGitHub Copilot Replacement: Some models like CodeLlama and Mistral are designed to assist with code generation and programming tasks, making them ideal replacements for GitHub Copilot. Combined with Visual Studio Code extensions, Ollama offers a powerful alternative for developers seeking to enhance their coding efficiency and productivity.\nOpenAI API Replacement: Ollama can serve as a Python API for advanced text processing, enabling users to leverage the capabilities of open-source LLMs for a wide range of natural language processing tasks. From sentiment analysis to language translation, Ollama opens new doors for researchers and professionals seeking to harness the power of AI in their work. For researchers in empirical finance, this means being able to use open-source LLMs to analyze and interpret financial documents, news articles, and other textual data with greater control, replicability, and privacy."
  },
  {
    "objectID": "posts/ollama-chatgpt/index.html#setting-up-ollama",
    "href": "posts/ollama-chatgpt/index.html#setting-up-ollama",
    "title": "Using Ollama as a ChatGPT Replacement",
    "section": "Setting Up Ollama",
    "text": "Setting Up Ollama\nOllama is a command-line tool that is available for macOS, Linux, and (experimental) Windows, making it accessible to a wide range of users. In this section, we‚Äôll walk through the process of downloading and installing Ollama, setting up the necessary LLM models, and running Ollama to interact with these models. We‚Äôll also explore the front-end applications that can be used to enhance the user experience when working with Ollama.\n\nDownload and Installation\nThe first step in setting up Ollama is to download and install the tool on your local machine. The installation process is straightforward and involves running a few commands in your terminal. Ollama‚Äôs download page provides installers for macOS and Windows, as well as instructions for Linux users. Once you‚Äôve downloaded the installer, follow the installation instructions to set up Ollama on your machine.\nIf you‚Äôre using a Mac, you can install Ollama using Homebrew by running the following command in your terminal:\nbrew install ollama\nThe benefit of using Homebrew is that it simplifies the installation process and also sets up Ollama as a service, allowing it to run in the background and manage the LLM models you download.\n\n\nDownloading and Running LLM Models\nThe list of available LLM models that can be run using Ollama is constantly expanding, with new models being added regularly. To see the current list, you can check the Ollama Library page. Once you‚Äôve identified the models you‚Äôd like to use, you can download them using the ollama pull command followed by the model name. For example, to download the llama2 model, you would run:\nollama pull llama2\nIn order to run the models, you will need to start the Ollama service. If you installed using Homebrew or activated the service during installation, you have nothing to do. If not, you can start the service using the ollama serve command. This will initialize the Ollama service and allow you to interact with the models you‚Äôve downloaded.\nAfter downloading the model, you can run it using the ollama run command followed by the model name. For example, to start the llama2 model, you would run:\nollama run llama2\nThis will let you interact with the model directly from the command line, allowing you to generate text, code, or other outputs based on your input. However, for a more user-friendly experience, you will want to explore the front-end applications that Ollama supports instead of running it directly from the command line.\nThe most popular models available for use with Ollama are llama2, mistral, and mixtral. Each of these models offers unique capabilities and performance characteristics, catering to different use cases and hardware configurations, but according to the latest benchmark, mixtral is the most powerful and capable model, offering the best performance across a wide range of tasks.\n\n\nModel Variants and Sizes\nModels like Llama 2 and Mistral come in different sizes and variants, each offering a unique balance of computational power and specificity.\nFor example, the llama2 model is available in multiple variants along three dimensions:\n\nNumber of parameters: 7b, 13b, and 70b, each representing the number of parameters in the model in billions. The larger the model, the more powerful and capable it is, but it also requires more computational resources to run.\nQuantization: q***, where *** represents the number of bits used to represent the model‚Äôs parameters and the quantization method used. LLM models weights are typically stored as 32-bit floating-point numbers, but quantization allows for the use of lower precision representations, such as 16-bit, 8-bit and as low as 2-bit. Lower quantization levels result in smaller model sizes and faster inference times, but may come at the cost of reduced model performance.\nTuning: text or chat, indicating whether the model has been fine-tuned for text generation or chat.\n\nThe mistral and mixtral models also come in different sizes and variants, each tailored to specific use cases and hardware configurations. By understanding the available model variants and sizes, you can choose the one that best suits your needs and computing environment, ensuring optimal performance and efficiency when running Ollama. This will require some experimentation to find the right balance between model size and performance for your specific use case and hardware configuration.\nOther models like CodeLlama and Mistral are designed specifically for code generation and programming tasks, offering a powerful alternative to GitHub Copilot. These models are optimized for understanding and generating code, making them ideal for developers and researchers working on programming-related projects.\n\n\nModels I Use\nHere are the commands to install the models I use on my MacBook Pro M3 Max with 64GB of RAM:\nollama pull llama2\nollama pull mistral\nollama pull Mixtral\nollama pull llama2-uncensored\nollama pull CodeLlama\nollama pull deepseek-coder\nMixtral is the most powerful, but it requires a lot of memory and a powerful GPU to run. I use it for generating long-form content and for more complex tasks. Llama2 is a good all-rounder, and I use it for most of my text-generation tasks. Mistral is a smaller model that is well-suited for tasks that require less computational power, and CodeLlama is my go-to model for code generation and programming tasks.\nThese are also some of the most popular models available for use with Ollama, but I have yet to explore the other models available in the Ollama Library."
  },
  {
    "objectID": "posts/ollama-chatgpt/index.html#setting-up-front-end-applications",
    "href": "posts/ollama-chatgpt/index.html#setting-up-front-end-applications",
    "title": "Using Ollama as a ChatGPT Replacement",
    "section": "Setting Up Front-End Applications",
    "text": "Setting Up Front-End Applications\nI use two front-end applications to interact with Ollama: Ollama-UI and Open WebUI. These applications provide a more user-friendly interface for interacting with the LLM models, offering a range of features and capabilities that enhance the overall user experience. In this section, we‚Äôll explore the installation and setup process for each of these front-end applications, highlighting their unique features and use cases.\n\nOllama-UI\nOllama-UI is a Chrome extension that allows users to access and interact with their LLM models directly from their web browser. This convenient front-end application offers a simple and intuitive interface for generating text, code, and other outputs based on user input. To install Ollama-UI, you need to get it from the Chrome Web Store. Once installed, you can access it by clicking on the Ollama icon in your browser‚Äôs toolbar.\nFor it to work, you need to have the Ollama service running on your local machine. The interface is simple and easy to use, allowing you to input text and receive outputs from the LLM models you‚Äôve downloaded.\n\n\nOpen WebUI\nOpen WebUI is a full-featured web interface that offers a comprehensive platform for interacting with your local LLM models. This powerful front-end application provides a range of features and capabilities, including the ability to manage and run multiple models simultaneously, customize model settings, and access advanced options for generating text, code, and other outputs. Installing it is a bit more involved than Ollama-UI, but it offers a more robust and feature-rich platform for leveraging the capabilities of your local LLM models.\nTo install Open WebUI, you will need to first install Docker if you don‚Äôt already have it on your machine. Docker is a platform running applications using containerization, i.e.¬†in self-contained environments. You can download and install Docker from the official website.\nIf you‚Äôre using a Mac, you can install Docker and Docker Desktop using Homebrew by running the following command in your terminal:\nbrew install docker\nbrew install --cask docker\nOnce you have Docker installed, you can run the following command in your terminal to start Open WebUI:\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nThis command will start the Open WebUI service and allow you to access the web interface by navigating to http://localhost:3000 in your web browser. From there, you can interact with your local LLM models, customize settings, and generate text, code, and other outputs based on your input.\nOnce you have Open WebUI up and running, you can explore its various features and capabilities. Note that the first time you start a new chat, Ollama will need to load the model, which can take a few seconds or longer depending on the model size and your hardware configuration. Once the model is loaded, you can start generating text and interacting with the model in real-time.\nIn the chat interface, you can select the model you want to use, input text, and receive outputs from the model. You can even select multiple models to run simultaneously, allowing you to compare the outputs and performance of different models. Aside from basic chat, here are the main features of Open WebUI that are available in the sidebar:\n\nModelfiles: These are WebUI‚Äôs equivalent of ChatGPT‚Äôs ‚ÄúGTPs‚Äù. They are pre-defined combinations of prompts and settings that can be used to generate specific types of outputs. For example, you might have a modelfile for generating code, another for summarizing text, and another for answering questions. You can start by looking at the modelfiles created by the community and available at https://openwebui.com.\nPrompts: A place to save and manage prompt templates that you frequently use.\nDocuments: A place to save and manage documents that you want your models to be able to refer to. Note that the documents are not accessible as a whole to the model, instead they are available through a RAG (retrieval-augmented generation) mechanism. In practice, this means that the model can search for information in the documents and get the most relevant ‚Äúchunks‚Äù of information to generate a response, but it doesn‚Äôt have access to the full documents. It is thus useful for accessing reference material, but not for summarizing or generating text based on the full content of the documents.\n\nOpen WebUI offers a range of advanced options and settings that allow you to customize the behavior of your local LLM models, making it a powerful platform for leveraging the capabilities of Ollama. Other features that go beyond the scope of this post include the ability to have multiple registered users, to generate images, and to access text-to-speech and speech-to-text capabilities."
  },
  {
    "objectID": "posts/ollama-chatgpt/index.html#conclusion",
    "href": "posts/ollama-chatgpt/index.html#conclusion",
    "title": "Using Ollama as a ChatGPT Replacement",
    "section": "Conclusion",
    "text": "Conclusion\nThe rise of open-source LLMs has ushered in a new era of innovation and accessibility in the world of artificial intelligence. There is so much potential for these models to transform the way we work and interact with AI, and Ollama is at the forefront of this revolution. By providing a streamlined solution for running LLMs on consumer hardware, Ollama is empowering researchers, developers, and enthusiasts to harness the full potential of these powerful models without relying on cloud-based solutions. It‚Äôs now up to us to explore the possibilities and push the boundaries of what‚Äôs possible with Ollama and open-source LLMs."
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html",
    "href": "posts/panel-ols-standard-errors/index.html",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "",
    "text": "In this post, I show how to estimate standard errors in panel data with Python and the linearmodels library.\nMore specifically, I show how to estimate the following class of models:\nIf you just want the code examples with no explanations, jump to the cheat sheet at the end of the post.\nFor time series models, see my post on estimating standard errors in time series data with Python and statsmodels."
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#video-tutorial",
    "href": "posts/panel-ols-standard-errors/index.html#video-tutorial",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Video tutorial",
    "text": "Video tutorial\nThis post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#panel-data",
    "href": "posts/panel-ols-standard-errors/index.html#panel-data",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Panel data",
    "text": "Panel data\nOne of the most common tasks in finance research is to estimate standard errors in panel data. Linear panel data models have the following form:\n\\[\ny_{it} =  \\alpha_i + \\alpha_t + x_{it}\\beta  + \\epsilon_{it},\n\\]\nwhere \\(y_{it}\\) is the dependent variable for entity \\(i\\in [1,I]\\) at time \\(t\\in [1,T]\\), \\(\\alpha_i\\) is an entity fixed effect, \\(\\alpha_t\\) is a time fixed effect, \\(x_{it}\\) is a vector of independent variables, \\(\\beta\\) is a vector of coefficients, and \\(\\epsilon_{it}\\) is the error term. For the sake of simplicity, in the rest of this post, I assume that the entity is a firm and the time is a year. The error term can be correlated across firms and years and can be heteroskedastic, meaning that the variance of the error term is not constant.\nWhen we get to the data, we write the model in matrix form:\n\\[\n\\mathbf{y} = \\mathbf{X}\\beta + \\mathbf{\\epsilon},\n\\]\nwhere \\(\\mathbf{y}\\) is a \\(N \\times 1\\) vector of dependent variables, \\(\\mathbf{X}\\) is a \\(N \\times K\\) matrix of independent variables, \\(\\beta\\) is a \\(K \\times 1\\) vector of coefficients, and \\(\\mathbf{\\epsilon}\\) is a \\(N \\times 1\\) vector of error terms. The number of observations is \\(N\\) and the number of independent variables is \\(K\\). For a balanced panel in which each firm is observed for the same number of years, \\(N=I \\times T\\).\n\n\n\n\n\n\nTipMore on matrix form representation\n\n\n\n\n\nIf the model includes fixed effects, we assume that the vector \\(\\mathbf{y}\\) and the matrix \\(\\mathbf{X}\\) are demeaned by the relevant fixed effect.\nIt is then common to stack the observations for each entity. For example, if we have \\(I\\) firms and \\(T\\) years, the vector \\(\\mathbf{y}\\) of elements \\(y_{i,t}\\) is defined as:\n\\[\n\\mathbf{y} = \\left[\\begin{array}{c}y_{1,1}\\\\ y_{1,2}\\\\ \\vdots  \\\\ y_{1, T} \\\\ y_{2,1}\\\\ y_{2,2}\\\\ \\vdots  \\\\ y_{2, T}\\\\ y_{I,1}\\\\ \\vdots  \\\\ y_{I, T} \\end{array} \\right],\n\\]\nThe matrix \\(\\mathbf{X}\\) formed with vectors \\(x_{i,t}\\) is defined as:\n\\[\n\\mathbf{X} = \\left[\\begin{array}{ccccc}\nx_{1, 1}^{(1)} & x_{1, 1}^{(2)} & \\cdots & x_{1, 1}^{(K-1)} & x_{1,1}^{(K)} \\\\\nx_{1, 2}^{(1)} & x_{1, 2}^{(2)} & \\cdots & x_{1, 2}^{(K-1)} & x_{1,2}^{(K)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nx_{1, T}^{(1)} & x_{1, T}^{(2)} & \\cdots & x_{1, T}^{(K-1)} & x_{1,T}^{(K)} \\\\\nx_{2, 1}^{(1)} & x_{2, 1}^{(2)} & \\cdots & x_{2, 1}^{(K-1)} & x_{2,1}^{(K)} \\\\\nx_{2, 2}^{(1)} & x_{2, 2}^{(2)} & \\cdots & x_{2, 2}^{(K-1)} & x_{2,2}^{(K)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nx_{2, T}^{(1)} & x_{2, T}^{(2)} & \\cdots & x_{2, T}^{(K-1)} & x_{2,T}^{(K)} \\\\\nx_{I, 1}^{(1)} & x_{I, 1}^{(2)} & \\cdots & x_{I, 1}^{(K-1)} & x_{I,1}^{(K)} \\\\\nx_{I, 2}^{(1)} & x_{I, 2}^{(2)} & \\cdots & x_{I, 2}^{(K-1)} & x_{I,2}^{(K)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nx_{I, T}^{(1)} & x_{I, T}^{(2)} & \\cdots & x_{I, T}^{(K-1)} & x_{I,T}^{(K)}\n\\end{array} \\right],\n\\]\nwhere \\(x_{i,t}^{(j)}\\) is the \\(j\\)-th element of the vector \\(x_{i,t}\\).\nFinally, the \\(\\beta\\) vector is defined as usual:\n\\[\n\\beta = \\left[\\begin{array}{c}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_{K-1} \\\\ \\beta_{K} \\end{array} \\right].\n\\]\n\n\n\nThe goal is then to estimate the coefficients \\(\\beta\\) and their associated standard errors. The standard errors are important because they allow us to perform statistical inference. For example, we can test whether the coefficients are statistically different from zero.\nIn his seminal paper, Petersen (2009) compares the performance of different standard errors estimators in panel data. In addition, he provides programming advice on how to estimate standard errors in panel data with Stata and other languages, but not for Python.\nIn this post, I explain how to estimate standard errors in panel data with Python and the linearmodels library. I do so by replicating the sample results from Petersen‚Äôs test data page in Python, and cover a few more standard errors estimators that are useful in finance research.\nI use the test data set provided by Mitchell Petersen and available here. The data set contains 500 firms and 10 years.\n\nimport pandas as pd\n\ndf = pd.read_table(\n    \"http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt\",\n    names=[\"firmid\", \"year\", \"x\", \"y\"],\n    sep=r\"\\s+\",\n)\ndf\n\n\n\n\n\n\n\n\nfirmid\nyear\nx\ny\n\n\n\n\n0\n1\n1\n-1.113973\n2.251535\n\n\n1\n1\n2\n-0.080854\n1.242346\n\n\n2\n1\n3\n-0.237607\n-1.426376\n\n\n3\n1\n4\n-0.152486\n-1.109394\n\n\n4\n1\n5\n-0.001426\n0.914686\n\n\n...\n...\n...\n...\n...\n\n\n4995\n500\n6\n-0.077057\n3.720502\n\n\n4996\n500\n7\n0.218847\n0.559121\n\n\n4997\n500\n8\n-0.155530\n-3.766785\n\n\n4998\n500\n9\n-0.040172\n0.903354\n\n\n4999\n500\n10\n-0.001172\n-0.529761\n\n\n\n\n5000 rows √ó 4 columns"
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#linearmodels",
    "href": "posts/panel-ols-standard-errors/index.html#linearmodels",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "linearmodels",
    "text": "linearmodels\nThe linearmodels library is a Python package written by Kevin Sheppard at the University of Oxford that extends the statsmodels library with functions commonly used in financial econometrics. Of note, the documentation is quite good and the library is actively maintained.\n\nInstallation\nYou can install the library with your package manager of choice:\npoetry:\npoetry add linearmodels\npip:\npip install linearmodels\nconda (linearmodels is not available in the default conda channel, but it is available in the conda-forge channel):\nconda install -c conda-forge linearmodels\n\n\nUsage\nThe library is easy to use, and intuitive for anyone familiar with statsmodels. In this post, I will use two model types: PanelOLS and FamaMacBeth. As their names imply, the PanelOLS class is used to estimate panel data models, and the FamaMacBeth class is used to estimate Fama-MacBeth regressions. The library also provides implementations for random effects and first differences models, and for between estimation and pooled OLS estimators. In addition to panel data models, the library also provides implementations for instrumental variables models (including 2SLS, 3SLS, GMM, and SUR) and for estimating linear factor models.\nA key particularity of linearmodels is that it uses the indexing capabilities of pandas to identify the panel data structure. In particular, the DataFrame that contains your panel data must have a MultiIndex with the entity dimension in the first level and the time dimension in the second level:\n\ndf = df.set_index([\"firmid\", \"year\"])\ndf\n\n\n\n\n\n\n\n\n\nx\ny\n\n\nfirmid\nyear\n\n\n\n\n\n\n1\n1\n-1.113973\n2.251535\n\n\n2\n-0.080854\n1.242346\n\n\n3\n-0.237607\n-1.426376\n\n\n4\n-0.152486\n-1.109394\n\n\n5\n-0.001426\n0.914686\n\n\n...\n...\n...\n...\n\n\n500\n6\n-0.077057\n3.720502\n\n\n7\n0.218847\n0.559121\n\n\n8\n-0.155530\n-3.766785\n\n\n9\n-0.040172\n0.903354\n\n\n10\n-0.001172\n-0.529761\n\n\n\n\n5000 rows √ó 2 columns\n\n\n\nYou define models by passing the dependent variable and the exogenous variables to the model class. Alternatively, following statsmodels, linearmodels leverages the patsy formula language to provide a more intuitive way to define models. For example, the following two lines of code are equivalent:\nmod = PanelOLS(data[\"y\"], data[[\"x1\", \"x2\"]])\nmod = PanelOLS.from_formula(formula=\"y ~ x1 + x2\", data)\nYou then estimate models by calling the fit() method. The fit() method returns a linearmodels regression results object, which contains the estimated coefficients, standard errors, and other statistics. The linearmodels results object is similar to the statsmodels results object, but it contains additional methods and attributes that are useful for panel data models."
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#ols-coefficients-and-standard-errors",
    "href": "posts/panel-ols-standard-errors/index.html#ols-coefficients-and-standard-errors",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "OLS coefficients and standard errors",
    "text": "OLS coefficients and standard errors\nThe first model I estimate is a vanilla OLS model.\nThe OLS estimator is the most common estimator in finance research. It is also the most efficient estimator when the errors are homoskedastic and uncorrelated across entities and time.\nThe OLS estimator is defined as:\n\\[\n\\hat{\\beta}_{OLS} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y},\n\\]\nwhere \\(\\mathbf{X}\\) is the matrix of exogenous variables and \\(\\mathbf{y}\\) is the vector of dependent variables. The covariance matrix \\(\\Sigma\\) of the estimator is defined as:\n\\[\n\\Sigma_{OLS}  =\\sigma_{\\mathbf{\\epsilon}}^{2}\\cdot\\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1},\n\\]\nwhere \\(\\sigma_{\\mathbf{\\epsilon}}^{2}\\) is usually estimated with the residuals \\(\\mathbf{\\hat{e}}=( \\mathbf{y}-\\mathbf{X}\\hat{\\beta}_{OLS})\\) from the OLS regression:\n\\[\n\\hat{\\sigma}_{\\mathbf{\\epsilon}}^{2} =\\frac{1}{N-K}\\mathbf{\\hat{e}}'\\mathbf{\\hat{e}}.\n\\]\nThe estimate of the covariance matrix is then:\n\\[\n\\hat{\\Sigma}_{OLS}  =\\frac{1}{N-K}\\mathbf{\\hat{e}}'\\mathbf{\\hat{e}}\\cdot\\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1},\n\\]\n\\[\n\\hat{\\Sigma}_{OLS}  =\\left[\\begin{array}{ccccc}\n\\hat{\\sigma}_{\\beta_1}^2 & \\hat{\\sigma}_{\\beta_1,\\beta_2} & \\cdots & \\hat{\\sigma}_{\\beta_1,\\beta_{K-1}} & \\hat{\\sigma}_{\\beta_1,\\beta_K} \\\\\n\\hat{\\sigma}_{\\beta_2,\\beta_1} & \\hat{\\sigma}_{\\beta_2}^2 & \\cdots & \\hat{\\sigma}_{\\beta_2,\\beta_{K-1}} & \\hat{\\sigma}_{\\beta_2,\\beta_K} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\hat{\\sigma}_{\\beta_{K-1},\\beta_1} & \\hat{\\sigma}_{\\beta_{K-1},\\beta_2} & \\cdots & \\hat{\\sigma}_{\\beta_{K-1}}^2 & \\hat{\\sigma}_{\\beta_{K-1},\\beta_K} \\\\\n\\hat{\\sigma}_{\\beta_K,\\beta_1} & \\hat{\\sigma}_{\\beta_K,\\beta_2} & \\cdots & \\hat{\\sigma}_{\\beta_K,\\beta_{K-1}} & \\hat{\\sigma}_{\\beta_K}^2\n\\end{array} \\right].\n\\]\nThe PanelOLS class can be used to estimate OLS models. The following code estimates the OLS coefficients and standard errors:\n\nfrom linearmodels import PanelOLS\n\nmod = PanelOLS.from_formula(\"y ~ 1 + x\", df)\n\nres = mod.fit()\nres.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.2078\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.2208\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1907\n\n\nDate:\nFri, Jan 19 2024\nR-squared (Overall):\n0.2078\n\n\nTime:\n08:23:25\nLog-likelihood\n-1.057e+04\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n1310.7\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4998)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n1310.7\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4998)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.0297\n0.0284\n1.0466\n0.2954\n-0.0259\n0.0853\n\n\nx\n1.0348\n0.0286\n36.204\n0.0000\n0.9788\n1.0909\n\n\n\n\n\n\nNote that because the intercept is usually excluded by default from panel data models because the use of fixed-effects renders it redundant, we need to add it manually to the model by adding 1 to the formula."
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#white-standard-errors",
    "href": "posts/panel-ols-standard-errors/index.html#white-standard-errors",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "White standard errors",
    "text": "White standard errors\nThe OLS estimator standard errors are consistent when the errors are homoskedastic and uncorrelated across entities and time. In this case, the OLS estimator is unbiased and consistent. However, this is not usually the case with financial data, where the errors are often heteroskedastic and correlated across entities and/or time. In this case, the OLS estimator is biased and inconsistent, and the standard errors are incorrect.\nA common solution to this problem is to use heteroskedasticity-robust standard errors. The most common heteroskedasticity-robust standard errors estimator is the White estimator, which is defined as\n\\[\n\\hat{\\Sigma}_{White} = (\\mathbf{X}' \\mathbf{X})^{-1}\\cdot\\left[\\mathbf{X}' \\cdot \\hat{\\Omega}_0  \\cdot \\mathbf{X}\\right] \\cdot\\left(\\mathbf{X}' \\mathbf{X}\\right)^{-1},\n\\]\nwhere \\(\\hat{\\Omega}_0\\) is a diagonal matrix with the squared residuals \\(\\mathbf{\\hat{e}}^{2}\\) on the diagonal, i.e., with the square of the j-th residual \\(\\hat{e}_{j}^{2}\\) at position \\((j,j)\\) and \\(0\\) elsewhere. The matrix \\(\\hat{\\Omega}_0\\) is defined as:\n\\[\n\\hat{\\Omega_0}= \\left[\\begin{array}{cccccccc}\ne_{1, 1}^{2} & 0 & \\cdots & 0 & 0& \\cdots & 0 & 0 \\\\\n0 & e_{1, 2}^{2} & \\cdots & 0 & 0 & \\cdots & 0 & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n  0 & 0& \\cdots &e_{2, 1}^{2} & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 &  \\cdots & 0 & e_{2, 2}^{2} & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n0 & 0 & \\cdots & 0 & 0 & \\cdots & e_{I,T-1}^{2} & 0 \\\\\n0 & 0 & \\cdots & 0 & 0 & \\cdots & 0 & e_{I,T}^{2}\n\\end{array} \\right].\n\\]\nThe PanelOLS class can be used to estimate OLS models with White standard errors. The following code estimates the OLS coefficients and White standard errors:\n\nres = mod.fit(cov_type=\"robust\")\nres.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.2078\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.2208\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1907\n\n\nDate:\nFri, Jan 19 2024\nR-squared (Overall):\n0.2078\n\n\nTime:\n08:23:25\nLog-likelihood\n-1.057e+04\n\n\nCov. Estimator:\nRobust\n\n\n\n\n\n\nF-statistic:\n1310.7\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4998)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n1328.2\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4998)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.0297\n0.0284\n1.0465\n0.2954\n-0.0259\n0.0853\n\n\nx\n1.0348\n0.0284\n36.444\n0.0000\n0.9792\n1.0905\n\n\n\n\n\n\nThe estimated coefficients are the same as with the OLS standard errors, but the standard errors are different. In turn, all statistical measures that depend on the standard errors, such as t-statistics and p-values, are different."
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#clustered-standard-errors",
    "href": "posts/panel-ols-standard-errors/index.html#clustered-standard-errors",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Clustered standard errors",
    "text": "Clustered standard errors\nBeyond homoskedasticity, a common concern with panel models is that the errors are correlated across entities and/or time. In this case, the OLS estimator is biased and inconsistent, and the standard errors are incorrect. Clustering standard errors is a widely used adjustment for this problem.\nThe intuition behind clustering is that often the errors are correlated within groups, but not across groups. For example, in a panel data set with firms, the errors could correlated within the observations of a firm, but not across firms. In this case, we can cluster the standard errors by firm to obtain consistent standard errors. Implicitly we assume that the errors have the following structure:1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirm 1\n\n\nFirm 2\n\n\nFirm 3\n\n\n\n\n\nFirm 1\n\\(\\epsilon_{11}^2\\)\n\\(\\epsilon_{11}\\epsilon_{12}\\)\n\\(\\epsilon_{11}\\epsilon_{13}\\)\n0\n0\n0\n0\n0\n0\n\n\n\n\\(\\epsilon_{12}\\epsilon_{11}\\)\n\\(\\epsilon_{12}^2\\)\n\\(\\epsilon_{12}\\epsilon_{13}\\)\n0\n0\n0\n0\n0\n0\n\n\n\n\\(\\epsilon_{13}\\epsilon_{11}\\)\n\\(\\epsilon_{13}\\epsilon_{12}\\)\n\\(\\epsilon_{13}^2\\)\n0\n0\n0\n0\n0\n0\n\n\nFirm 2\n0\n0\n0\n\\(\\epsilon_{21}^2\\)\n\\(\\epsilon_{21}\\epsilon_{22}\\)\n\\(\\epsilon_{21}\\epsilon_{23}\\)\n0\n0\n0\n\n\n\n0\n0\n0\n\\(\\epsilon_{22}\\epsilon_{21}\\)\n\\(\\epsilon_{22}^2\\)\n\\(\\epsilon_{22}\\epsilon_{23}\\)\n0\n0\n0\n\n\n\n0\n0\n0\n\\(\\epsilon_{23}\\epsilon_{21}\\)\n\\(\\epsilon_{23}\\epsilon_{22}\\)\n\\(\\epsilon_{23}^2\\)\n0\n0\n0\n\n\nFirm 3\n0\n0\n0\n0\n0\n0\n\\(\\epsilon_{31}^2\\)\n\\(\\epsilon_{31}\\epsilon_{32}\\)\n\\(\\epsilon_{31}\\epsilon_{33}\\)\n\n\n\n0\n0\n0\n0\n0\n0\n\\(\\epsilon_{32}\\epsilon_{31}\\)\n\\(\\epsilon_{32}^2\\)\n\\(\\epsilon_{32}\\epsilon_{33}\\)\n\n\n\n0\n0\n0\n0\n0\n0\n\\(\\epsilon_{33}\\epsilon_{31}\\)\n\\(\\epsilon_{33}\\epsilon_{32}\\)\n\\(\\epsilon_{33}^2\\)\n\n\n\nIf you assume that the errors are correlated within the observations of a firm, and across firms within each time period, you can use standard errors clustered by firm and year. In this case, the errors have the following structure:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirm 1\n\n\nFirm 2\n\n\nFirm 3\n\n\n\n\n\nFirm 1\n\\(\\epsilon_{11}^2\\)\n\\(\\epsilon_{11}\\epsilon_{12}\\)\n\\(\\epsilon_{11}\\epsilon_{13}\\)\n\\(\\epsilon_{11}\\epsilon_{21}\\)\n0\n0\n\\(\\epsilon_{11}\\epsilon_{31}\\)\n0\n0\n\n\n\n\\(\\epsilon_{12}\\epsilon_{11}\\)\n\\(\\epsilon_{12}^2\\)\n\\(\\epsilon_{12}\\epsilon_{13}\\)\n0\n\\(\\epsilon_{12}\\epsilon_{22}\\)\n0\n0\n\\(\\epsilon_{12}\\epsilon_{32}\\)\n0\n\n\n\n\\(\\epsilon_{13}\\epsilon_{11}\\)\n\\(\\epsilon_{13}\\epsilon_{12}\\)\n\\(\\epsilon_{13}^2\\)\n0\n0\n\\(\\epsilon_{13}\\epsilon_{23}\\)\n0\n0\n\\(\\epsilon_{13}\\epsilon_{33}\\)\n\n\nFirm 2\n\\(\\epsilon_{21}\\epsilon_{11}\\)\n0\n0\n\\(\\epsilon_{21}^2\\)\n\\(\\epsilon_{21}\\epsilon_{22}\\)\n\\(\\epsilon_{21}\\epsilon_{23}\\)\n\\(\\epsilon_{21}\\epsilon_{31}\\)\n0\n0\n\n\n\n0\n\\(\\epsilon_{22}\\epsilon_{12}\\)\n0\n\\(\\epsilon_{22}\\epsilon_{21}\\)\n\\(\\epsilon_{22}^2\\)\n\\(\\epsilon_{22}\\epsilon_{23}\\)\n0\n\\(\\epsilon_{22}\\epsilon_{32}\\)\n0\n\n\n\n0\n0\n\\(\\epsilon_{23}\\epsilon_{13}\\)\n\\(\\epsilon_{23}\\epsilon_{21}\\)\n\\(\\epsilon_{23}\\epsilon_{22}\\)\n\\(\\epsilon_{23}^2\\)\n0\n0\n\\(\\epsilon_{23}\\epsilon_{33}\\)\n\n\nFirm 3\n\\(\\epsilon_{31}\\epsilon_{11}\\)\n0\n0\n\\(\\epsilon_{31}\\epsilon_{21}\\)\n0\n0\n\\(\\epsilon_{31}^2\\)\n\\(\\epsilon_{31}\\epsilon_{32}\\)\n\\(\\epsilon_{31}\\epsilon_{33}\\)\n\n\n\n0\n\\(\\epsilon_{32}\\epsilon_{12}\\)\n0\n0\n\\(\\epsilon_{32}\\epsilon_{22}\\)\n0\n\\(\\epsilon_{32}\\epsilon_{31}\\)\n\\(\\epsilon_{32}^2\\)\n\\(\\epsilon_{32}\\epsilon_{33}\\)\n\n\n\n0\n0\n\\(\\epsilon_{33}\\epsilon_{13}\\)\n0\n0\n\\(\\epsilon_{33}\\epsilon_{23}\\)\n\\(\\epsilon_{33}\\epsilon_{31}\\)\n\\(\\epsilon_{33}\\epsilon_{32}\\)\n\\(\\epsilon_{33}^2\\)\n\n\n\nYou can easily estimate clustered standard errors with the PanelOLS class by passing cov_type=\"clustered\" and the entity_effects and time_effects arguments to fit() method. The following code estimates the OLS coefficients and standard errors clustered by firm, year, and firm and year:\n\n# Firm only\nres = mod.fit(\n    cov_type=\"clustered\", cluster_entity=True, cluster_time=False, group_debias=True\n)\nres.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.2078\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.2208\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1907\n\n\nDate:\nFri, Jan 19 2024\nR-squared (Overall):\n0.2078\n\n\nTime:\n08:23:25\nLog-likelihood\n-1.057e+04\n\n\nCov. Estimator:\nClustered\n\n\n\n\n\n\nF-statistic:\n1310.7\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4998)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n418.32\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4998)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.0297\n0.0670\n0.4429\n0.6579\n-0.1017\n0.1611\n\n\nx\n1.0348\n0.0506\n20.453\n0.0000\n0.9356\n1.1340\n\n\n\n\n\n\n\n# Year only\nres = mod.fit(\n    cov_type=\"clustered\", cluster_entity=False, cluster_time=True, group_debias=True\n)\nres.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.2078\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.2208\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1907\n\n\nDate:\nFri, Jan 19 2024\nR-squared (Overall):\n0.2078\n\n\nTime:\n08:23:25\nLog-likelihood\n-1.057e+04\n\n\nCov. Estimator:\nClustered\n\n\n\n\n\n\nF-statistic:\n1310.7\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4998)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n960.59\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4998)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.0297\n0.0234\n1.2691\n0.2045\n-0.0162\n0.0755\n\n\nx\n1.0348\n0.0334\n30.993\n0.0000\n0.9694\n1.1003\n\n\n\n\n\n\n\n# Firm and year\nres = mod.fit(\n    cov_type=\"clustered\", cluster_entity=True, cluster_time=True, group_debias=True\n)\nres.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.2078\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.2208\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1907\n\n\nDate:\nFri, Jan 19 2024\nR-squared (Overall):\n0.2078\n\n\nTime:\n08:23:25\nLog-likelihood\n-1.057e+04\n\n\nCov. Estimator:\nClustered\n\n\n\n\n\n\nF-statistic:\n1310.7\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4998)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n373.33\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4998)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.0297\n0.0651\n0.4562\n0.6483\n-0.0979\n0.1572\n\n\nx\n1.0348\n0.0536\n19.322\n0.0000\n0.9298\n1.1398\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDifference with Stata and statsmodels\n\n\n\nBy default, Stata and statsmodels estimators adjust the degrees of freedom when estimating clustered standard errors with small clusters. linearmodels does not do that by default, that is what the group_debias parameter is for. In this case it will make a difference because our time dimension is small.\nNote that there is a corresponding debiased parameter for the OLS and White estimator, but it is set to True by default so we do not need to set it explicitly."
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#panel-regression-with-fixed-effects",
    "href": "posts/panel-ols-standard-errors/index.html#panel-regression-with-fixed-effects",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Panel regression with fixed effects",
    "text": "Panel regression with fixed effects\nIt is common in finance research to control for entity fixed effects to capture unobserved heterogeneity across entities and for time fixed effects to capture unobserved heterogeneity across time. For example, in a panel of firms, we can control for firm fixed effects to capture unobserved firm-specific characteristics that are constant over time. These fixed effects correspond to the \\(\\alpha_i\\) and \\(\\alpha_t\\) in our panel regression:\n\\[\ny_{it} =  \\alpha_i + \\alpha_t + x_{it}\\beta + \\epsilon_{it},\n\\]\nThe PanelOLS class can be used to estimate panel regression with fixed effects by adding EntityEffects or TimeEffects to the formula. In theory this is equivalent to adding a dummy variable for each entity or time period (i.e.¬†adding C(firmid) or C(year)), but in practice it is much more efficient because linear models will use a efficient transformation to apply the effetc of the dummy variables without including them in the regression estimation. linearmodels will thus not provide coefficients for the fixed effects. Also, the intercept is usually excluded by default from panel data models because the use of fixed-effects renders it redundant, so linearmodels will not provide an intercept either.\nThe following code estimates the OLS coefficients and standard errors with firm fixed effects, year fixed effects, and firm and year fixed effects, and clustered standard errors along the same dimensions:\n\n# Firm only\nmod_ffe = PanelOLS.from_formula(\"y ~ x + EntityEffects\", df)\nres = mod_ffe.fit(\n    cov_type=\"clustered\", cluster_entity=True, cluster_time=False, group_debias=True\n)\nres.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.1916\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.2187\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1916\n\n\nDate:\nFri, Jan 19 2024\nR-squared (Overall):\n0.2070\n\n\nTime:\n08:23:25\nLog-likelihood\n-8532.8\n\n\nCov. Estimator:\nClustered\n\n\n\n\n\n\nF-statistic:\n1066.3\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4499)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n1035.4\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4499)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nx\n0.9699\n0.0301\n32.177\n0.0000\n0.9108\n1.0290\n\n\n\nF-test for Poolability: 11.372P-value: 0.0000Distribution: F(499,4499)Included effects: Entity\n\n\n\n# Year only\nmod_yfe = PanelOLS.from_formula(\"y ~ x + TimeEffects\", df)\nres = mod_yfe.fit(\n    cov_type=\"clustered\", cluster_entity=False, cluster_time=True, group_debias=True\n)\nres.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.2077\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.2208\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1907\n\n\nDate:\nFri, Jan 19 2024\nR-squared (Overall):\n0.2078\n\n\nTime:\n08:23:25\nLog-likelihood\n-1.057e+04\n\n\nCov. Estimator:\nClustered\n\n\n\n\n\n\nF-statistic:\n1307.5\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4989)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n961.52\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4989)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nx\n1.0351\n0.0334\n31.008\n0.0000\n0.9696\n1.1005\n\n\n\nF-test for Poolability: 0.6799P-value: 0.7279Distribution: F(9,4989)Included effects: Time\n\n\n\n# Firm and year\nmod_fyfe = PanelOLS.from_formula(\"y ~ x + EntityEffects + TimeEffects\", df)\nres = mod_fyfe.fit(\n    cov_type=\"clustered\", cluster_entity=True, cluster_time=True, group_debias=True\n)\nres.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.1913\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.2187\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1916\n\n\nDate:\nFri, Jan 19 2024\nR-squared (Overall):\n0.2070\n\n\nTime:\n08:23:25\nLog-likelihood\n-8525.9\n\n\nCov. Estimator:\nClustered\n\n\n\n\n\n\nF-statistic:\n1062.0\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4490)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n972.96\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4490)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nx\n0.9700\n0.0311\n31.192\n0.0000\n0.9091\n1.0310\n\n\n\nF-test for Poolability: 11.203P-value: 0.0000Distribution: F(508,4490)Included effects: Entity, Time"
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#driscoll-kraay-standard-errors",
    "href": "posts/panel-ols-standard-errors/index.html#driscoll-kraay-standard-errors",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Driscoll-Kraay standard errors",
    "text": "Driscoll-Kraay standard errors\nThe Driscoll-Kraay standard errors estimator is a heteroskedasticity-robust standard errors estimator that is robust to serial correlation in the errors. This is particularly important in datasets where the assumption that observations are independent across cross-sections may not hold. See Driscoll and Kraay (1998) for more details.\nTo use the Driscoll-Kraay standard errors estimator, you need to pass the following arguments to the fit() method:\n\ncov_type=\"kernel\": To use the Driscoll-Kraay HAC estimator.\nkernel=\"bartlett\": The kernel to use. Bartlett is the kernel used by the Newey-West estimator. linearmodels also supports the Parzen (parzen) and the Quadratic Spectral (qs) kernels.\nbandwith=3: the number of lags to use, automatically computed if not specified.\n\n\nres = mod.fit(cov_type=\"kernel\", kernel=\"bartlett\", bandwidth=3)\nres.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.2078\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.2208\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1907\n\n\nDate:\nSat, Jan 20 2024\nR-squared (Overall):\n0.2078\n\n\nTime:\n07:01:02\nLog-likelihood\n-1.057e+04\n\n\nCov. Estimator:\nDriscoll-Kraay\n\n\n\n\n\n\nF-statistic:\n1310.7\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4998)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n1708.6\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4998)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.0297\n0.0218\n1.3622\n0.1732\n-0.0130\n0.0724\n\n\nx\n1.0348\n0.0250\n41.335\n0.0000\n0.9858\n1.0839"
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#fama-macbeth-coefficients-and-standard-errors",
    "href": "posts/panel-ols-standard-errors/index.html#fama-macbeth-coefficients-and-standard-errors",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Fama-MacBeth coefficients and standard errors",
    "text": "Fama-MacBeth coefficients and standard errors\nThe Fama-MacBeth regression, introduced in Fama and MacBeth (1973), is a robust method for estimating financial models in the context of panel data in empirical finance research that is effective in dealing with cross-sectional correlation and heteroskedasticity. This technique involves two key steps. For our model, we would first estimate the following cross-sectional regression for each time period \\(t\\):\n\\[\ny_{it} =  \\alpha_t + x_{it}\\beta_t + \\epsilon_{it},\n\\]\nand then average the coefficients \\(\\beta_t\\) over all time periods to provide final estimates:\n\\[\n\\bar{\\beta} = \\frac{1}{T} \\sum_{t=1}^{T} \\beta_t.\n\\]\nThe covariance matrix can then be computed as:\n\\[\n\\hat{\\Sigma}_{FM} = \\frac{1}{T-1} \\sum_{t=1}^{T} (\\beta_t - \\bar{\\beta})(\\beta_t - \\bar{\\beta})'.\n\\]\nlinearmodels provides a FamaMacBeth class that can be used to estimate Fama-MacBeth regressions. The following code estimates the Fama-MacBeth coefficients and standard errors:\n\nfrom linearmodels import FamaMacBeth\n\nmod_fm = FamaMacBeth.from_formula(\"y ~ 1 + x\", df)\nres = mod_fm.fit()\nres.summary\n\n\nFamaMacBeth Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.2078\n\n\nEstimator:\nFamaMacBeth\nR-squared (Between):\n0.2208\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1907\n\n\nDate:\nSat, Jan 20 2024\nR-squared (Overall):\n0.2078\n\n\nTime:\n06:09:49\nLog-likelihood\n-1.057e+04\n\n\nCov. Estimator:\nFama-MacBeth Standard Cov\n\n\n\n\n\n\nF-statistic:\n1310.7\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4998)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n964.72\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4998)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.0313\n0.0234\n1.3392\n0.1806\n-0.0145\n0.0771\n\n\nx\n1.0356\n0.0333\n31.060\n0.0000\n0.9702\n1.1010"
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#newey-west-standard-errors",
    "href": "posts/panel-ols-standard-errors/index.html#newey-west-standard-errors",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Newey-West standard errors",
    "text": "Newey-West standard errors\nIt is common in practice to use Newey-West standard errors (introduced in Newey and West 1987) instead of the standard errors above in Fama-MacBeth regressions to account for heteroskedasticity and autocorrelation in the residuals. For more details on the Newey-West estimator, see my post on the topic.\nTo use Newey-West standard errors, you can pass the following arguments to the fit() method:\n\ncov_type=\"kernel\": To use HAC standard errors. See the documentation on Kernel (HAC) for more details.\nkernel=\"bartlett\": The Newey-West estimator uses the Bartlett kernel. linearmodels also supports the Parzen (parzen) and the Quadratic Spectral (qs) kernels.\nbandwith=3: the number of lags to use, automatically computed if not specified.\n\n\nfm_res = fm_mod.fit(cov_type=\"kernel\", kernel=\"bartlett\", bandwidth=3)\nfm_res.summary\n\n\nFamaMacBeth Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.2078\n\n\nEstimator:\nFamaMacBeth\nR-squared (Between):\n0.2208\n\n\nNo. Observations:\n5000\nR-squared (Within):\n0.1907\n\n\nDate:\nSat, Jan 20 2024\nR-squared (Overall):\n0.2078\n\n\nTime:\n06:34:46\nLog-likelihood\n-1.057e+04\n\n\nCov. Estimator:\nFama-MacBeth Kernel Cov\n\n\n\n\n\n\nF-statistic:\n1310.7\n\n\nEntities:\n500\nP-value\n0.0000\n\n\nAvg Obs:\n10.0000\nDistribution:\nF(1,4998)\n\n\nMin Obs:\n10.0000\n\n\n\n\nMax Obs:\n10.0000\nF-statistic (robust):\n1440.8\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n10\nDistribution:\nF(1,4998)\n\n\nAvg Obs:\n500.00\n\n\n\n\nMin Obs:\n500.00\n\n\n\n\nMax Obs:\n500.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.0313\n0.0224\n1.3934\n0.1636\n-0.0127\n0.0753\n\n\nx\n1.0356\n0.0273\n37.958\n0.0000\n0.9821\n1.0891"
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#cheat-sheet",
    "href": "posts/panel-ols-standard-errors/index.html#cheat-sheet",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Cheat sheet",
    "text": "Cheat sheet\nThe following table summarizes the standard errors estimators available in linearmodels and their corresponding methods:\nfrom linearmodels import PanelOLS\n\n# Model with constant\nmod_cst = PanelOLS.from_formula(\"y ~ 1 + x1 + x2\", data)\n# Non-robust errors (regular OLS)\nres = mod_cst.fit()\n# White standard errors (heteroskedasticity-robust)\nres = mod_cst.fit(cov_type=\"robust\")\n\n# Clustered standard errors\n# By entity (firm)\nres = mod_cst.fit(\n    cov_type=\"clustered\", cluster_entity=True, cluster_time=False, group_debias=True\n)\n# By time (year)\nres = mod_cst.fit(\n    cov_type=\"clustered\", cluster_entity=False, cluster_time=True, group_debias=True\n)\n# By entity and time\nres = mod_cst.fit(\n    cov_type=\"clustered\", cluster_entity=True, cluster_time=True, group_debias=True\n)\n\n# Fixed effects models\n# Model with entity (firm) fixed effects\nmod_fe = PanelOLS.from_formula(\"y ~ x1 + x2 + EntityEffects\", data)\n# Model with time (year) fixed effects\nmod_fe = PanelOLS.from_formula(\"y ~ x1 + x2 + TimeEffects\", data)\n# Model with entity and time fixed effects\nmod_fe = PanelOLS.from_formula(\"y ~ x1 + x2 + EntityEffects+ TimeEffects\", data)\n\n# To cluster standard errors by entity and time, the usage is the same as above,\n# but applied to the FE model.\n\n# By entity (firm)\nres = mod_fe.fit(\n    cov_type=\"clustered\", cluster_entity=True, cluster_time=False, group_debias=True\n)\n# By time (year)\nres = mod_fe.fit(\n    cov_type=\"clustered\", cluster_entity=False, cluster_time=True, group_debias=True\n)\n# By entity and time\nres = mod_fe.fit(\n    cov_type=\"clustered\", cluster_entity=True, cluster_time=True, group_debias=True\n)\n\n# Driscoll-Kraay standard errors\nres = mod_cst.fit(cov_type=\"kernel\", kernel=\"bartlett\", bandwidth=3)\n\n# Fama-MacBeth regressions\nfrom linearmodels import FamaMacBeth\n\nmod_fm = FamaMacBeth.from_formula(\"y ~ 1 + x1 + x2\", data)\n# Non-adjusted standard errors\nres = mod_fm.fit()\n# Newey-West standard errors with 3 lags\nres = mod_fm.fit(cov_type=\"kernel\", kernel=\"bartlett\", bandwidth=3)"
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#learn-more",
    "href": "posts/panel-ols-standard-errors/index.html#learn-more",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Learn more",
    "text": "Learn more\nTo learn more about the linearmodels library, I recommend reading the documentation, especially the section on panel data model estimation.\nUsing a powerful tool like linearmodels can help you save time and avoid errors when estimating panel data models. However, it is important to understand the underlying econometric theory. If you are interested in learning more about panel data models and their associated standard error, I recommend reading the Petersen (2009) paper or watching the talk he gave at the 2008 FMA Annual meetings."
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#references",
    "href": "posts/panel-ols-standard-errors/index.html#references",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "References",
    "text": "References\n\n\nDriscoll, John C, and Aart C Kraay. 1998. ‚ÄúConsistent Covariance Matrix Estimation with Spatially Dependent Panel Data.‚Äù Review of Economics and Statistics 80 (4): 549‚Äì60. https://www.mitpressjournals.org/doi/abs/10.1162/003465398557825.\n\n\nFama, Eugene F, and James D MacBeth. 1973. ‚ÄúRisk, Return, and Equilibrium: Empirical Tests.‚Äù Journal of Political Economy 81 (3): 607‚Äì36. https://www.journals.uchicago.edu/doi/abs/10.1086/260061.\n\n\nNewey, Whitney K, and Kenneth D West. 1987. ‚ÄúA Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.‚Äù Econometrica 55 (3): 703‚Äì8. https://www.jstor.org/stable/1913610.\n\n\nPetersen, Mitchell A. 2009. ‚ÄúEstimating Standard Errors in Finance Panel Data Sets: Comparing Approaches.‚Äù The Review of Financial Studies 22 (1): 435‚Äì80. https://academic.oup.com/rfs/article-abstract/22/1/435/1585940."
  },
  {
    "objectID": "posts/panel-ols-standard-errors/index.html#footnotes",
    "href": "posts/panel-ols-standard-errors/index.html#footnotes",
    "title": "Estimating standard errors in panel data with Python and linearmodels",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCredit: the two tables are taken from Petersen (2009).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/python-basics-tutorial/index.html",
    "href": "posts/python-basics-tutorial/index.html",
    "title": "Python basic tutorial",
    "section": "",
    "text": "I posted a tutorial on the basics of the Python syntax. This tutorial has been updated for Python 3.12, and covers the basics of the Python syntax. It is intended for beginners and is a good starting point for those who want to learn Python. It does not cover the whole syntax, only the elements that I find essential to get started with data analysis and empirical research using Python. After the tutorial, you should have enough knowledge to start learning the modules that are relevant to data analysis such as pandas, numpy, matplotlib, etc.\nI am in the process of recording a video tutorial that will be posted on my YouTube channel. I will post the link to the video here once it is ready.\nIf you haven‚Äôt done so already, I recommend you to install Python on your computer. You can find instructions on how to do so here.\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "posts/ollama-copilot/index.html",
    "href": "posts/ollama-copilot/index.html",
    "title": "Using Ollama with Continue as an AI-Powered Coding and Writing Assistant",
    "section": "",
    "text": "GitHub Copilot is an amazing AI-assistant that helps developers write code faster and with fewer errors. However, it does have some limitations, such as the need for an internet connection‚Äìnot ideal for airplane mode‚Äìand it sends your code to cloud servers for processing. In this post, I will show how you can use Continue, a Visual Studio Code extension that provides an AI-assistant, with Ollama, a tool to manage and run local LLMs, to get similar functionality as GitHub Copilot without the need for an internet connection or sending your code to the cloud. Your code stays on your machine, and best of all it is all free.\nThis setup allows you to have a private, local AI-powered coding and writing assistant that runs on your machine and doesn‚Äôt require an internet connection. Indeed, this works not only for coding but also for writing if you use Visual Studio Code for writing in LaTeX or Markdown, for example."
  },
  {
    "objectID": "posts/ollama-copilot/index.html#video-tutorials",
    "href": "posts/ollama-copilot/index.html#video-tutorials",
    "title": "Using Ollama with Continue as an AI-Powered Coding and Writing Assistant",
    "section": "Video tutorials",
    "text": "Video tutorials\nThe content of this post is also available as two video tutorials on YouTube."
  },
  {
    "objectID": "posts/ollama-copilot/index.html#ollama",
    "href": "posts/ollama-copilot/index.html#ollama",
    "title": "Using Ollama with Continue as an AI-Powered Coding and Writing Assistant",
    "section": "Ollama",
    "text": "Ollama\nOllama is a tool to manage and run local LLMs, such as Meta‚Äôs Llama3. I discussed how to use Ollama as a private, local ChatGPT replacement in a previous post. Ollama can also be used to run LLMs for code completion, such as Codellama, which is a model trained on code.\n\nInstall Ollama\nThe first step in setting up Ollama is to download and install the tool on your local machine. The installation process is straightforward and involves running a few commands in your terminal. Ollama‚Äôs download page provides installers for macOS and Windows, as well as instructions for Linux users. Once you‚Äôve downloaded the installer, follow the installation instructions to set up Ollama on your machine.\nIf you‚Äôre using a Mac, you can install Ollama using Homebrew by running the following command in your terminal:\nbrew install ollama\nThe benefit of using Homebrew is that it simplifies the installation process and also sets up Ollama as a service, allowing it to run in the background and manage the LLM models you download.\nAt the moment, the most popular code models on Ollama are:\n\ncodellama\ndeepseek-coder\nwizardcoder\nstarcoder2\ncodegemma\n\nAfter installing Ollama, you can install a model from the command line using the pull command:\nollama pull codellama\nWhile you can run the model from the command line, it is more convenient to use it with Continue, a Visual Studio Code extension that provides an AI-assistant directly in your editor."
  },
  {
    "objectID": "posts/ollama-copilot/index.html#continue",
    "href": "posts/ollama-copilot/index.html#continue",
    "title": "Using Ollama with Continue as an AI-Powered Coding and Writing Assistant",
    "section": "Continue",
    "text": "Continue\nContinue is an open-source autopilot for developers that seamlessly integrates with popular IDEs like VS Code and JetBrains. It offers features such as task and tab autocomplete, allowing it to generate, refactor, and explain entire sections of code efficiently. With Continue, developers can easily highlight sections of code to get another perspective on their work, ask specific coding questions, instruct the tool to refactor code in natural language, or even generate new files from scratch across various programming languages and frameworks. This makes coding more intuitive and significantly reduces the need for manual lookups or switching between windows for assistance.\nOne of the most compelling aspects of Continue is its flexibility and integration capabilities. It allows developers to choose from a wide array of large language models (LLMs) and providers, whether deploying locally or in the cloud. Continue ensures that your codebase serves as context for queries, enhancing the relevance and accuracy of its assistance. Its local-first, modular design promotes offline use and privacy, making it a great alternative to cloud-based solutions like GitHub Copilot.\n\nInstalling Continue in Visual Studio Code\nTo use Continue, you need to install the Visual Studio Code extension. You can find it in the Visual Studio Code Marketplace. Once installed, the Continue icon will appear in your left sidebar. For better accessibility, the Continue developers recommended placing it on the right-hand side. This lets you keep the assistant open while accessing other features in the left sidebar.\n\n\n\nContinue Extension Installed\n\n\nOnce installed, you can configure it to use Ollama as a provider for code completion in your Continue config file, which you can access by clicking on the gear icon in the Continue sidebar.\n\n\n\nAccess Continue Settings\n\n\n~/.continue/config.json:\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama3\"\n    },\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"AUTODETECT\"\n    }\n  ],\n...\n}\nIf you have multiple models installed, you can add them similarly. This allows you to switch between models for different tasks, providing flexibility in your workflow."
  },
  {
    "objectID": "posts/ollama-copilot/index.html#using-continue-as-a-coding-assistant",
    "href": "posts/ollama-copilot/index.html#using-continue-as-a-coding-assistant",
    "title": "Using Ollama with Continue as an AI-Powered Coding and Writing Assistant",
    "section": "Using Continue as a coding assistant",
    "text": "Using Continue as a coding assistant\nContinue offers several features that can help you write code more efficiently, including understanding code, refactoring code, and debugging code. There are two main ways to interact with Continue: through the chat interface and with inline commands.\n\nChat interface\nTo interact with the chat, you can highlight a piece of code and press command-L on Mac or control-L on Windows and Linux to paste it into the chat. You can then ask questions or give instructions to the assistant, and it will provide suggestions based on the context of the code.\n\n\n\nSelecting Text\n\n\nYou can then tell Continue what you want it to do with the code, such as explain it, refactoring it, or debugging it. Continue will analyze the code and provide suggestions based on the task you specified. You can accept, reject, or retry the suggestions to refine the code further.\n\n\n\nChat Example\n\n\nIn the chat interface, you can also use the @ symbol to mention a specific context, such as your project code, the terminal, or the error panel. This helps Continue understand the context of your query and provide more relevant suggestions.\n\n\n\nChat Context List\n\n\nYou can also use the / symbol to access slash commands which are convenient pre-defined commands that you can use to interact with the assistant. For example, you can use /comment to ask Continue to write comments the code, or /cmd to ask it to write a shell command. We will see later how to define your own custom commands.\n\n\n\nChat Slash Commands\n\n\n\n\nInline commands\nIn addition to the chat interface, Continue also supports inline commands that allow you to interact with the assistant directly in your code editor. You can use these commands to perform specific tasks, such as understanding code, refactoring code, or debugging code, without having to switch to the chat interface. To use it, just highlight the code you want to work on and press command-I on Mac or control-I on Windows and Linux. It will open a prompt where you can specify the task you want to perform, and Continue will provide suggestions based on the context of the code. You will then see the proposed changes, and you can accept, reject, or retry them as needed.\n\n\n\nInline Code Example\n\n\n\n\nContextual menu options\nContinue also offers several contextual options: - Add to Context: Add code to the chat bar for further assistance. (equivalent to command-L) - Fix/Optimize: Automatically fix or optimize the code. - Write Docstring: Generate documentation for your functions.\n\n\n\nContextual Menu Options\n\n\nBy integrating Continue with Ollama, you can significantly enhance your coding efficiency. This setup allows you to leverage powerful LLMs locally, making your development process smoother and more cost-effective.\n\n\nAuto-complete\nContinue also supports auto-complete functionality, allowing you to quickly generate code snippets and complete statements as you type. This feature is currently in beta, and I have had mixed results with it. However, it is a promising feature that will likely improve over time.\nThis feature is particularly useful for speeding up your coding workflow and reducing the time spent on repetitive tasks. The auto-complete feature in Continue is customizable, allowing you to select a different model than for the chat features. This is especially useful because the use case is different and latency is more critical, so you will likely want to use a smaller model optimized for completion instead of a powerful chat model like Llama3.\nYou can enable or disable auto-complete for specific languages, configure the trigger keys, and adjust the completion delay to fine-tune the behavior of the assistant.\n\n\nCustom commands\nContinue also lets you define custom slash commands that can be used to interact with the AI assistant. This is particularly useful for running specific queries or commands that are not covered by the default functionality. You can define custom commands in the Continue config file and then execute them directly from the editor. This feature allows you to tailor the AI assistant to your specific needs and workflow, making it even more powerful and versatile. In the next section, we‚Äôll explore how to define custom commands in Continue and use them to use Continue as a writing assistant."
  },
  {
    "objectID": "posts/ollama-copilot/index.html#using-continue-as-a-writing-assistant",
    "href": "posts/ollama-copilot/index.html#using-continue-as-a-writing-assistant",
    "title": "Using Ollama with Continue as an AI-Powered Coding and Writing Assistant",
    "section": "Using Continue as a writing assistant",
    "text": "Using Continue as a writing assistant\nSince I use Visual Studio Code not only for coding but also for writing in LaTeX and Markdown, I wanted to explore how Continue could be used as a writing assistant. While Continue is primarily designed for coding, it can also be used for writing tasks, such as generating text, writing summaries, or providing suggestions for improving your writing. By defining custom commands in the Continue config file, you can tailor the assistant to your specific writing needs and use it to enhance your writing process. Here are the five custom commands I defined specifically to turn Continue into a powerful writing assistant. These should be added to the customCommands section of your config file, and they will appear in the chat menu. You can find my full Continue config file at the end of this post.\n\n\n\nCustom Slash Commands\n\n\n{\n  \"name\": \"french\",\n  \"description\": \"Translate selected text to French\",\n  \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Translate selected text to French. Return the input text free from any typo or grammatical error. Return only the text. Do not include a preamble.\"\n}\n\nSpell checking\nThe first custom command I created is spell. This command checks the spelling and grammar of the selected text. Here‚Äôs the configuration:\n{\n  \"name\": \"spell\",\n  \"description\": \"Check spelling and grammar for the selected text\",\n  \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Check the selected text for typos and grammatical errors. Return the input text free from any typo or grammatical error. Return only the text. Do not include a preamble.\"\n}\nYou will first need to select the text you want to check, then press Command + L to send it to the sidebar. Next, type /spell and hit enter. The prompt instructs the assistant to check the selected text (which is fed input the {{ input }} tag) for typos and grammatical errors and return the input text free from any errors. I have tried it with Llama 3 with good results, although it might sometimes include a preamble despite the instruction, this setup generally works well.\nYou can then use the buttons in the chat interface to copy the corrected text back into your document.\n\n\nImproving text\nWhen writing, it‚Äôs essential to ensure that your text is clear, concise, and free from errors. The improve command helps in making the selected text sound smarter and more professional. Here‚Äôs the configuration, I‚Äôm using, but you should adjust the prompt to your needs:\n{\n  \"name\": \"improve\",\n  \"description\": \"Improve selected text\",\n  \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Improve the selected text by making it sound smart and professional. Avoid using rare words or overselling. Return the input text free from any typo or grammatical error. Return only the text. Do not include a preamble.\"\n}\n\n\nExpanding text\nWhen writing a first draft, I often like to get my ideas down quickly without worrying too much about the details. This is where the expand command comes in handy. It can turn a single paragraph into many, or take a bullet list and turn it into full sentences. It‚Äôs great for overcoming writer‚Äôs block, but you will need to edit the output to make sure it fits your style and context. Here‚Äôs the configuration I‚Äôm using:\n{\n  \"name\": \"expand\",\n  \"description\": \"Expand selected text\",\n  \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Expand selected text as full paragraphs. Make it longer by adding details. Return the input text free from any typo or grammatical error. Return only the text. Do not include a preamble.\"\n}\n\n\nCritiquing text\nGetting feedback on your writing is essential for improving your skills. While I have good friends and colleagues who can provide feedback, it‚Äôs not always convenient to ask them for help until I have a more polished draft. This is where the critique command comes in handy. It provides constructive feedback on a paragraph or larger text sections. Here‚Äôs the configuration I‚Äôm using:\n{\n  \"name\": \"critique\",\n  \"description\": \"Critique selected text\",\n  \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Critique the selected text and suggest ways to improve it.\"\n},\n\n\nTranslating text\nSince I work at a French-language university and teach both in English and French, I often need to translate text to French. I use the french command to translate the selected text into French. Here‚Äôs the configuration:\n{\n  \"name\": \"french\",\n  \"description\": \"Translate selected text to French\",\n  \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Translate selected text to French. Return the input text free from any typo or grammatical error. Return only the text. Do not include a preamble.\"\n}\n\n\nCreating Your own commands\nCreating custom commands in Continue is straightforward, all you need is to: 1. Choose a unique name for the command. 2. Write a clear description. 3. Craft a prompt that directs the LLM effectively.\nYou can use my examples as a starting point and modify them to suit your specific writing needs. Experiment with different prompts to see what works best for your needs!"
  },
  {
    "objectID": "posts/ollama-copilot/index.html#limitations-of-continue",
    "href": "posts/ollama-copilot/index.html#limitations-of-continue",
    "title": "Using Ollama with Continue as an AI-Powered Coding and Writing Assistant",
    "section": "Limitations of Continue",
    "text": "Limitations of Continue\nWhile Continue is a powerful tool that can significantly enhance your coding and writing workflow, it does have some limitations. The most significant for my personal workflow is that it uses a global configuration that applies to all Visual Studio Code profiles. I like to have separate profiles based on the type of work I‚Äôm doing, such as coding in Python, in Rust, or writing, and it would be nice to have different configurations for each profile. You can somewhat overcome this limitation by having workspace-specific configurations, but it‚Äôs not as flexible as having separate profiles.\nOther than that, the auto-complete feature is still in beta and can be hit or miss, depending on the model you‚Äôre using and the language you‚Äôre working with. I‚Äôve had mixed results with it, and it‚Äôs not as reliable as the chat interface or inline commands. However, it‚Äôs a promising feature that will likely improve over time as the technology matures.\nLastly, Continue relies on LLMs to provide suggestions and assistance, so the quality of the suggestions you receive will depend on the model you‚Äôre using. Some models are more accurate and reliable than others, so it‚Äôs essential to experiment with different models to find the one that works best for your needs. Continue provides a wide range of models to choose from, so you can select the one that best fits your workflow and coding style."
  },
  {
    "objectID": "posts/ollama-copilot/index.html#conclusion",
    "href": "posts/ollama-copilot/index.html#conclusion",
    "title": "Using Ollama with Continue as an AI-Powered Coding and Writing Assistant",
    "section": "Conclusion",
    "text": "Conclusion\nFor my part, while I do use Continue on occasion, my daily driver for code completion is still GitHub Copilot, as it provides a more seamless and responsive experience, and I‚Äôm willing to trade off the privacy and control of a local solution for the convenience and reliability of a cloud-based one. However, I‚Äôm glad to have the option to use Ollama and other local LLMs when I need them, and I‚Äôm excited to see how this technology continues to evolve and improve in the future.\nAnd as for writing goes, the custom commands I defined in Continue have proven to be quite useful, and I find myself using them more and more to improve my writing and generate text. I‚Äôm looking forward to exploring more ways to use Continue as a writing assistant and to see how it can help me become a better writer."
  },
  {
    "objectID": "posts/ollama-copilot/index.html#full-continue-config-file",
    "href": "posts/ollama-copilot/index.html#full-continue-config-file",
    "title": "Using Ollama with Continue as an AI-Powered Coding and Writing Assistant",
    "section": "Full Continue config file",
    "text": "Full Continue config file\nHere is my full Continue config file, which includes the models, slash commands, custom commands, context providers, and other settings I use with Ollama. You can use this as a template to configure Continue with your preferred settings and providers. I try to keep it as minimal as possible and will update it as I discover new features or providers that I want to use.\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama3\"\n    },\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"AUTODETECT\"\n    }\n  ],\n  \"slashCommands\": [\n    {\n      \"name\": \"edit\",\n      \"description\": \"Edit selected code\"\n    },\n    {\n      \"name\": \"comment\",\n      \"description\": \"Write comments for the selected code\"\n    },\n    {\n      \"name\": \"share\",\n      \"description\": \"Download and share this session\"\n    },\n    {\n      \"name\": \"cmd\",\n      \"description\": \"Generate a shell command\"\n    }\n  ],\n  \"customCommands\": [\n    {\n      \"name\": \"spell\",\n      \"description\": \"Check spelling and grammar for the selected text\",\n      \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Check the selected text for typos and grammatical errors. Return the input text free from any typo or grammatical error. Return only the text. Do not include a preamble.\"\n    },\n    {\n      \"name\": \"improve\",\n      \"description\": \"Improve selected text\",\n      \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Improve the selected text by making it sound smart and professional. Avoid using rare words or overselling. Return the input text free from any typo or grammatical error. Return only the text. Do not include a preamble.\"\n    },\n    {\n      \"name\": \"expand\",\n      \"description\": \"Expand selected text\",\n      \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Expand selected text as full paragraphs. Make it longer by adding details. Return the input text free from any typo or grammatical error. Return only the text. Do not include a preamble.\"\n    },\n    {\n      \"name\": \"critique\",\n      \"description\": \"Critique selected text\",\n      \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Critique the selected text and suggest ways to improve it.\"\n    },\n    {\n      \"name\": \"french\",\n      \"description\": \"Translate selected text to French\",\n      \"prompt\": \"{{{ input }}}\\n\\n---\\n\\n Translate selected text to French. Return the input text free from any typo or grammatical error. Return only the text. Do not include a preamble.\"\n    }\n\n  ],\n  \"contextProviders\": [\n    {\n      \"name\": \"diff\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"open\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"terminal\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"problems\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"codebase\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"code\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"docs\",\n      \"params\": {}\n    }\n  ],\n  \"allowAnonymousTelemetry\": true,\n  \"tabAutocompleteModel\": {\n    \"title\": \"sc\",\n    \"provider\": \"ollama\",\n    \"model\": \"starcoder2:3b\"\n  },\n  \"embeddingsProvider\": {\n    \"provider\": \"transformers.js\"\n  }\n}"
  },
  {
    "objectID": "posts/sentiment/index.html",
    "href": "posts/sentiment/index.html",
    "title": "Financial news sentiment analysis in Python",
    "section": "",
    "text": "In the dynamic world of finance, understanding market sentiment is crucial for researchers and practitioners alike.\nIn this tutorial,I present three methods for extracting sentiments from financial news using Python. Starting with a simple yet effective dictionary-based approach, we progress to exploring FinBert, a sophisticated BERT-based model tailored for financial texts. Lastly, we examine the capabilities of large language models (LLMs) in sentiment analysis.\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/sentiment/index.html#video-tutorial",
    "href": "posts/sentiment/index.html#video-tutorial",
    "title": "Financial news sentiment analysis in Python",
    "section": "Video tutorial",
    "text": "Video tutorial\nThis post is also available as a video tutorial on YouTube.\n\n\nimport pandas as pd"
  },
  {
    "objectID": "posts/sentiment/index.html#dependencies",
    "href": "posts/sentiment/index.html#dependencies",
    "title": "Financial news sentiment analysis in Python",
    "section": "Dependencies",
    "text": "Dependencies\nTo follow along with this tutorial, you will need the following Python libraries:\nFor managing data:\n\npandas\n\nFor loading the news data:\n\nlangchain\nnewspaper3k\nlxml-html-clean\n\nFor BERT-based sentiment analysis:\n\ntransformers\nsentence-transformers\ntorch\nscipy\n\nFor large language models:\n\nlangchain\ntenacity\n\nOptional (to use additional LLMs those in Ollama):\n\nlangchain-experimental\nopenai\nlangchain-openai"
  },
  {
    "objectID": "posts/sentiment/index.html#financial-news-data",
    "href": "posts/sentiment/index.html#financial-news-data",
    "title": "Financial news sentiment analysis in Python",
    "section": "Financial news data",
    "text": "Financial news data\nFirst of all, let‚Äôs load some financial news data to work with. For that, we will use the langchain library, which provides a simple interface to load news data from various sources. In our case, we will load news articles from Yahoo Finance, and load it into a pandas DataFrame.\n\nfrom langchain_community.document_loaders import NewsURLLoader\n\n\nurls = [\n    \"https://ca.finance.yahoo.com/news/rising-oil-price-doesnt-shake-160556917.html\",\n    \"https://ca.finance.yahoo.com/news/venezuela-detains-two-former-maduro-173140679.html\",\n    \"https://ca.finance.yahoo.com/news/norfolk-southern-agrees-pay-600m-121211343.html\",\n    \"https://ca.finance.yahoo.com/news/boeing-shares-fall-nyt-report-164509069.html\",\n    \"https://ca.finance.yahoo.com/news/restaurants-along-eclipses-path-totality-153650201.html\",\n]\n\n\nloader = NewsURLLoader(urls=urls)\ndata = loader.load()\n\n\ndata\n\n[Document(page_content='TORONTO ‚Äî The price of oil has been on a steady climb all year, but the talk at Canada\\'s biggest oil and gas conference is still focused on spending discipline.\\n\\nIndustry leaders at the Canadian Association of Petroleum Producers conference, held in Toronto this year, have been emphasizing their predictability and focus on returning money to shareholders, rather than talk of growth.\\n\\nSuncor Energy Inc. chief executive Rich Kruger, who was named head of the oil and gas producer last year as it struggled with safety and operational issues, said his goal is to bring clarity and simplicity to the company.\\n\\n\"I want to become consistently and boringly excellent,\" said Kruger. \"I\\'m not a big one for surprise parties.\"\\n\\nADVERTISEMENT\\n\\nKruger has been working to standardize operations and create a steadier production plan, in contrast to some of the more rushed decisions when growth was the answer to all of the industry\\'s questions.\\n\\nThe early development of the Fort Hills oilsands site, for example, saw mine plans that had slope angles too steep, and not enough was done to check for water issues, in what were fairly short-sighted decisions made to feed the processing plant faster, he said.\\n\\n\"If you go back 10-plus years ago, we lived in a world we thought had resource scarcity, oil prices are going be $100 or better, where growth in production volumes was synonymous with growth in value, a different world than we live in today.\"\\n\\nEven with oil up about US$15 per barrel so far this year to US$85, industry leaders at the conference have been emphasizing that they no longer see production growth as so deeply tied to value, and that each added barrel has to be weighed against returning money to shareholders.\\n\\nThe shift is happening as investors worry about long-term demand prospects for fossil fuels as the push to reduce carbon emissions ramps up.\\n\\nHowever, forecasts do show that oil demand is still growing, said BMO analyst Randy Ollenberger.\\n\\nStory continues\\n\\n\"We often hear the narrative that oil demand has peaked, that it\\'s not growing and how that\\'s negative for the space. That\\'s not true, oil demand is actually continuing to grow, and in fact, it\\'s continuing to grow at a pace that\\'s higher than the average over the last 13 years.\"\\n\\nStill, with investors looking for the industry to reliably pump out cash, as much, if not more than they\\'re looking for growth, company leaders are eager to assure they won\\'t be lost in exuberance as prices rise.\\n\\nCenovus Energy Inc. CEO Jon McKenzie said his company is planning restrained and strategic growth, focused on reducing bottlenecks and finishing shelved projects.\\n\\n\"Growth that we‚Äôve kicked off in 2023 is very different than the kind of growth you would have seen 10, 15 years ago. We‚Äôre not talking about greenfield expansion, we‚Äôre not talking about phased expansions.\"\\n\\nSmaller producers were also keen to emphasize that they were no longer growing for growth\\'s sake, including Whitecap Resources Inc. chief executive Grant Fagerheim.\\n\\n\"Managing growth in a very disciplined manner, I think that‚Äôs a mantra that has been introduced to the energy sector, and I‚Äôm proud to be part of it.\"\\n\\nThis report by The Canadian Press was first published April 9, 2024.\\n\\nCompanies in this story: (TSX:SU, TSX:CVE, TSX:WCP)\\n\\nThe Canadian Press\\n\\nNote to readers: This is a corrected story. A previous version incorrectly called Suncor Energy Canada\\'s largest oil and gas producer.', metadata={'title': \"Higher oil doesn't shake industry talk on spending discipline at CAPP conference\", 'link': 'https://ca.finance.yahoo.com/news/rising-oil-price-doesnt-shake-160556917.html', 'authors': ['The Canadian Press'], 'language': 'en', 'description': \"TORONTO ‚Äî The price of oil has been on a steady climb all year, but the talk at Canada's biggest oil and gas conference is still focused on spending discipline. Industry leaders at the Canadian Association of Petroleum Producers conference, held in Toronto this year, have been emphasizing their predictability and focus on returning money to shareholders, rather than talk of growth. Suncor Energy Inc. chief executive Rich Kruger, who was named head of the oil and gas producer last year as it stru\", 'publish_date': None}),\n Document(page_content='(Bloomberg) -- Venezuela detained former oil and finance ministers Tareck El Aissami and Sim√≥n Zerpa more than a year after an investigation into billions of lost Petroleos de Venezuela SA revenue that‚Äôs led to a purge of the ruling elite‚Äôs inner circle.\\n\\nMost Read from Bloomberg\\n\\nPublic Prosecutor Tarek William Saab said El Aissami and Zerpa, once President Nicol√°s Maduro‚Äôs closest allies, are part of a group of more than 50 others involved in a scheme to ‚Äúdestroy Venezuela‚Äôs economy,‚Äù including ‚Äúcorrupt‚Äù bankers based in Miami and Washington. Aissami‚Äôs business partner, Samark L√≥pez, was also arrested, Saab said.\\n\\nADVERTISEMENT\\n\\nEl Aissami, who was shown wearing handcuffs and a black t-shirt, will be charged with treason, appropriation and money laundering, Saab said.\\n\\nThe arrests come at a pivotal moment for Maduro, who is facing criticism of unfair conditions ahead of presidential vote in July, in which he is seeking to win a third consecutive term. The US has said it‚Äôs willing to let an important oil and gas license expire on April 18 if the socialist government doesn‚Äôt take steps toward freer elections.\\n\\nRead more: Power Struggle and Missing Billions Roil Venezuelan Ruling Elite\\n\\nEl Aissami resigned as Venezuela‚Äôs oil minister in March 2023 when Maduro launched a sweeping anti-corruption operation after an internal audit revealed a financial black hole at state-owned PDVSA, the government‚Äôs most important source of funding. That triggered a widening corruption investigation that ensnared judges, elected officials, and the head of the nation‚Äôs crypto regulator.\\n\\nSaab said that El Aissami paid millions of dollars in bribes to allocate Venezuelan crude, pet-coke and fuel oil below market value through shell companies that bypassed the central bank. They also used the revenue from those sales to manipulate the country‚Äôs currency exchange system, Saab said.\\n\\nStory continues\\n\\n--With assistance from Fabiola Zerpa.\\n\\n(Updates with charges against El Aissami and US context starting in third paragraph.)\\n\\nMost Read from Bloomberg Businessweek\\n\\n¬©2024 Bloomberg L.P.', metadata={'title': 'Venezuela Detains Former Maduro Confidantes in PDVSA Probe', 'link': 'https://ca.finance.yahoo.com/news/venezuela-detains-two-former-maduro-173140679.html', 'authors': ['Patricia Laya', 'Andreina Itriago Acosta'], 'language': 'en', 'description': '(Bloomberg) -- Venezuela detained former oil and finance ministers Tareck El Aissami and Sim√≥n Zerpa more than a year after an investigation into billions of lost Petroleos de Venezuela SA revenue that‚Äôs led to a purge of the ruling elite‚Äôs inner circle. Most Read from BloombergUS Slams Strikes on Russia Oil Refineries as Risk to Oil MarketsBond Trader Places Record Futures Bet on Eve of Inflation DataIran‚Äôs Better, Stealthier Drones Are Remaking Global WarfareTrumpism Is Emptying ChurchesUkrain', 'publish_date': None}),\n Document(page_content=\"Norfolk Southern has agreed to pay $600 million in a class-action lawsuit settlement for a fiery February 2023 train derailment in Ohio, but residents worry the money not only won‚Äôt go far enough to cover future health needs that could be tremendous but also won't amount to much once divvied up.\\n\\n‚ÄúIt‚Äôs not nowhere near my needs, let alone what the health effects are going to be five or 10 years down the road,‚Äù said Eric Cozza, who lived just three blocks from the derailment and had 47 family members living within a mile (1.61 kilometers).\\n\\nMore than three dozen of the freight train's 149 cars derailed on the outskirts of East Palestine, a town of almost 5,000 residents near the Pennsylvania state line. Several cars spilled a cocktail of hazardous materials that caught fire. Three days later, officials, fearing an explosion, blew open five tankcars filled with vinyl chloride and burned the toxic chemical ‚Äî sending thick, black plumes of smoke into the air. Some 1,500 to 2,000 residents were evacuated.\\n\\nNorfolk Southern said the agreement, if approved by the court, will resolve all class action claims within a 20-mile (32-kilometer) radius of the derailment and, for residents who choose to participate, personal injury claims within a 10-mile (16-kilometer) radius of the derailment.\\n\\nADVERTISEMENT\\n\\nThe area includes East Palestine and people who evacuated, as well as several other larger towns.\\n\\nThe settlement, which doesn‚Äôt include or constitute any admission of liability, wrongdoing or fault, represents only a small slice of the $3 billion in revenue Norfolk Southern generated just in the first three months of this year. The railroad said that even after the settlement it still made a $213 million profit in the quarter.\\n\\nEast Palestine resident Krissy Ferguson called the settlement a ‚Äúheart-wrenching day.‚Äù\\n\\n‚ÄúI just feel like we‚Äôve been victimized over and over and over again,‚Äù she said. ‚ÄúWe fought and we‚Äôre still fighting. And contamination is still flowing down the creeks. People are still sick. And I think people that had the power to fight took an easy way out.‚Äù\\n\\nStory continues\\n\\nMore than a year later residents still complain about respiratory problems and unexplained rashes and nosebleeds, but the greater fear is that people will develop cancer or other serious conditions because of the chemicals they were exposed to. Researchers have only begun to work on determining the lasting repercussions of the derailment.\\n\\nThe company said Tuesday that individuals and businesses will be able to use compensation from the settlement in any manner they see fit.\\n\\nThe settlement is expected to be submitted for preliminary approval to the U.S. District Court for the Northern District of Ohio this month. Payments could begin to arrive by the end of the year, subject to final court approval.\\n\\nNorfolk Southern has already spent more than $1.1 billion on its response to the derailment, including more than $104 million in direct aid to East Palestine and its residents. Partly because Norfolk Southern is paying for the cleanup, President Joe Biden has never declared a disaster in the town, which remains a sore point for many.\\n\\nThe railroad has promised to create a fund to help pay for the long-term health needs of the community, but that hasn‚Äôt been finalized yet.\\n\\nThe plaintiffs' attorneys said the deal follows a year of intense investigation and should provide meaningful relief to residents.\\n\\nStill, residents like Misti Allison have many unanswered questions.\\n\\n‚ÄúWhat goes through my head is, after all the lawyers are paid and the legal fees are accounted for, how much funding will be provided for families? And is that going to be enough for any of these potential damages moving forward?‚Äù she said.\\n\\nJami Wallace, too, worries about having a settlement without knowing the long-term impact of the derailment.\\n\\n‚ÄúI would really like to see the numbers because in my opinion, taking a plea deal only is in the best interest of the attorneys,‚Äù she said. ‚ÄúThey‚Äôre all going to get their money. But we‚Äôre the residents that are still going to be left to suffer.‚Äù\\n\\nCozza said he spent about $8,000 to move out of town and that ‚Äî along with medical bills and the cost of replacing his contaminated belongings ‚Äî exhausted what little savings he had. And he can't put a price on the 10-year relationship he lost or the way his extended family was scattered after the derailment.\\n\\nThe CEO of Threshold Residential, one of the biggest employers in town, estimates that his business has lost well over $100,000.\\n\\nLast week federal officials said that the aftermath of the train derailment doesn‚Äôt qualify as a public health emergency because widespread health problems and ongoing chemical exposure haven‚Äôt been documented, contrasting residents' reports.\\n\\nThe head of the National Transportation Safety Board recently said the agency‚Äôs investigation showed that venting and burning of the vinyl chloride was unnecessary because the producer of the chemical ascertained that no dangerous reaction occurred inside the tank cars. Officials who made the decision ‚Äî Ohio‚Äôs governor and the local fire chief leading the response ‚Äî have said they were never told that.\\n\\nThe NTSB‚Äôs full investigation into the cause of the derailment won‚Äôt be complete until June, but the agency has said that an overheating wheel bearing on one of the railcars, which wasn‚Äôt detected in time by a trackside sensor, likely caused the crash.\\n\\nThe EPA has said cleanup in East Palestine is expected to be completed this year.\\n\\nThe railroad announced preliminary first-quarter earnings of 23 cents per share Tuesday, which reflects the cost of the $600 million settlement. Without the settlement and some other one-time costs, the railroad said it would have made $2.39 per share.\\n\\nRailroad CEO Alan Shaw, who is fighting for his job against an activist investor aiming to overhaul the railroad's operations, said Norfolk Southern is ‚Äúbecoming a more productive and efficient railroad‚Äù but acknowledged there is more work to do.\\n\\nAncora Holdings is trying to persuade investors to support its nominees for Norfolk Southern's board and its plan to replace Shaw and the rest of the management team at the railroad's May 9 annual meeting. Ancora says the company's profits have lagged behind the other major freight railroads for years and the investors question Shaw's leadership.\\n\\nThe railroad said that in addition to the settlement, its results were hurt by $91 million in unusual expenses including money it is spending to fight back against Ancora, management layoffs and the $25 million it gave to CPKC last month for the right to hire one of that railroad's executives to be Norfolk Southern's new chief operating officer.\\n\\nThe railroad said Tuesday that even though volume was up 4% during the first quarter, company revenue fell by 4% because of lower fuel surcharge revenue and changes in the mix of shipments it handled to include more containers of imported goods that railroads get paid less to deliver than other commodities.\\n\\nShares of Norfolk Southern Corp., based in Atlanta, were up slightly through the day after a flat start following the settlement announcement.\\n\\n___\\n\\nAssociated Press writer Brooke Schultz contributed to this report from Harrisburg, Pennsylvania, and Michelle Chapman contributed from New York.\\n\\nJosh Funk, The Associated Press\", metadata={'title': 'Norfolk Southern agrees to $600M settlement in fiery Ohio derailment. Locals fear it‚Äôs not enough', 'link': 'https://ca.finance.yahoo.com/news/norfolk-southern-agrees-pay-600m-121211343.html', 'authors': ['The Canadian Press'], 'language': 'en', 'description': \"Norfolk Southern has agreed to pay $600 million in a class-action lawsuit settlement for a fiery February 2023 train derailment in Ohio, but residents worry the money not only won‚Äôt go far enough to cover future health needs that could be tremendous but also won't amount to much once divvied up. ‚ÄúIt‚Äôs not nowhere near my needs, let alone what the health effects are going to be five or 10 years down the road,‚Äù said Eric Cozza, who lived just three blocks from the derailment and had 47 family memb\", 'publish_date': None}),\n Document(page_content='(Bloomberg) -- Boeing Co. faces a deepening crisis of confidence after an engineer at the US planemaker alleged the company took manufacturing shortcuts on its 787 Dreamliner aircraft in order to ease production bottlenecks of its most advanced airliner.\\n\\nMost Read from Bloomberg\\n\\nFactory workers wrongly measured and filled gaps that can occur when airframe segments of the 787 are joined together, according to Sam Salehpour, a longtime Boeing employee who made his concerns public on Tuesday. That assembly process could create ‚Äúsignificant fatigue‚Äù in the composite material of the barrel sections and impair the structural integrity of more than 1,000 of the widebody jets in service, he said.\\n\\nADVERTISEMENT\\n\\nSalehpour, who according to his attorneys at Katz Banks Kumin LLP in Washington worked on the 787 from 2020 through early 2022, said the issues he described ‚Äúmay dramatically reduce the life of the plane.‚Äù Boeing disputes the allegations.\\n\\n‚ÄúIn a mad rush to reduce the backlog of the planes and get them to market, Boeing did not follow its own engineering requirements,‚Äù the engineer said on a conference call with reporters and his lawyers.\\n\\nThe claims risk opening another flank at the embattled planemaker, which is already facing intense scrutiny of its manufacturing and quality practices since a fuselage panel blew off a nearly new 737 Max 9 shortly after takeoff on Jan. 5. The allegations now extend the spotlight to the Dreamliner, a critical source of cash for Boeing as 737 output remains muted under close oversight by the US Federal Aviation Administration.\\n\\nAfter the allegations were made public, Senator Richard Blumenthal, a Connecticut Democrat, announced that he had asked Boeing‚Äôs departing Chief Executive Officer Dave Calhoun to appear at an April 17 subcommittee hearing called to examine the planemaker‚Äôs safety culture. Calhoun last month announced that he‚Äôs stepping down by the end of the year, part of a wider management shakeup in the wake of the Jan. 5 accident.\\n\\nStory continues\\n\\nHalted Deliveries\\n\\n‚ÄúBoeing understands the important oversight responsibilities of the subcommittee and we are cooperating with this inquiry,‚Äù the company said, when asked if Calhoun or other executives planned to testify. ‚ÄúWe have offered to provide documents, testimony and technical briefings, and are in discussions with the subcommittee regarding next steps.‚Äù\\n\\nIn separate statements, Boeing disputed Salehpour‚Äôs account. The company noted it had halted 787 deliveries for nearly two years earlier this decade under close FAA supervision after it found a spate of tiny structural imperfections in the joints where the carbon-fiber barrel sections are bolted together.\\n\\n‚ÄúThese claims about the structural integrity of the 787 are inaccurate and do not represent the comprehensive work Boeing has done to ensure the quality and long-term safety of the aircraft,‚Äù the planemaker said in a statement responding to the allegations, which were reported earlier Tuesday by the New York Times.\\n\\nCompany engineers are ‚Äúcompleting exhaustive analysis to determine any long-term inspection and maintenance required, with oversight from the FAA,‚Äù Boeing said.\\n\\nThe latest allegations cast Boeing in an unfavorable light as it grapples with a crisis of confidence after the Jan. 5 panel blowout. While nobody on that flight was seriously hurt, the issue has put the spotlight on Boeing‚Äôs manufacturing and safety procedures and has led to a wholesale makeover of senior management. The crisis has jolted investors as well.\\n\\nBoeing shares fell 1.9% Tuesday, extending their decline to almost 32% this year, the worst performance on the Dow Jones Industrial Average.\\n\\nSenate Hearing\\n\\nSalehpour plans to discuss the manufacturing shortfalls he witnessed during the hearing before the Senate Permanent Subcommittee on Investigations scheduled for April 17.\\n\\nIn a March 19 letter to Calhoun, Blumenthal and Wisconsin‚Äôs Ron Johnson, the panel‚Äôs top-ranking Republican, asked for Boeing‚Äôs ‚Äúimmediate cooperation‚Äù with the panel‚Äôs review of Salehpour‚Äôs allegations.\\n\\nSalehpour‚Äôs attorneys flagged the issues to the FAA in a whistleblower letter dated Jan. 19. The agency has launched an investigation and interviewed Salehpour, his attorneys said, adding that other whistleblowers have come forward.\\n\\n‚ÄúVoluntary reporting without fear of reprisal is a critical component in aviation safety,‚Äù the FAA said in a statement. ‚ÄúWe strongly encourage everyone in the aviation industry to share information. We thoroughly investigate all reports.‚Äù\\n\\nBoeing disputed his attorney‚Äôs suggestion that in-service 787 Dreamliners face shorter commercial lives. The company said it has confirmed the finding through extensive fatigue testing, including forces that were more than 10 times the maximum allowed in production.\\n\\nSalehpour said he flagged his concerns to Boeing management but was ignored and ultimately transferred to the 777 program. There he claims to have also witnessed improper production practices after the planemaker ripped out a flawed robotic system without properly redesigning the relevant parts to match the new assembly process.\\n\\nIn a separate statement, Boeing said the claims were also inaccurate. ‚ÄúWe are fully confident in the safety and durability of the 777 family,‚Äù the company said.\\n\\n(Adds whistleblower comment in fourth paragraph.)\\n\\nMost Read from Bloomberg Businessweek\\n\\n¬©2024 Bloomberg L.P.', metadata={'title': 'Boeing Crisis of Confidence Deepens With 787 Now Under Scrutiny', 'link': 'https://ca.finance.yahoo.com/news/boeing-shares-fall-nyt-report-164509069.html', 'authors': ['Siddharth Philip', 'Julie Johnsson'], 'language': 'en', 'description': '(Bloomberg) -- Boeing Co. faces a deepening crisis of confidence after an engineer at the US planemaker alleged the company took manufacturing shortcuts on its 787 Dreamliner aircraft in order to ease production bottlenecks of its most advanced airliner.Most Read from BloombergUS Slams Strikes on Russia Oil Refineries as Risk to Oil MarketsChinese Cement Maker Halted After 99% Crash in 15 MinutesBond Trader Places Record Futures Bet on Eve of Inflation DataApple‚Äôs India iPhone Output Hits $14 Bi', 'publish_date': None}),\n Document(page_content='Restaurants on the eclipse\\'s path of totality saw a jump in sales on Monday as people flocked to find the best spots to see the celestial event, according to sales data from payments technology company Square.\\n\\n\"The eclipse was genuinely a unique cosmic event, but also a unique event for commerce,\" said Ara Kharazian, research and data lead at Square.\\n\\nSquare said Tuesday restaurants that use its technology in Niagara Falls, Ont., which saw a huge influx of visitors for the eclipse, saw 404 per cent higher sales than the average Monday in 2024.\\n\\nHamilton, Ont., saw a 67 per cent jump, while Montreal restaurants saw sales rise 55 per cent.\\n\\nADVERTISEMENT\\n\\nThe increases mirrored a similar pattern in the U.S., where some counties saw restaurant sales rise by more than 500 per cent.\\n\\nThis kind of spike in spending is almost as rare as an eclipse, Kharazian said, as multiple places across North America saw sales skyrocket.\\n\\n\"It‚Äôs simultaneously expected and also pretty stunning,\" he said.\\n\\n\"It is genuinely rare to see this level of highly localized and also supercharged level of spending.\"\\n\\nThe spike gave a welcome boost to restaurants, which are highly seasonal, he said.\\n\\n\"It wasn\\'t a one-day event, we know that we saw some spending leading up to the weekend as well,\" he added.\\n\\n\"That excess sales revenue is going to be very helpful to a business ... We hear often from restaurants who report that some of their biggest challenges have to do with access to cash and liquidity.\"\\n\\nMunicipalities across Central and Eastern Canada spent months preparing for the brief window of time in which the sun, Earth and moon aligned on Monday afternoon.\\n\\nDemand for hotels and short-term rentals surged for the weekend ahead of the eclipse, while municipalities planned events centred around the phenomenon.\\n\\nA February report from Airbnb said Montreal and the Niagara Region were among the most popular cities on its platform along the path of totality.\\n\\nStory continues\\n\\nMunicipalities like Hamilton and Niagara Falls urged visitors to plan ahead, anticipating heavy traffic and high demand. The mayor of Niagara Falls said about a million people were expected to fill the city, the largest crowd in its history.\\n\\n\"It was not a typical Monday in April, that‚Äôs for sure,\" said Janice Thomson, president and CEO of Niagara Falls Tourism.\\n\\nThough Niagara Falls is a yearlong tourist destination, Thomson said she\\'d never experienced anything like what happened on Monday, with a huge crowd of people cheering every time the clouds parted.\\n\\nCanada\\'s telecom companies deployed additional infrastructure in preparation for large crowds of people. In a statement, Rogers said its network handled more than six times the amount of traffic it normally does in Niagara Falls, thanks in part to portable mobile towers.\\n\\nThis report by The Canadian Press was first published April 9, 2024.\\n\\nRosa Saba, The Canadian Press', metadata={'title': \"Restaurants along eclipse's path of totality saw sales boom: Square\", 'link': 'https://ca.finance.yahoo.com/news/restaurants-along-eclipses-path-totality-153650201.html', 'authors': ['The Canadian Press'], 'language': 'en', 'description': 'Restaurants on the eclipse\\'s path of totality saw a jump in sales on Monday as people flocked to find the best spots to see the celestial event, according to sales data from payments technology company Square. \"The eclipse was genuinely a unique cosmic event, but also a unique event for commerce,\" said Ara Kharazian, research and data lead at Square. Square said Tuesday restaurants that use its technology in Niagara Falls, Ont., which saw a huge influx of visitors for the eclipse, saw 404 per ce', 'publish_date': None})]\n\n\n\ndf = pd.DataFrame(\n    [{\"title\": d.metadata[\"title\"], \"text\": d.page_content} for d in data]\n)\n\ndf\n\n\n\n\n\n\n\n\ntitle\ntext\n\n\n\n\n0\nHigher oil doesn't shake industry talk on spen...\nTORONTO ‚Äî The price of oil has been on a stead...\n\n\n1\nVenezuela Detains Former Maduro Confidantes in...\n(Bloomberg) -- Venezuela detained former oil a...\n\n\n2\nNorfolk Southern agrees to $600M settlement in...\nNorfolk Southern has agreed to pay $600 millio...\n\n\n3\nBoeing Crisis of Confidence Deepens With 787 N...\n(Bloomberg) -- Boeing Co. faces a deepening cr...\n\n\n4\nRestaurants along eclipse's path of totality s...\nRestaurants on the eclipse's path of totality ..."
  },
  {
    "objectID": "posts/sentiment/index.html#dictionnary-based-approach",
    "href": "posts/sentiment/index.html#dictionnary-based-approach",
    "title": "Financial news sentiment analysis in Python",
    "section": "Dictionnary-based approach",
    "text": "Dictionnary-based approach\nA dictionary-based approach is a simple way to perform sentiment analysis. It consists of using a list of words with associated sentiment scores. The sentiment score of a sentence is the sum of the sentiment scores of the words it contains. You can also normalize the score by the number of words in the sentence.\nFor this type of approach, we will need to use a financial sentiment dictionary. I will use the Loughran-McDonald dictionary, which is the most commonly used used in finance reasearch. Note that this dictionary is designed for financial statements and earnings calls, so it may not be the best choice for news articles.\nYou can download the dictionary from here (direct link: Loughran-McDonald_MasterDictionary_1993-2023.csv)\n\nPre-processing\nThe preprocessing steps to use a dictionnary based approach depend on the dictionnary you are using and the final measure you want to obtain. In this case, we will use the Loughran-McDonald dictionary, which contains variation of similar words, including plural forms, verb forms, etc. Therefore, we do not need to perform stemming or lemmatization, a common step in text preprocessing.\nThe preprocessing steps we will perform are:\n\nLowercasing\nRemoving punctuation\nRemoving stopwords (frequent words that do not carry much meaning)\nRemoving numbers\n\nThe removal of stopwords and numbers is optional, but it will affect the sentiment score of the text as measure as a ratio of the number of words in the text. Other common filtering includes removing URLs, emails, cities, company names, etc.\n\n# Using the same ones as Loughran-McDonald\nwith open(\"stopwords.txt\", \"r\") as f:\n    stopwords = f.read().split(\"\\n\")[:-1]\nstopwords[:10]\n\n['about', 'and', 'from', 'now', 'where', 'you', 'am', 'until', 'them', 'in']\n\n\n\ndef preprocess_text(text):\n    words = text.split()\n    words = [w.lower() for w in words]\n    words = [w for w in words if w not in stopwords]\n    # Remove punctuation and numbers\n    words = [w for w in words if w.isalpha()]\n    return \" \".join(words)\n\n\ndf[\"text_clean\"] = df[\"text\"].apply(preprocess_text)\ndf\n\n\n\n\n\n\n\n\ntitle\ntext\ntext_clean\n\n\n\n\n0\nHigher oil doesn't shake industry talk on spen...\nTORONTO ‚Äî The price of oil has been on a stead...\ntoronto price oil a steady climb talk biggest ...\n\n\n1\nVenezuela Detains Former Maduro Confidantes in...\n(Bloomberg) -- Venezuela detained former oil a...\nvenezuela detained former oil finance minister...\n\n\n2\nNorfolk Southern agrees to $600M settlement in...\nNorfolk Southern has agreed to pay $600 millio...\nnorfolk southern agreed pay million a lawsuit ...\n\n\n3\nBoeing Crisis of Confidence Deepens With 787 N...\n(Bloomberg) -- Boeing Co. faces a deepening cr...\nboeing faces a deepening crisis confidence eng...\n\n\n4\nRestaurants along eclipse's path of totality s...\nRestaurants on the eclipse's path of totality ...\nrestaurants path totality saw a jump sales mon...\n\n\n\n\n\n\n\nAfter preprocessing, each article is a string containing only lower case words separated by spaces.\n\n\nDictionary\nNext, will load the dictionary and make a list of positive words and a list of negative words.\n\nlm_dict = pd.read_csv(\"Loughran-McDonald_MasterDictionary_1993-2023.csv\")\nlm_dict\n\n\n\n\n\n\n\n\nWord\nSeq_num\nWord Count\nWord Proportion\nAverage Proportion\nStd Dev\nDoc Count\nNegative\nPositive\nUncertainty\nLitigious\nStrong_Modal\nWeak_Modal\nConstraining\nComplexity\nSyllables\nSource\n\n\n\n\n0\nAARDVARK\n1\n664\n2.690000e-08\n1.860000e-08\n4.050000e-06\n131\n0\n0\n0\n0\n0\n0\n0\n0\n2\n12of12inf\n\n\n1\nAARDVARKS\n2\n3\n1.210000e-10\n8.230000e-12\n9.020000e-09\n1\n0\n0\n0\n0\n0\n0\n0\n0\n2\n12of12inf\n\n\n2\nABACI\n3\n9\n3.640000e-10\n1.110000e-10\n5.160000e-08\n7\n0\n0\n0\n0\n0\n0\n0\n0\n3\n12of12inf\n\n\n3\nABACK\n4\n29\n1.170000e-09\n6.330000e-10\n1.560000e-07\n28\n0\n0\n0\n0\n0\n0\n0\n0\n2\n12of12inf\n\n\n4\nABACUS\n5\n9349\n3.790000e-07\n3.830000e-07\n3.460000e-05\n1239\n0\n0\n0\n0\n0\n0\n0\n0\n3\n12of12inf\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n86548\nZYGOTE\n86529\n69\n2.790000e-09\n1.310000e-09\n2.940000e-07\n45\n0\n0\n0\n0\n0\n0\n0\n0\n2\n12of12inf\n\n\n86549\nZYGOTES\n86530\n1\n4.050000e-11\n1.720000e-11\n1.880000e-08\n1\n0\n0\n0\n0\n0\n0\n0\n0\n2\n12of12inf\n\n\n86550\nZYGOTIC\n86531\n0\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n12of12inf\n\n\n86551\nZYMURGIES\n86532\n0\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n12of12inf\n\n\n86552\nZYMURGY\n86533\n0\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n12of12inf\n\n\n\n\n86553 rows √ó 17 columns\n\n\n\n\npos_words = lm_dict[lm_dict[\"Positive\"] != 0][\"Word\"].str.lower().to_list()\nneg_words = lm_dict[lm_dict[\"Negative\"] != 0][\"Word\"].str.lower().to_list()\n\npos_words[:10]\n\n['able',\n 'abundance',\n 'abundant',\n 'acclaimed',\n 'accomplish',\n 'accomplished',\n 'accomplishes',\n 'accomplishing',\n 'accomplishment',\n 'accomplishments']\n\n\n\nneg_words[:10]\n\n['abandon',\n 'abandoned',\n 'abandoning',\n 'abandonment',\n 'abandonments',\n 'abandons',\n 'abdicated',\n 'abdicates',\n 'abdicating',\n 'abdication']\n\n\n\n\nSentiment score\nWe first need to compute the total number of words, and the number of positive and negative words in each article.\n\ndf[\"n\"] = df[\"text_clean\"].apply(lambda x: len(x.split()))\ndf[\"n_pos\"] = df[\"text_clean\"].apply(\n    lambda x: len([w for w in x.split() if w in pos_words])\n)\ndf[\"n_neg\"] = df[\"text_clean\"].apply(\n    lambda x: len([w for w in x.split() if w in neg_words])\n)\n\ndf\n\n\n\n\n\n\n\n\ntitle\ntext\ntext_clean\nn\nn_pos\nn_neg\n\n\n\n\n0\nHigher oil doesn't shake industry talk on spen...\nTORONTO ‚Äî The price of oil has been on a stead...\ntoronto price oil a steady climb talk biggest ...\n261\n1\n7\n\n\n1\nVenezuela Detains Former Maduro Confidantes in...\n(Bloomberg) -- Venezuela detained former oil a...\nvenezuela detained former oil finance minister...\n179\n1\n12\n\n\n2\nNorfolk Southern agrees to $600M settlement in...\nNorfolk Southern has agreed to pay $600 millio...\nnorfolk southern agreed pay million a lawsuit ...\n578\n8\n31\n\n\n3\nBoeing Crisis of Confidence Deepens With 787 N...\n(Bloomberg) -- Boeing Co. faces a deepening cr...\nboeing faces a deepening crisis confidence eng...\n402\n3\n38\n\n\n4\nRestaurants along eclipse's path of totality s...\nRestaurants on the eclipse's path of totality ...\nrestaurants path totality saw a jump sales mon...\n255\n4\n1\n\n\n\n\n\n\n\nThe sentiment of a text is the sum of the sentiment scores of the words it contains (I call this the level). We can also normalize the score by the number of words in the text or by the number of sentiment words in the text. If we want to end up with a categorical classification, we can use a threshold to determine if the text is positive, negative or neutral.\n\ndf[\"lm_level\"] = df[\"n_pos\"] - df[\"n_neg\"]\n\ndf[\"lm_score1\"] = (df[\"n_pos\"] - df[\"n_neg\"]) / df[\"n\"]\ndf[\"lm_score2\"] = (df[\"n_pos\"] - df[\"n_neg\"]) / (df[\"n_pos\"] + df[\"n_neg\"])\n\nCUTOFF = 0.3\ndf[\"lm_sentiment\"] = df[\"lm_score2\"].apply(\n    lambda x: \"positive\" if x &gt; CUTOFF else \"negative\" if x &lt; -CUTOFF else \"neutral\"\n)\ndf\n\n\n\n\n\n\n\n\ntitle\ntext\ntext_clean\nn\nn_pos\nn_neg\nlm_level\nlm_score1\nlm_score2\nlm_sentiment\n\n\n\n\n0\nHigher oil doesn't shake industry talk on spen...\nTORONTO ‚Äî The price of oil has been on a stead...\ntoronto price oil a steady climb talk biggest ...\n261\n1\n7\n-6\n-0.022989\n-0.750000\nnegative\n\n\n1\nVenezuela Detains Former Maduro Confidantes in...\n(Bloomberg) -- Venezuela detained former oil a...\nvenezuela detained former oil finance minister...\n179\n1\n12\n-11\n-0.061453\n-0.846154\nnegative\n\n\n2\nNorfolk Southern agrees to $600M settlement in...\nNorfolk Southern has agreed to pay $600 millio...\nnorfolk southern agreed pay million a lawsuit ...\n578\n8\n31\n-23\n-0.039792\n-0.589744\nnegative\n\n\n3\nBoeing Crisis of Confidence Deepens With 787 N...\n(Bloomberg) -- Boeing Co. faces a deepening cr...\nboeing faces a deepening crisis confidence eng...\n402\n3\n38\n-35\n-0.087065\n-0.853659\nnegative\n\n\n4\nRestaurants along eclipse's path of totality s...\nRestaurants on the eclipse's path of totality ...\nrestaurants path totality saw a jump sales mon...\n255\n4\n1\n3\n0.011765\n0.600000\npositive"
  },
  {
    "objectID": "posts/sentiment/index.html#finbert",
    "href": "posts/sentiment/index.html#finbert",
    "title": "Financial news sentiment analysis in Python",
    "section": "FinBert",
    "text": "FinBert\nBERT-based models are often called ‚Äústate-of-the-art models‚Äù in recent papers in the finance even if the original Bert paper dates from 2018 and many more advanced models have come along since. They are pre-trained on a large corpus of text and fine-tuned on a specific task. In this tutorial, we will use FinBert, a BERT model fine-tuned on financial data (see FinBert paper).\nThe way these models work is by taking a sequence of tokens as input and generating a vector embedding for the text, which is a representation of the information in the text. This vector can be used as input to a classifier to predict the sentiment of the text. The FinBert model is trained to output softmax outputs (ie, probabilities) for three classes: positive, negative, and neutral.\n\nPre-processing\nI won‚Äôt perform any pre-processing on the text before feeding it to the model. The model will take care of tokenizing the text and converting it to a sequence of tokens. Common pre-processing for Bert models include masking some words (date, company names, etc.), but I won‚Äôt do that here.\n\n\nUsage\nMany Bert models, including FinBert, are available in the HuggingFace Transformers library and can be fetched automatically.\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport scipy\nimport torch\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n\nTo estimate the sentiment of a text, we will use a tokenizer to convert the text to a sequence of tokens, and then feed the tokens to the model to get the softmax outputs. This tokenizer will also pad or truncate the sequence to a fixed length if needed. The sentiment of the text is the class with the highest probability, which we can compute by applying the softmax function to the output of the model. We will also extract the probability associated with each class. Finally, we put the code behind a with torch.no_grad(): block to avoid computing gradients, as we are only interested in the output of the model, not training the model.\n\ndef finbert_sentiment(text: str) -&gt; tuple[float, float, float, str]:\n    with torch.no_grad():\n        inputs = tokenizer(\n            text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n        )\n        outputs = model(**inputs)\n        logits = outputs.logits\n        scores = {\n            k: v\n            for k, v in zip(\n                model.config.id2label.values(),\n                scipy.special.softmax(logits.numpy().squeeze()),\n            )\n        }\n        return (\n            scores[\"positive\"],\n            scores[\"negative\"],\n            scores[\"neutral\"],\n            max(scores, key=scores.get),\n        )\n\nWe apply our sentiment function to each article in the DataFrame to get the sentiment of each article using the apply() method. We then apply the pd.Series constructor to convert the returned tuples so that we can assign the results to new columns in the DataFrame. Finally, we can also get a numerical score by taking the difference between the positive and negative probabilities.\n\n# Notice that this is the raw text, no preprocessing\ndf[[\"finbert_pos\", \"finbert_neg\", \"finbert_neu\", \"finbert_sentiment\"]] = (\n    df[\"text\"].apply(finbert_sentiment).apply(pd.Series)\n)\ndf[\"finbert_score\"] = df[\"finbert_pos\"] - df[\"finbert_neg\"]\n\n\ndf[\n    [\n        \"title\",\n        \"text\",\n        \"finbert_pos\",\n        \"finbert_neg\",\n        \"finbert_neu\",\n        \"finbert_sentiment\",\n        \"finbert_score\",\n    ]\n]\n\n\n\n\n\n\n\n\ntitle\ntext\nfinbert_pos\nfinbert_neg\nfinbert_neu\nfinbert_sentiment\nfinbert_score\n\n\n\n\n0\nHigher oil doesn't shake industry talk on spen...\nTORONTO ‚Äî The price of oil has been on a stead...\n0.363470\n0.054056\n0.582474\nneutral\n0.309414\n\n\n1\nVenezuela Detains Former Maduro Confidantes in...\n(Bloomberg) -- Venezuela detained former oil a...\n0.022471\n0.666017\n0.311512\nnegative\n-0.643546\n\n\n2\nNorfolk Southern agrees to $600M settlement in...\nNorfolk Southern has agreed to pay $600 millio...\n0.030382\n0.732271\n0.237347\nnegative\n-0.701889\n\n\n3\nBoeing Crisis of Confidence Deepens With 787 N...\n(Bloomberg) -- Boeing Co. faces a deepening cr...\n0.009635\n0.949322\n0.041043\nnegative\n-0.939687\n\n\n4\nRestaurants along eclipse's path of totality s...\nRestaurants on the eclipse's path of totality ...\n0.783017\n0.038716\n0.178267\npositive\n0.744301"
  },
  {
    "objectID": "posts/sentiment/index.html#llm-models",
    "href": "posts/sentiment/index.html#llm-models",
    "title": "Financial news sentiment analysis in Python",
    "section": "LLM models",
    "text": "LLM models\nLLM models are large language models that are trained on a large corpus of text. They are often used for text generation, but they can also be used for sentiment analysis. The approach is to design a prompt that will make the model output a sentiment score. Langchain is a library that makes it easy to use LLM models for different tasks, including sentiment analysis.\n\nOllama\nOllama is a tool to manage and run local LLMs, such as Meta‚Äôs Llama2 and Mistral‚Äôs Mixtral. I discussed how to use Ollama as a private, local ChatGPT replacement in a previous post.\nFor this example, I will be using two open-source LLM models served locally by Ollama. The process would be similar if you were using other models such as OpenAI‚Äôs GPT-3.5 or GPT-4. The main difference in execution is that Langchain supports OpenAI‚Äôs function API which insures that the output is properly formatted, which as we will see can be an issue with Llama2 and Mixtral, the two models I will use.\nThe first step in setting up Ollama is to download and install the tool on your local machine. The installation process is straightforward and involves running a few commands in your terminal. Ollama‚Äôs download page provides installers for macOS and Windows, as well as instructions for Linux users. Once you‚Äôve downloaded the installer, follow the installation instructions to set up Ollama on your machine.\nIf you‚Äôre using a Mac, you can install Ollama using Homebrew by running the following command in your terminal:\nbrew install ollama\nThe benefit of using Homebrew is that it simplifies the installation process and also sets up Ollama as a service, allowing it to run in the background and manage the LLM models you download.\nAt the moment, the most popular code models on Ollama are:\nAfter installing Ollama, you can install a model from the command line using the pull command:\nollama pull mixtral\n\nLangchain\nLangchain is a Python library that provides a simple interface to interact with LLM models. Specifically, our approach will be the following:\n\nDefine the desired output format using Langchain‚Äôs support for Pydantic models.\nDefine a prompt template that will be used to generate the prompt for the model.\nDefine the chat model that we want to use.\nFinally, combine the prompt, the model, and the output format to get the sentiment of the text.\n\nIn case the returned output is not in the expected format, we can use the tenacity library to retry the request a few times until we get the expected output. Langchain also provides various features to handle bad outputs which I suggest you explore once you are comfortable with the basics.\n\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import PydanticOutputParser\n\n\nfrom langchain_community.chat_models import ChatOllama\n\n\n# For handling errors\nfrom tenacity import retry, stop_after_attempt, RetryError\n\nWe will first define the desired output format as a Pydantic model, from which we will create a PydanticOutputParser object. This object will be used to inject output definition into the prompt and to parse the output of the model.\n\nclass SentimentClassification(BaseModel):\n    sentiment: str = Field(\n        ...,\n        description=\"The sentiment of the text\",\n        enum=[\"positive\", \"negative\", \"neutral\"],\n    )\n    score: float = Field(..., description=\"The score of the sentiment\", ge=-1, le=1)\n    justification: str = Field(..., description=\"The justification of the sentiment\")\n    main_entity: str = Field(..., description=\"The main entity discussed in the text\")\n\nWe then define our llm_sentiment() function that will create the output parser, the prompt, and the model and combine them in a chain. We will then wrap the call to chain.invoke() in order to handle bad outputs and retry the request a few times until we get the expected output with the @retry decorator from the tenacity library.\n\n@retry(stop=stop_after_attempt(5))\ndef run_chain(text: str, chain) -&gt; dict:\n    return chain.invoke({\"news\": text}).dict()\n\n\ndef llm_sentiment(text: str, llm) -&gt; tuple[str, float, str, str]:\n    parser = PydanticOutputParser(pydantic_object=SentimentClassification)\n\n    prompt = PromptTemplate(\n        template=\"Describe the sentiment of a text of financial news.\\n{format_instructions}\\n{news}\\n\",\n        input_variables=[\"news\"],\n        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n    )\n\n    chain = prompt | llm | parser\n\n    try:\n        result = run_chain(text, chain)\n\n        return (\n            result[\"sentiment\"],\n            result[\"score\"],\n            result[\"justification\"],\n            result[\"main_entity\"],\n        )\n    except RetryError as e:\n        print(f\"Error: {e}\")\n        return \"error\", 0, \"\", \"\"\n\nFinally, we apply our sentiment function to each article in the DataFrame to get the sentiment of each article using the apply() method.\n\n# Replace with the correct model, or use ChatOpenAI if you want to use OpenAI\nllama2 = ChatOllama(model=\"llama2\", temperature=0.1)\n\ndf[\n    [\"llama2_sentiment\", \"llama2_score\", \"llama2_justification\", \"llama2_main_entity\"]\n] = (df[\"text\"].apply(lambda x: llm_sentiment(x, llama2)).apply(pd.Series))\n\nError: RetryError[&lt;Future at 0x329c35e50 state=finished raised OutputParserException&gt;]\nError: RetryError[&lt;Future at 0x329c629c0 state=finished raised OutputParserException&gt;]\nError: RetryError[&lt;Future at 0x3295c7680 state=finished raised OutputParserException&gt;]\n\n\n\ndf[\n    [\n        \"title\",\n        \"text\",\n        \"llama2_sentiment\",\n        \"llama2_score\",\n        \"llama2_justification\",\n        \"llama2_main_entity\",\n    ]\n]\n\n\n\n\n\n\n\n\ntitle\ntext\nllama2_sentiment\nllama2_score\nllama2_justification\nllama2_main_entity\n\n\n\n\n0\nHigher oil doesn't shake industry talk on spen...\nTORONTO ‚Äî The price of oil has been on a stead...\npositive\n0.7\nThe article focuses on the oil and gas industr...\nSuncor Energy Inc.\n\n\n1\nVenezuela Detains Former Maduro Confidantes in...\n(Bloomberg) -- Venezuela detained former oil a...\nerror\n0.0\n\n\n\n\n2\nNorfolk Southern agrees to $600M settlement in...\nNorfolk Southern has agreed to pay $600 millio...\nerror\n0.0\n\n\n\n\n3\nBoeing Crisis of Confidence Deepens With 787 N...\n(Bloomberg) -- Boeing Co. faces a deepening cr...\nnegative\n0.7\nThe article reports on allegations of manufact...\nBoeing Co.\n\n\n4\nRestaurants along eclipse's path of totality s...\nRestaurants on the eclipse's path of totality ...\nerror\n0.0\n\n\n\n\n\n\n\n\n\nAs we can see, Llama 2 struggled to provide a valid result for some of the articles, even after 3 tries. This is a common issue with LLMs, and it is important to handle bad outputs properly. I will check if I get better results with Mixtral (more specifically dolphin-mixtral, a fine-tuned version of Mixtral), another LLM model available on Ollama.\n\nmixtral = ChatOllama(model=\"dolphin-mixtral:latest\", temperature=0.1)\n\ndf[\n    [\n        \"mixtral_sentiment\",\n        \"mixtral_score\",\n        \"mixtral_justification\",\n        \"mixtral_main_entity\",\n    ]\n] = (\n    df[\"text\"].apply(lambda x: llm_sentiment(x, mixtral)).apply(pd.Series)\n)\n\nError: RetryError[&lt;Future at 0x329d5b320 state=finished raised OutputParserException&gt;]\n\n\n\ndf[\n    [\n        \"title\",\n        \"text\",\n        \"mixtral_sentiment\",\n        \"mixtral_score\",\n        \"mixtral_justification\",\n        \"mixtral_main_entity\",\n    ]\n]\n\n\n\n\n\n\n\n\ntitle\ntext\nmixtral_sentiment\nmixtral_score\nmixtral_justification\nmixtral_main_entity\n\n\n\n\n0\nHigher oil doesn't shake industry talk on spen...\nTORONTO ‚Äî The price of oil has been on a stead...\nneutral\n0.00\nThe text discusses the focus on spending disci...\nCanadian Association of Petroleum Producers co...\n\n\n1\nVenezuela Detains Former Maduro Confidantes in...\n(Bloomberg) -- Venezuela detained former oil a...\nnegative\n-0.80\nThe text discusses the detention of former oil...\nTareck El Aissami and Sim√≥n Zerpa\n\n\n2\nNorfolk Southern agrees to $600M settlement in...\nNorfolk Southern has agreed to pay $600 millio...\nerror\n0.00\n\n\n\n\n3\nBoeing Crisis of Confidence Deepens With 787 N...\n(Bloomberg) -- Boeing Co. faces a deepening cr...\nnegative\n-0.80\nThe text discusses a crisis of confidence for ...\nBoeing\n\n\n4\nRestaurants along eclipse's path of totality s...\nRestaurants on the eclipse's path of totality ...\npositive\n0.85\nThe text discusses a significant increase in s...\neclipse\n\n\n\n\n\n\n\nMixtral managed to provide a valid result for all the articles but one, which is an improvement over Llama 2. One advantage of LLMs is that they can provide not only a sentiment score but also a summary of the reasoning behind the score. This can be useful for understanding the model‚Äôs decision-making process and for debugging bad outputs.\n\nimport textwrap\n\nprint(textwrap.fill(df.iloc[0][\"text\"][:500] + \"...\") + \"\\n\")\nprint(\"Llama2: \" + textwrap.fill(df.iloc[0][\"llama2_justification\"]) + \"\\n\")\nprint(\"Mixtral: \" + textwrap.fill(df.iloc[0][\"mixtral_justification\"]))\n\nTORONTO ‚Äî The price of oil has been on a steady climb all year, but\nthe talk at Canada's biggest oil and gas conference is still focused\non spending discipline.  Industry leaders at the Canadian Association\nof Petroleum Producers conference, held in Toronto this year, have\nbeen emphasizing their predictability and focus on returning money to\nshareholders, rather than talk of growth.  Suncor Energy Inc. chief\nexecutive Rich Kruger, who was named head of the oil and gas producer\nlast year as it st...\n\nLlama2: The article focuses on the oil and gas industry in Canada and how\ncompanies are shifting their priorities from growth to spending\ndiscipline. The CEOs of Suncor Energy, Cenovus Energy, and Whitecap\nResources emphasize the importance of returning money to shareholders\nrather than talking about growth. The article highlights the changing\nnarrative around oil demand and how it's still growing despite\ninvestor concerns. The overall tone of the article is positive as\ncompanies are taking a more disciplined approach to their operations.\n\nMixtral: The text discusses the focus on spending discipline and predictability\nin the Canadian oil and gas industry, with leaders emphasizing their\ngoal of returning money to shareholders rather than pursuing growth.\nThe sentiment is neutral as it does not express strong positive or\nnegative emotions."
  },
  {
    "objectID": "posts/sentiment/index.html#conclusion",
    "href": "posts/sentiment/index.html#conclusion",
    "title": "Financial news sentiment analysis in Python",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, each class of models has its own strengths and weaknesses. Dictionary-based approaches are simple and interpretable, but they may not be as accurate as more sophisticated models. BERT-based models like FinBert are more accurate and can handle more complex text, but they require more computational resources than simple word-counting methods. LLMs like Llama 2 and Mixtral can provide detailed reasoning behind their predictions, but they can be slow, more costly to run, and may not always provide valid outputs.\nIt is worth exploring various approaches to see which one works best for your specific use case. Note also that when it comes to dictionary-based approaches and BERT-based models, the dictionaries and models are usually designed for a specific type of text, so it is important to use the right one for your use case.\n\ndf[\n    [\n        \"title\",\n        \"text\",\n        \"lm_sentiment\",\n        \"finbert_sentiment\",\n        \"llama2_sentiment\",\n        \"mixtral_sentiment\",\n    ]\n]\n\n\n\n\n\n\n\n\ntitle\ntext\nlm_sentiment\nfinbert_sentiment\nllama2_sentiment\nmixtral_sentiment\n\n\n\n\n0\nHigher oil doesn't shake industry talk on spen...\nTORONTO ‚Äî The price of oil has been on a stead...\nnegative\nneutral\npositive\nneutral\n\n\n1\nVenezuela Detains Former Maduro Confidantes in...\n(Bloomberg) -- Venezuela detained former oil a...\nnegative\nnegative\nerror\nnegative\n\n\n2\nNorfolk Southern agrees to $600M settlement in...\nNorfolk Southern has agreed to pay $600 millio...\nnegative\nnegative\nerror\nerror\n\n\n3\nBoeing Crisis of Confidence Deepens With 787 N...\n(Bloomberg) -- Boeing Co. faces a deepening cr...\nnegative\nnegative\nnegative\nnegative\n\n\n4\nRestaurants along eclipse's path of totality s...\nRestaurants on the eclipse's path of totality ...\npositive\npositive\nerror\npositive"
  },
  {
    "objectID": "posts/sentiment/index.html#references",
    "href": "posts/sentiment/index.html#references",
    "title": "Financial news sentiment analysis in Python",
    "section": "References",
    "text": "References\n\nFinBert\nBert\nLoughran and McDonald"
  },
  {
    "objectID": "posts/apple-mlx/index.html",
    "href": "posts/apple-mlx/index.html",
    "title": "Speed up your scientific computing workflows with MLX",
    "section": "",
    "text": "The rapidly evolving landscape in machine learning (ML) tools and libraries sometimes offer positive spillovers for scientific computing and empirical finance research. One such innovation that has recently caught my attention is MLX, an array framework developed by Apple Machine Learning Research. Designed to leverage the power of Apple silicon (M1/M2/M3 chips), MLX is engineered to optimize machine learning workflows with remarkable efficiency. But could this library, with its suite of features tailored for ML research, also benefit workflows in empirical finance research and other scientific domains?\nIn this post, I explore the potential of MLX in scientific computing using a sample empirical finance research task."
  },
  {
    "objectID": "posts/apple-mlx/index.html#video-tutorial",
    "href": "posts/apple-mlx/index.html#video-tutorial",
    "title": "Speed up your scientific computing workflows with MLX",
    "section": "Video tutorial",
    "text": "Video tutorial\nThis post is also available as a video tutorial on YouTube."
  },
  {
    "objectID": "posts/apple-mlx/index.html#what-is-mlx",
    "href": "posts/apple-mlx/index.html#what-is-mlx",
    "title": "Speed up your scientific computing workflows with MLX",
    "section": "What is MLX?",
    "text": "What is MLX?\nMLX emerges as a promising solution for researchers seeking to harness the capabilities of Apple silicon, using the combined power of the CPU and GPU. At its core, MLX is an array framework that prioritizes efficiency, flexibility, and user-friendly experience, characteristics that are increasingly demanded in the fast-paced world of scientific research. Here‚Äôs a summary of MLX‚Äôs key feature:\n\nFamiliar APIs Across Languages: MLX offers a Python API that mirrors NumPy, making the transition seamless for those already acquainted with Python‚Äôs scientific computing ecosystem. It also provides APIs specific for ML tasks that are designed to be intuitive and easy to use.\nComposable Function Transformations: The library‚Äôs support for composable function transformations‚Äîencompassing automatic differentiation, automatic vectorization, and computation graph optimization simplifies the process of building complex models.\nLazy Computation and Dynamic Graph Construction: MLX‚Äôs approach to computation, characterized by laziness and dynamic graph construction, ensures that arrays are only materialized when necessary. This not only optimizes memory usage but also simplifies debugging and accelerates the iterative research process, as changes to the shapes of function arguments do not incur the penalty of slow compilations.\nMulti-Device Support and Unified Memory: The framework‚Äôs ability to run operations across different devices (currently CPU and GPU) without requiring data transfers is a notable advantage. This unified memory model is a departure from conventional frameworks, promising significant efficiency gains and ease of use in a multi-device environment."
  },
  {
    "objectID": "posts/apple-mlx/index.html#sample-problem",
    "href": "posts/apple-mlx/index.html#sample-problem",
    "title": "Speed up your scientific computing workflows with MLX",
    "section": "Sample problem",
    "text": "Sample problem\nFor this post, I will focus on a task that is commonplace in empirical finance research: simulating time series processes. My objective is to evaluate the performance and ease of use of MLX, a new array framework designed for machine learning research on Apple silicon, in comparison to more traditional approaches. To do this, I will contrast MLX against workflows based on NumPy and NumPy optimized with Numba.\nNumba is a library that enhances the speed of Python code, particularly when working with NumPy arrays, through just-in-time (JIT) compilation. This means it compiles functions at runtime, significantly boosting their execution speed on the CPU. While NumPy does offer GPU support, it is limited to Nvidia GPUs, which are not available on Mac laptops. Therefore, our comparison will involve two primary workflows: one based on NumPy, another optimized with Numba for CPU execution, and finally, our workflow utilizing MLX.\n\nAR(3)-GARCH(1,1) Process\nThe focus of my example will be on a simulation-based workflow where I simulate an AR(3)-GARCH(1,1) process multiple times. For those less familiar with time series analysis, AR(3) refers to a third-order autoregressive process, and GARCH(1,1) denotes a generalized autoregressive conditional heteroskedasticity process with one lag for both the autoregressive and moving average components. This combination is commonly used to model time series data exhibiting volatility clustering‚Äîa phenomenon often observed in financial markets.\nTo describe an AR(3)-GARCH(1,1) process, we need to define both the autoregressive and the volatility (GARCH) components of the model. Here‚Äôs how each component is mathematically represented:\n\nAR(3) Component\nThe AR(3) (Autoregressive model of order 3) component models the value of a time series at a particular time as a linear combination of its three previous values, plus a constant term and a stochastic error term. The equation for the AR(3) model is:\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\phi_3 y_{t-3} + \\epsilon_t\n\\]\nwhere:\n\n\\(y_t\\) is the value of the time series at time \\(t\\),\n\\(c\\) is a constant, which I set to 0 in my simulations,\n\\(\\phi_1, \\phi_2, \\phi_3\\) are the parameters of the model for each of the three lags,\n\\(\\epsilon_t\\) is the error term at time \\(t\\), which is assumed to be normally distributed with zero mean and variance \\(\\sigma_t^2\\), i.e., \\(\\epsilon_t \\sim N(0, \\sigma_t^2)\\).\n\n\n\nGARCH(1,1) Component\nThe GARCH(1,1) (Generalized Autoregressive Conditional Heteroskedasticity model of order (1,1)) component models the variance of the error term (\\(\\sigma_t^2\\)) as a function of its own past values and the past values of the squared error term (\\(\\epsilon_{t-1}^2\\)). The equation for the GARCH(1,1) model is:\n\\[\n\\sigma_t^2 = \\alpha_0 + \\alpha_1 \\epsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2\n\\]\nwhere:\n\n\\(\\sigma_t^2\\) is the conditional variance (squared volatility) of the error term at time \\(t\\),\n\\(\\alpha_0\\) is a constant term (must be positive),\n\\(\\alpha_1\\) and \\(\\beta_1\\) are parameters of the model that measure the impact of the previous period‚Äôs squared error and the previous period‚Äôs variance, respectively, on the current variance.\n\n\n\nCombined AR(3)-GARCH(1,1) Model\nIn a combined AR(3)-GARCH(1,1) model, the \\(\\epsilon_t\\) in the AR(3) component follows the conditional variance equation specified by the GARCH(1,1) model. This combination allows the model to capture both the autocorrelation in the mean of the series (via the AR component) and the changing volatility (via the GARCH component), which is particularly useful for financial time series that exhibit volatility clustering.\n\n\n\nt-Statistic Calculation\nFor each simulated time series, we aim to compute the t-statistic to test the null hypothesis that the average return (i.e.¬†the \\alpha) in the sample is zero. The t-statistic is calculated using the formula:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{\\text{SE}_{\\text{White}}}\n\\]\nwhere:\n\n\\(\\bar{x}\\) is the sample mean,\n\\(\\mu_0\\) is the population mean under the null hypothesis (in this case, 0),\n\\(\\text{SE}_{\\text{White}}\\) the White standard error of the mean:\n\n\\[\n\\text{SE}_{\\text{White}} = \\sqrt{\\frac{1}{n^2} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\]\nThis process allows us to assess the capability of MLX in handling computational tasks typically performed with NumPy and see if MLX can offer any advantages, whether in terms of speed, ease of use, or compatibility with Apple silicon. For those interested in the technical specifics or who wish to follow along with the code, all relevant materials have been made available on GitHub."
  },
  {
    "objectID": "posts/apple-mlx/index.html#getting-started-with-mlx",
    "href": "posts/apple-mlx/index.html#getting-started-with-mlx",
    "title": "Speed up your scientific computing workflows with MLX",
    "section": "Getting started with MLX",
    "text": "Getting started with MLX\nTo install MLX, you can do so using your package manager of choice:\npip install mlx\npoetry add mlx\nconda install conda-forge::mlx\nThen, you can import MLX in your Python script or Jupyter notebook:\nimport mlx.core as mx\nAnd in most cases, what you will have to do is replace np. but mx. in your code. MLX works with mx.array objects which can be converted to NumPy array by passing them to the np.array() function. Conversely, you can convert a Numpy array to a MLX array using mx.array(). It is important to note that MLX arrays support 32-bit floats, while NumPy arrays support 64-bit floats by default. This is something to keep in mind when working with MLX."
  },
  {
    "objectID": "posts/apple-mlx/index.html#simulation-with-mlx",
    "href": "posts/apple-mlx/index.html#simulation-with-mlx",
    "title": "Speed up your scientific computing workflows with MLX",
    "section": "Simulation with MLX",
    "text": "Simulation with MLX\nMy simulation code is defined as four functions, plus one additional function to calculate the t-statistic:\n\nsimulate_shocks() This function generates the error terms \\(\\epsilon_t\\) for the AR(3)-GARCH(1,1) process.\ngarch_shocks() This function computes the conditional variance \\(\\sigma_t^2\\) using the GARCH(1,1) model.\nar() This function computes the AR(3) component of the model.\nsimulate_ar3_garch11() This function combines the AR(3) and GARCH(1,1) components to simulate the time series.\ncompute_tstats_white() This function calculates the t-statistic for each simulated time series.\n\nThe full code is available on GitHub. To see the (very few) changes needed to adapt the code to MLX, let‚Äôs look at various functions:\n\nImports and simulate_shocks():\n\nnumpyMLXnumba\n\n\nimport numpy as np\n\nBURNIN = 100\n\n\ndef simulate_shocks(\n    N: int, T: int, mu: float = 0.0, sigma: float = 1.0\n) -&gt; np.ndarray:\n    return np.random.normal(mu, sigma, size=(N, T))\n\n\nimport mlx.core as mx\nimport numpy as np\n\nBURNIN = 100\n\n\ndef simulate_shocks(\n    N: int, T: int, mu: float = 0.0, sigma: float = 1.0\n) -&gt; mx.array:\n1    return mx.random.normal(loc=mu, scale=sigma, shape=[N, T])\n\n1\n\nI have to use shape instead of size.\n\n\n\n\nimport numpy as np\nfrom numba import njit, prange\n\nBURNIN = 100\n\n\n@njit\ndef simulate_shocks(\n    N: int, T: int, mu: float = 0.0, sigma: float = 1.0\n) -&gt; np.ndarray:\n    return np.random.normal(mu, sigma, size=(N, T))\n\n\n\n\n\ngarch_shocks():\n\nnumpyMLXnumba\n\n\ndef garch_shocks(\n    omega: float, alpha: float, beta: float, shocks: np.ndarray\n) -&gt; np.ndarray:\n    N, T = shocks.shape\n    sigma2 = np.zeros((N, T))\n    epsilon = np.zeros((N, T))\n    if (alpha + beta) == 1:\n        sigma2[:, 0] = omega\n    else:\n        sigma2[:, 0] = omega / (1 - alpha - beta)\n\n    for t in range(1, T):\n        sigma2[:, t] = (\n            omega + alpha * epsilon[:, t - 1] ** 2 + beta * sigma2[:, t - 1]\n        )\n        epsilon[:, t] = np.sqrt(sigma2[:, t]) * shocks[:, t]\n    return epsilon\n\n\ndef garch_shocks(\n    omega: float, alpha: float, beta: float, shocks: mx.array\n) -&gt; mx.array:\n    N, T = shocks.shape\n1    sigma2 = mx.zeros((N, T))\n    epsilon = mx.zeros((N, T))\n    if (alpha + beta) == 1:\n        sigma2[:, 0] = omega\n    else:\n        sigma2[:, 0] = omega / (1 - alpha - beta)\n\n    for t in range(1, T):\n        sigma2[:, t] = (\n            omega + alpha * epsilon[:, t - 1] ** 2 + beta * sigma2[:, t - 1]\n        )\n2        epsilon[:, t] = mx.sqrt(sigma2[:, t]) * shocks[:, t]\n    return epsilon\n\n1\n\nReplace np. with mx..\n\n2\n\nReplace np. with mx..\n\n\n\n\n@njit\ndef garch_shocks(\n    omega: float, alpha: float, beta: float, shocks: np.ndarray\n) -&gt; np.ndarray:\n    N, T = shocks.shape\n    sigma2 = np.zeros((N, T))\n    epsilon = np.zeros((N, T))\n    if (alpha + beta) == 1:\n        sigma2[:, 0] = omega\n    else:\n        sigma2[:, 0] = omega / (1 - alpha - beta)\n\n    for t in range(1, T):\n        sigma2[:, t] = (\n            omega + alpha * epsilon[:, t - 1] ** 2 + beta * sigma2[:, t - 1]\n        )\n        epsilon[:, t] = np.sqrt(sigma2[:, t]) * shocks[:, t]\n    return epsilon\n\n\n\n\n\nar():\n\nnumpyMLXnumba\n\n\ndef ar(phi1: float, phi2: float, phi3: float, shocks: np.ndarray) -&gt; np.ndarray:\n    N, T = shocks.shape\n    y = np.zeros((N, T))\n    p = 3\n    y[:, :p] = shocks[:, :p]\n    for t in range(p, T):\n        y[:, t] = (\n            y[:, t - 1] * phi1 + y[:, t - 2] * phi2 \n            + y[:, t - 3] * phi3 + shocks[:, t]\n        )\n    return y\n\n\ndef ar(phi1: float, phi2: float, phi3: float, shocks: mx.array) -&gt; mx.array:\n    N, T = shocks.shape\n    y = mx.zeros((N, T))\n    p = 3\n    y[:, :p] = shocks[:, :p]\n    for t in range(p, T):\n        y[:, t] = (\n            y[:, t - 1] * phi1 + y[:, t - 2] * phi2 \n            + y[:, t - 3] * phi3 + shocks[:, t]\n        )\n    return y\n\n\n@njit\ndef ar(phi1: float, phi2: float, phi3: float, shocks: np.ndarray) -&gt; np.ndarray:\n    N, T = shocks.shape\n    y = np.zeros((N, T))\n    p = 3\n    y[:, :p] = shocks[:, :p]\n    for t in range(p, T):\n        y[:, t] = (\n            y[:, t - 1] * phi1 + y[:, t - 2] * phi2 \n            + y[:, t - 3] * phi3 + shocks[:, t]\n        )\n    return y\n\n\n\n\n\nsimulate_ar3_garch11():\n\nnumpyMLXnumba\n\n\ndef simulate_ar3_garch11(\n    N: int,\n    T: int,\n    phi1: float,\n    phi2: float,\n    phi3: float,\n    omega: float,\n    alpha: float,\n    beta: float,\n    burnin: int = BURNIN,\n) -&gt; np.ndarray:\n    return ar(\n        phi1=phi1,\n        phi2=phi2,\n        phi3=phi3,\n        shocks=garch_shocks(\n            omega=omega,\n            alpha=alpha,\n            beta=beta,\n            shocks=simulate_shocks(N, T + BURNIN, 0.0, 1.0),\n        ),\n    )[:, burnin:]\n\n\ndef simulate_ar3_garch11(\n    N: int,\n    T: int,\n    phi1: float,\n    phi2: float,\n    phi3: float,\n    omega: float,\n    alpha: float,\n    beta: float,\n    burnin: int = BURNIN,\n) -&gt; mx.array:\n    return ar(\n        phi1=phi1,\n        phi2=phi2,\n        phi3=phi3,\n        shocks=garch_shocks(\n            omega=omega,\n            alpha=alpha,\n            beta=beta,\n            shocks=simulate_shocks(N, T + BURNIN, 0.0, 1.0),\n        ),\n    )[:, burnin:]\n\n\ndef simulate_ar3_garch11(\n    N: int,\n    T: int,\n    phi1: float,\n    phi2: float,\n    phi3: float,\n    omega: float,\n    alpha: float,\n    beta: float,\n    burnin: int = BURNIN,\n) -&gt; np.ndarray:\n    return ar(\n        phi1=phi1,\n        phi2=phi2,\n        phi3=phi3,\n        shocks=garch_shocks(\n            omega=omega,\n            alpha=alpha,\n            beta=beta,\n            shocks=simulate_shocks(N, T + BURNIN, 0.0, 1.0),\n        ),\n    )[:, burnin:]\n\n\n\n\n\ncompute_tstats_white():\n\nnumpyMLXnumba\n\n\ndef compute_tstats_white(x: np.ndarray) -&gt; np.ndarray:\n    _, m = x.shape\n    means = np.mean(x, axis=1)\n    std_devs = np.std(x, axis=1, ddof=m - 1)\n    return means / std_devs * m\n\n\ndef compute_tstats_white(x: mx.array) -&gt; mx.array:\n    n, m = x.shape\n    means = mx.mean(x, axis=1)\n1    std_devs = mx.sqrt(mx.sum(mx.square(x - means.reshape(n, 1)), axis=1))\n    return means / std_devs * m\n\n1\n\nstd() doesn‚Äôt exist in MLX, so I have to compute it manually.\n\n\n\n\n@njit(parallel=True)\ndef compute_means(x: np.ndarray) -&gt; np.ndarray:\n    n, m = x.shape\n    means = np.empty(n)\n    for i in prange(n):\n        means[i] = np.mean(x[i, :])\n    return means\n\n\n@njit(parallel=True)\ndef compute_std_devs_demeaned(x: np.ndarray) -&gt; np.ndarray:\n    n, m = x.shape\n    std_devs = np.empty(n)\n    for i in prange(n):\n        std_devs[i] = np.sqrt(np.sum(x[i, :] ** 2))\n    return std_devs\n\n\n@njit(parallel=True)\ndef compute_tstats_white(x: np.ndarray) -&gt; np.ndarray:\n    n, m = x.shape\n    means = compute_means(x)\n    std_devs = compute_std_devs_demeaned(x - means.reshape((n, 1)))\n    return means / std_devs * m"
  },
  {
    "objectID": "posts/apple-mlx/index.html#benchmark-results",
    "href": "posts/apple-mlx/index.html#benchmark-results",
    "title": "Speed up your scientific computing workflows with MLX",
    "section": "Benchmark results",
    "text": "Benchmark results\nTo compare the performance of MLX with NumPy and NumPy optimized with Numba, I ran simulations for different numbers of time series N (from 1,000 to 1,000,000) and time periods T (250 and 500.) on my MacBook Pro M3 Max with 64 GB of RAM. I measured the execution time for each library and configuration and recorded the results in a CSV file. The results from the benchmark are visualized in the following plots:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-pastel')\ndf = pd.read_csv(\"times.csv\")\ndf_250 = df[df['t'] == 250]\ndf_500 = df[df['t'] == 500]\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 3), sharey=True)\n\nsns.barplot(x='n', y='time', data=df_250, ax=ax[0], hue='library')\nax[0].set_yscale(\"log\")\nax[0].set_title('t = 250')\nax[0].set_ylabel('Execution Time (s)')\nax[0].set_xlabel('Number of Simulations')\nax[0].legend(title='Library')\n\nsns.barplot(x='n', y='time', data=df_500, ax=ax[1], hue='library')\nax[1].set_yscale(\"log\")\nax[1].set_title('t = 500')\nax[0].set_ylabel('Execution Time (s)')\nax[1].set_xlabel('Number of Simulations')\nax[0].legend(title='Library')\n\n\n/Users/vincentgregoire/Documents/GitHub/vcf/website/.venv/lib/python3.12/site-packages/seaborn/_base.py:948: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n/Users/vincentgregoire/Documents/GitHub/vcf/website/.venv/lib/python3.12/site-packages/seaborn/_base.py:948: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n/Users/vincentgregoire/Documents/GitHub/vcf/website/.venv/lib/python3.12/site-packages/seaborn/_base.py:948: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n/Users/vincentgregoire/Documents/GitHub/vcf/website/.venv/lib/python3.12/site-packages/seaborn/_base.py:948: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n/Users/vincentgregoire/Documents/GitHub/vcf/website/.venv/lib/python3.12/site-packages/seaborn/_base.py:948: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n/Users/vincentgregoire/Documents/GitHub/vcf/website/.venv/lib/python3.12/site-packages/seaborn/_base.py:948: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n\n\n\n\n\nExecution Time (shorter is better)\n\n\n\n\nAs we can see, there is a performance gain for the heavier workloads when using MLX compared to NumPy and NumPy optimized with Numba. This suggests that MLX is well-suited for handling large-scale scientific computing tasks. It is however important to keep in mind that the results may vary depending on the specific task and the hardware being used. Another thing to consider is that while the MLX code was running, CPU usage remained low, which suggests that you could potentially run GPU-bound tasks in parallel with CPU-bound tasks without much interference."
  },
  {
    "objectID": "posts/apple-mlx/index.html#final-thoughts",
    "href": "posts/apple-mlx/index.html#final-thoughts",
    "title": "Speed up your scientific computing workflows with MLX",
    "section": "Final thoughts",
    "text": "Final thoughts\nThis is certainly interesting and I am looking forward to exploring MLX further in the context of empirical finance research and other scientific computing tasks. However, while there are benefits to using MLX for heavier workloads, results will vary depending on the specific task and the hardware being used. Keep in mind that MLX is still in its early stages, and its feature set is not as extensive as that of established libraries like NumPy and PyTorch. This may limit its applicability to certain scientific computing workflows, especially those that rely heavily on specialized functionality. I also mean that MLX will likely improve over time as the library matures and more features are added, so I will be keeping an eye on its development."
  },
  {
    "objectID": "lists/econometrics.html",
    "href": "lists/econometrics.html",
    "title": "Vincent Codes Finance",
    "section": "",
    "text": "Causal Inference for The Brave and True: Online textbook on causal inference with Python.\n\n\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "intro-to-python/python-basics/index.html",
    "href": "intro-to-python/python-basics/index.html",
    "title": "Python Basics",
    "section": "",
    "text": "In this tutorial, we lay the foundation for your programming skills by exploring the basic syntax of Python.\nMy aim is to make this process as accessible as possible for non-programmers while giving you the necessary tools to excel in the world of empirical finance research.\nThe objectives of this tutorial are to:\n\nProvide a gentle introduction to the basic syntax of Python, allowing you to read and understand Python code.\nEnable you to write simple programs that will serve as building blocks for more advanced applications.\nEquip you with the knowledge and confidence to further explore advanced topics in Python and its applications in finance.\n\nBy the end of this tutorial, you will have a solid grasp of Python‚Äôs basic syntax, empowering you to use it as a versatile tool for finance-related tasks. Remember, the key to success in learning any programming language is practice. As you work through this tutorial, be sure to experiment with the examples provided and try writing your own code to reinforce your understanding.\nI am purposefully avoiding more advanced topics such as object-oriented programming, functional programming, and parallel computing. These topics are important, but they are not necessary to get started with Python. I will cover these topics in future tutorials.\nAfter completing this tutorial, you will be ready to move on to the next tutorial in this series, Introduction to data analysis in Python using pandas 2.1 (coming soon), where we will explore how to use Python to explore and study financial data.\nLet‚Äôs dive into the world of Python and begin your journey toward becoming a proficient financial empiricist."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#introduction",
    "href": "intro-to-python/python-basics/index.html#introduction",
    "title": "Python Basics",
    "section": "",
    "text": "In this tutorial, we lay the foundation for your programming skills by exploring the basic syntax of Python.\nMy aim is to make this process as accessible as possible for non-programmers while giving you the necessary tools to excel in the world of empirical finance research.\nThe objectives of this tutorial are to:\n\nProvide a gentle introduction to the basic syntax of Python, allowing you to read and understand Python code.\nEnable you to write simple programs that will serve as building blocks for more advanced applications.\nEquip you with the knowledge and confidence to further explore advanced topics in Python and its applications in finance.\n\nBy the end of this tutorial, you will have a solid grasp of Python‚Äôs basic syntax, empowering you to use it as a versatile tool for finance-related tasks. Remember, the key to success in learning any programming language is practice. As you work through this tutorial, be sure to experiment with the examples provided and try writing your own code to reinforce your understanding.\nI am purposefully avoiding more advanced topics such as object-oriented programming, functional programming, and parallel computing. These topics are important, but they are not necessary to get started with Python. I will cover these topics in future tutorials.\nAfter completing this tutorial, you will be ready to move on to the next tutorial in this series, Introduction to data analysis in Python using pandas 2.1 (coming soon), where we will explore how to use Python to explore and study financial data.\nLet‚Äôs dive into the world of Python and begin your journey toward becoming a proficient financial empiricist."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#prerequisites",
    "href": "intro-to-python/python-basics/index.html#prerequisites",
    "title": "Python Basics",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis tutorial assumes no prior knowledge of Python or programming in general. However, it is helpful to have some familiarity with basic mathematical concepts such as functions, variables, and equations. Because most of the examples in this tutorial are related to finance, it is also helpful to have some basic knowledge of finance and economics. However, this is not a requirement.\nThis tutorial does not cover the installation of Python. If you have not already installed Python, you can refer to my previous post on how to install Python 3.12."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#data-types",
    "href": "intro-to-python/python-basics/index.html#data-types",
    "title": "Python Basics",
    "section": "Data types",
    "text": "Data types\nThe Python language offers many built-in fundamental data types. These data types serve as the building blocks for working with different kinds of data, which is critical in many applications. The basic data types you should be familiar with are presented in Table¬†1.\n\n\n\nTable¬†1: Main data types in Python\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nExample\n\n\n\n\nInteger\nint\nIntegers represent whole positive and negative numbers. They are used for counting, indexing, and various arithmetic operations.\n1\n\n\nFloat-Point Number\nfloat\nFloats represent real numbers with decimals. They are used for working with financial data that require precision, such as interest rates, stock prices, and percentages.\n1.0\n\n\nComplex\ncomplex\nComplex numbers consist of real and imaginary parts, represented as a + bj. While less commonly used in finance, they may be relevant in specific advanced applications, such as signal processing or quantitative finance.\n1.0 + 2.0j\n\n\nBoolean\nbool\nBooleans represent the truth values of True and False. They are used in conditional statements, comparisons, and other logical operations.\nTrue\n\n\nText String\nstr\nStrings are sequences of characters used for storing and manipulating text data, such as stock symbols, company names, or descriptions.\n\"Hello\"\n\n\nBytes\nbytes\nBytes are sequences of integers in the range of 0-255, often used for representing binary data or encoding text. Bytes may be used when working with binary file formats or network communication.\nb\"Hello\"\n\n\nNone\nNone\nNone is a special data type representing the absence of a value or a null value. It is used to signify that a variable has not been assigned a value or that a function returns no value.\nNone\n\n\n\n\n\n\n\nLiterals\nA literal is a notation for representing a fixed value in source code. For example, 42 is a literal for the integer value of forty-two. The following are examples of literals in Python. Each code block contains code and is followed by the output of the code.\n\nint\nint literals are written as positive and negative whole numbers.\n\n42\n\n\n-99\n\nThey can also include underscores to make them more readable.\n\n1_000_000\n\n\n\nfloat\nfloat literals are written as decimal numbers.\n\n2.25\n\nThey can be written in scientific notation by using e to indicate the exponent.\n\n2.25e8\n\nTo define a whole number literal as a float instead of an int, you can append a decimal point to the number.\n\n2.0\n\n\n\ncomplex\nComplex numbers consist of a real part and an imaginary part, represented as a + bj.\n\n2.3 + 4.5j\n\n\n\nNone\nNone is a special data type that represents the absence of a value or a null value. It is used to signify that a variable has not been assigned a value or that a function returns no value.\n\nNone\n\n\n\nbool\nbool is a data type that represents the truth values of True and False. They are used in conditional statements, comparisons, and other logical operations.\n\nTrue\n\n\n\n\nstr\nStrings are sequences of characters. Strings literals are written by enclosing a sequence of characters in single or double quotes. Note that doubles quotes are preferred by the black code formatter, which is used in this book, but most Python environments will use single quotes by default when displaying strings.\n\n\"USD\"\n\nStrings are sequences of Unicode characters, which means they can represent any character in any language.\n\n\"Bitcoin  üöÄ\"\n\nString literals can span multiple lines by enclosing them in triple quotes or triple double quotes. This is useful for writing multiline strings.\n\n# Multiline strings\n\n\"\"\"GAFA is a group of companies:\n\n- Google\n- Apple\n- Facebook\n- Amazon\n\n\"\"\"\n\nMultiline strings, or any strings with special characters, can be displayed using the print function.\n\nprint(\n    \"\"\"GAFA is a group of companies:\n\n- Google\n- Apple\n- Facebook\n- Amazon\n\n\"\"\"\n)\n\n\n\nbytes\nbytes are sequences of integers in the range of 0-255. They are often used for representing binary data or encoding text. Bytes literals are written by prepending a string literal with b.\n\nb\"Hello\"\n\n\n\n\n\n\n\nCautionBytes vs strings\n\n\n\nBytes can be confused with strings, but they are not the same. Strings are sequences of Unicode characters, while bytes are sequences of integers in the range of 0-255. Bytes are often used for representing binary data or encoding text. In most cases, you will be working with strings, but you may encounter bytes when working with binary file formats or network communication."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#variables",
    "href": "intro-to-python/python-basics/index.html#variables",
    "title": "Python Basics",
    "section": "Variables",
    "text": "Variables\nA variable in Python is a named location in the computer‚Äôs memory that holds a value. It serves as a container for data, allowing you to reference and manipulate the data stored within it. Variables are created by assigning a value to a name using the assignment operator (=). They can store data of various types, such as integers, floats, strings, or even more complex data structures like lists.\nUnderstanding the concept of variables and their naming conventions will help you write clean, readable, and maintainable code. An overview of variable naming rules in Python is presented in Table¬†2, and Table¬†3 presents some examples of valid and invalid variable names.\n\n\n\nTable¬†2: Variable naming rules\n\n\n\n\n\n\n\n\n\nRule\nDescription\n\n\n\n\nCan contain letters, numbers, and underscores\nVariable names can include any combination of letters (both uppercase and lowercase), numbers, and underscores (_). Python variable names support Unicode characters, enabling you to use non-English characters in your variable names. However, they must follow the other rules mentioned below.\n\n\nCannot start with a number\nAlthough variable names can contain numbers, they must not begin with a number. For example, 1_stock is an invalid variable name, whereas stock_1 is valid.\n\n\nCannot be a reserved word\nPython has a set of reserved words (e.g., if, else, while) that have special meanings within the language. You should not use these words as variable names.\n\n\n\n\n\n\n\n\n\nTable¬†3: Variable naming examples\n\n\n\n\n\nValid\nInvalid\n\n\n\n\nticker\n1ceo\n\n\nfirm_size\n@price\n\n\ntotal_sum_2023\nclass\n\n\n_tmp_buffer\nfor\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCase-sensitive\n\n\n\nPython is case-sensitive, so ret and RET are two different variables.\n\n\nBeyond the rules mentioned above, there are also some conventions that you should follow when naming variables. These conventions are not enforced by Python, but they are widely adopted by the Python community. Table¬†4 summarizes the most common conventions.\n\n\n\nTable¬†4: Variable naming conventions\n\n\n\n\n\n\n\n\n\nConvention\nDescription\n\n\n\n\nUse lowercase letters and underscores for variable names\nTo enhance code readability, use lowercase letters for variable names and separate words with underscores. For example, market_cap is a recommended variable name, whereas MarketCap or marketCap are not. This naming convention is known as snake case.\n\n\nUse uppercase letters for constants\nConstants are values that do not change throughout the program. Use uppercase letters and separate words with underscores to differentiate them from regular variables. For example, INFLATION_TARGET is a suitable constant name. Note that Python does not support constants like other languages, so this is just a convention, but Python won‚Äôt stop you from changing the value of a constant.\n\n\n\n\n\n\nBy adhering to these guidelines, you will improve your coding style and ensure that your code is easier to understand, maintain, and collaborate on with your peers.\n\n\n\n\n\n\nTipReserved keywords\n\n\n\nReserved keywords cannot be used as variable names. You can check the complete list of reserved keywords by running the following command in the Python console:\n\nhelp(\"keywords\")\n\nNote that some reserved keywords may be confusing when thinking about finance problems. For example, return, yield, raise, global, class, and lambda are all reserved keywords, so you cannot use them as variable names. Most modern IDEs, such as Visual Studio Code, will highlight reserved keywords in a different color to help you avoid using them as variable names.\n\n\n\nDeclaring variables\nA simple way to think about variables is to consider them labels that you can use to refer to values. For example, you can create a variable x and assign it a value of 42 using the assignment operator (=). You can then use the variable x to refer to the value 42 in your code.\n\nx = 42\nx\n\n\n\n\n\n\n\nNoteThe walrus operator\n\n\n\nIn the previous example, we added x to the last line of the code to display the value of x. This is necessary in the interactive window and in Jupyer Notebooks, as they automatically display the result of the last line of the code. However, the assignment operator (=) does not return a value, so the value of x is not displayed without that last line.\n\nx = 42\n\nIntroduced in Python 3.8, the := operator, also known as the walrus operator, allows you to assign a value to a variable and return that value in a single expression. For example, you can use the walrus operator to assign a value of 10 to a variable z and use that variable in the same expression, assigning the result to y.\n\ny = (z := 10) * 2\n\nNote, however, that the walrus operator cannot be used to assign a value to a variable without using it in an expression. For example, the following code will raise an error.\n\nx := 42\n\n\n\nYou can reassign the value of a variable by assigning a new value to it. Once you reassign the value of a variable, the old value is lost. For example, you can reassign the value of x to 32 by running the following code.\n\nx = 32\nx\n\nYou can perform operations on variables, just like you would on values. For example, you can add 10 to x.\n\nx + 10\n\nYou can assign the result of an operation to a new variable. For example, you can assign the result of 2 * 10 to a new variable y.\n\ny = 2 * x\ny\n\n\nz = x + y\nz\n\nIf you try to use a variable name that is invalid, Python will raise an error. For example, if you try to assign a variable 1ceo, Python will raise an error because variable names cannot start with a number.\n\n1ceo = 2\n\nYou can, however, use Unicode characters in variable names. For example, you can use accents such as √© in a variable name.\n\ncote_de_cr√©dit = \"AAA\"\n\nA leading underscore in a variable name indicates that the variable is private, which means that it should not be accessed outside of the module or scope in which it is defined. For example, you can use a leading underscore in a variable name to indicate that the variable is private. This is a convention that is widely adopted by the Python community, but it is not enforced by Python.\n\n_hidden = 30_000\n\nAnother convention is to use all caps for constants. For example, you can use all caps to indicate that INFLATION_TARGET is a constant.\n\nINFLATION_TARGET = 0.02\n\nPython will raise an error if you attempt to use a variable that has not been declared. For instance, if you try to use the variable inflation_target instead of INFLATION_TARGET, Python will generate an error. It‚Äôs important to note that Python is case-sensitive, so variables must be referenced with the exact casing as their declaration.\n\ninflation_target\n\n\n\nVariable types\nPython is a dynamically typed language, meaning you do not need to specify the variable type when you declare it. Instead, Python will automatically infer the type of a variable based on the value you assign to it. For example, if you assign an integer value to a variable, Python will infer that the variable is an integer. Similarly, if you assign a string value to a variable, Python will infer that the variable is a string. You can use the type() function to check the type of a variable. For example, you can check the type of a by running the following code.\n\na = 3.3\ntype(a)\n\n\nb = 2\ntype(b)\n\n\nmarket_open = True\ntype(market_open)\n\n\ncurrency = \"CAD\"\ntype(currency)\n\n\n\n\n\n\n\nTipVariables explorer in Visual Studio Code\n\n\n\nVS Code has a built-in variable explorer that allows you to view the variables in your workspace when using the interactive window or a Jupyer Notebook. You can open the Variables View by clicking on the Variables button in the top toolbar of the editor:\n\n\n\nVariable View button\n\n\nThe Variables View will appear at the bottom of the window, showing the variables in your workspace, along with their values, types, and size for collections. For example, the following screenshot shows the variables in the workspace after running the code in this section:\n\n\n\nVariable View\n\n\n\n\n\nConverting between types\nYou can convert a variable from one type to another using the built-in functions float(), int(), str(), and bool(). For example, you can convert the variable x, which is currently an int, to a float by running the following code.\n\nfloat(x)\n\nThe same way, you can convert the variable y, which is currently a float, to an int by running the following code. Note that the int() function will round down the value of y to the nearest integer.\n\nint(a)\n\nSimilarly, you can convert the variable x to a string by running the following code.\n\nstr(x)\n\nYou can convert a string to an integer or a float if the string contains a valid representation of a number. For example, you can convert the string \"42\" to an integer by running the following code.\n\nint('42')\n\nHowever, you cannot convert a string that does not contain a valid representation of a number to an integer. For example, you cannot convert the string \"42.5\" to an integer.\n\nint('42.5')\n\nWhen converting to a boolean value, most values will be converted to True, except for 0, 0.0, and \"\", which will be converted to False.\n\nbool(0)\n\n\nbool(1)\n\n\nbool(\"\")\n\n\nbool(\"33\")\n\nThe None value is a special type in Python that represents the absence of a value. You can use the None value to initialize a variable without assigning it a value. For example, you can initialize a variable problem to None by running the following code.\n\nproblem = None\ntype(problem)"
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#comments",
    "href": "intro-to-python/python-basics/index.html#comments",
    "title": "Python Basics",
    "section": "Comments",
    "text": "Comments\nComments are an essential part of writing clear, maintainable code. They help explain the purpose, logic, or any specific details of the code that might not be obvious at first glance. However, excessive or unnecessary commenting can clutter your code and make it harder to read. To strike the right balance, consider the guidelines listed in Table¬†5 when deciding when to use comments and when to avoid them:\n\n\n\nTable¬†5: Guidelines for comments\n\n\n\n\n\n\n\n\n\nGuideline\nDescription\n\n\n\n\nUse comments when the code is complex or non-obvious\nWhen your code involves complex algorithms, calculations, or logic that may be difficult for others (or yourself) to understand at a glance, use comments to explain the reasoning behind the code or to provide additional context.\n\n\nAvoid comments for simple or self-explanatory code\nFor code that is simple, clear, and easy to understand, avoid adding comments. Instead, use descriptive variable and function names that convey the purpose of the code.\n\n\nUse comments to explain the ‚Äòwhy‚Äô, not the ‚Äòhow‚Äô\nGood comments explain the purpose of a piece of code or the reasoning behind a decision. Focus on providing context and insight that isn‚Äôt immediately apparent from reading the code. Avoid repeating what the code is doing, as this can be redundant and clutter the code.\n\n\nAvoid commenting out large blocks of code\nInstead of leaving large blocks of commented-out code in your final version, remove them. It‚Äôs better to use version control systems like Git to keep track of previous versions of your code.\n\n\nKeep comments up-to-date\nEnsure that your comments are always up-to-date with the code they describe. Outdated comments can be confusing and misleading, making it harder to understand the code.\n\n\nUse comments to provide additional information\nUse comments to provide references to external resources, such as links to relevant documentation, papers, or articles. This can be helpful for providing additional context or background information related to the code.\n\n\nUse consistent commenting style\nFollow a consistent commenting style throughout your codebase. This makes it easier for others to read and understand your comments.\n\n\n\n\n\n\n\nWriting comments\nIn Python, comments are created using the # symbol. Any text that follows the # symbol on the same line is ignored by the Python interpreter. Comments can be placed on a separate line or at the end of a line of code.:\n\n# This is a single-line comment\n\nprice = 150  # This is an inline comment\n\nYou can also create multi-line comments by enclosing the text in triple quotes (\"\"\" or '''). Multi-line comments are often used to provide docstrings (documentation strings) for functions and classes. We‚Äôll learn more about functions and classes in a later section. Note that multi-line comments are technically strings, but the Python interpreter ignores them and does not store them in memory because they are not assigned to a variable.\n\n\"\"\"\nThis is a multi-line comment.\nYou can write your comments across multiple lines.\n\"\"\"\n\nComments can occur alongside code to document its purpose or explain the logic.\n\n# Calculate compound interest\nprincipal = 1000  # Principal amount\nrate = 0.05  # Annual interest rate\ntime = 5  # Time in years\n\n# Future value with compound interest formula\nfuture_value = principal * (1 + rate) ** time\n\n# Display the result\nprint(f\"Future value: {future_value:.2f}\")\n\n\n\n\n\n\n\nTipDon‚Äôt overdo it\n\n\n\nComments are useful for providing additional context or explanation, but they can also be overdone. Avoid adding comments for trivial or self-explanatory code. For example, the code above is simple and clear enough to understand without comments, so adding comments is decreasing readability instead of improving it.\n\n\nComments are usually written in English, but you can use any language as long as the file is UTF-8 encoded. You can also use emojis in comments if you like üòä."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#numbers",
    "href": "intro-to-python/python-basics/index.html#numbers",
    "title": "Python Basics",
    "section": "Numbers",
    "text": "Numbers\nPython provides built-in functions and operators to perform mathematical operations on numbers. Some commonly used mathematical functions include abs(), round(), min(), max(), and pow(). Additionally, Python‚Äôs math library offers more advanced functions like trigonometry and logarithms.\n\n\n\n\n\n\nWarningRounding errors\n\n\n\nFloating-point numbers may be subject to rounding errors due to the limitations of their binary representation. Keep this in mind when comparing or performing calculations with floats. Consider using the Decimal data type from Python‚Äôs decimal library to avoid floating-point inaccuracies when dealing with high-precision financial data.\n\n\n\n\n\n\n\n\nNotePerformance\n\n\n\nWhen working with large datasets or performing complex calculations, consider using third-party libraries like NumPy and pandas, which are covered in later chapters, for improved performance and additional functionality.\n\n\n\nOperations\nThe Python language supports many mathematical operations. Table¬†6 lists some of the most commonly used operators.\n\n\n\nTable¬†6: Basic Arithmetic Operations\n\n\n\n\n\nOperator\nName\nExample\nResult\n\n\n\n\n+\nAddition\n1 + 2\n3\n\n\n-\nSubtraction\n1 - 2\n-1\n\n\n*\nMultiplication\n3 * 4\n12\n\n\n/\nDivision\n1 / 2\n0.5\n\n\n**\nExponentiation\n2 ** 3\n8\n\n\n//\nFloor division\n14 // 3\n4\n\n\n%\nModulo (remainder)\n14 % 3\n2\n\n\n\n\n\n\n\na = 5\nb = 3\n\nprint(f\"Addition: a + b = {a + b}\")\nprint(f\"Subtraction: a - b = {a - b}\")\nprint(f\"Multiplication: a * b = {a * b}\")\nprint(f\"Division: a / b = {a / b}\")\nprint(f\"Exponentiation: a ** b = {a ** b}\")\nprint(f\"Floor Division: a // b = {a // b}\")\nprint(f\"Modulo: a % b = {a % b}\")\n\n\n\n\n\n\n\nNotef-strings\n\n\n\nThe previous examples use a special type of strings called f-strings to format the output. f-strings are a convenient way to embed variables and expressions inside strings. They are denoted by the f prefix and curly braces ({}) containing the variable or expression to be evaluated.\nWe cover f-strings in more details in Section¬†8.2.\n\n\n\n\nCommon mathematical functions\nTo round numbers, use the round() function. The round() function takes two arguments: the number to be rounded and the number of decimal places to round to. The number is rounded to the nearest integer if the second argument is omitted.\n\nrounded_num = round(5.67, 1)\n\nprint(rounded_num)\nprint(type(rounded_num))\n\n\nrounded_to_int = round(5.67)\n\nprint(rounded_to_int)\nprint(type(rounded_to_int))\n\nSome mathematical functions will require the use of the math module from the Python Standard Library. The standard library is a collection of modules included with every Python installation. You can use the functions and types in these modules by importing them into your code using the import statement.\nFor example, to calculate the square root of a number, you can use the sqrt() function from the math module:\n\n1import math\n\nmath.sqrt(25)\n\n\n1\n\nImports the math module, making its functions available in the current code.\n\n\n\n\nThis is only one of the many functions in the math module. You can view the complete list of functions in the module documentation. The math module also contains constants like pi and e, which you can access using the dot notation.\n\nmath.pi\n\n\n\nRandom numbers\nIt is often useful to generate random numbers for simulations and other applications. Python‚Äôs random module provides functions for generating pseudo-random1 numbers from different distributions.\n\n\n\n\n\n\nImportantPseudo-random number generator\n\n\n\nThe random module uses the Mersenne Twister algorithm to generate pseudo-random numbers. This algorithm is deterministic, meaning that given the same seed value, it will produce the same sequence of numbers every time. This is useful for debugging and testing but not for security purposes. If you need a cryptographically secure random number generator, use the secrets module instead.\n\n\nThe random.seed() function initializes the pseudo-random number generator. If you do not call this function, Python will automatically call it the first time you generate a random number. The random.seed() function takes an optional argument that can be used to set the seed value. This can be useful for debugging and testing, allowing you to generate the same sequence of random numbers every time. If you do not specify a seed, Python will use the system time as the seed value, so you will get a different sequence of random numbers every time.\n\nimport random\n\n1random.seed(42)\n\n\n1\n\nSets the seed value to 42. Why 42? Because it‚Äôs the answer to life, the universe, and everything.\n\n\n\n\nrandom.random() generates a random float between 0 and 1 (exclusive).\n\nrand_num = random.random()\n\nrand_num\n\nrandom.randint(a, b) generates a random integer between a and b (inclusive).\n\nrand_int = random.randint(1, 10)\n\nrand_int\n\nrandom.uniform(a, b) generates a random float between a and b (exclusive).\n\nrand_float = random.uniform(0, 1)\n\nrand_float\n\nrandom.normalvariate(mu, sigma) generates a random float from a normal distribution with mean mu and standard deviation sigma.\n\nrand_norm = random.normalvariate(0, 1)\n\nrand_norm\n\nThe full list of functions in the random module can be found in the module documentation.\n\n\nFloats and decimals\nBecause of the way computers store numbers, floating-point numbers are not exact. This can lead to unexpected results when performing arithmetic operations on floats.\n\n2.33 + 4.44\n\nTo avoid this problem when exact results are needed, use the Decimal type from the decimal module to perform arithmetic operations on decimal numbers. You could import the module using import decimal but this would require you to prefix all the functions and types in the module with decimal. To avoid this, you can directly import the Decimal type from the decimal module using from decimal import Decimal.\n\n1from decimal import Decimal\n\nDecimal(\"2.33\") + Decimal(\"4.44\")\n\n\n1\n\nImports the Decimal type from the decimal module. You can now refer to the Decimal type directly without having to prefix it with decimal.\n\n\n\n\n\n\n\n\n\n\nNoteDecimals vs.¬†floats\n\n\n\nUsing the Decimal type in Python provides precise decimal arithmetic and avoids rounding errors, making it suitable for financial and monetary calculations, while floats offer faster computation and are more memory-efficient but can introduce small inaccuracies due to limited precision and binary representation.\n\n\n\n\nFinancial formulas\nMany financial calculations involve performing arithmetic operations on financial data. Here are two examples of common calculations in finance and how they can be implemented in Python.\n\nCalculating the present value of a future cash flow\nThe formula for calculating the present value of a future cash flow is:  PV = \\frac{FV_t}{(1 + r)^t},  where FV_t is the future value of the cash flow at time t, r is the discount rate, and t is the number of periods.\n\nfuture_value = 1000\ndiscount_rate = 0.05\nperiods = 5\n\npresent_value = future_value / (1 + discount_rate) ** periods\n\npresent_value\n\n\n\nCalculating the future value of an annuity\nThe formula for calculating the future value of an annuity is:  FV = PMT \\frac{(1 + r)^t - 1}{r},  where PMT is the payment, r is the interest rate, and t is the number of periods.\nIt can be written in Python as:\n\npayment = 100\nrate = 0.05\nperiods = 5\n\nfuture_value_annuity = payment * ((1 + rate) ** periods - 1) / rate\n\nfuture_value_annuity\n\n\n\n\n\n\n\nTipParentheses and operator precedence\n\n\n\nPython, just like mathematics, follows a specific order of operations when evaluating expressions. The complete list of precedence rules can be found in the Python documentation.\nWhen in doubt, use parentheses to make the order of operations explicit.\nFor arithmetic operations, the order of operations is as follows:\n\nExponents\nNegative (-)\nMultiplication and division\nAddition and subtraction"
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#sec-defining-functions",
    "href": "intro-to-python/python-basics/index.html#sec-defining-functions",
    "title": "Python Basics",
    "section": "Defining functions",
    "text": "Defining functions\nFunctions are blocks of organized and reusable code that perform a specific action. They allow you to encapsulate a set of instructions, making your code modular and easier to maintain. Functions can take input parameters, perform operations on those inputs, and return a result.\nDefining a function in Python involves the following steps:\n\nUse the def keyword: Start by using the def keyword, followed by the function name and parentheses that enclose any input parameters.\nAdd input parameters: Specify any input parameters within the parentheses, separated by commas. These parameters allow you to pass values to the function, which it can then use in its calculations or operations.\nWrite the function body: After the parentheses, add a colon (:) and indent the following lines to create the function body. This block of code contains the instructions that the function will execute when called.\nReturn a result (optional): If your function produces a result, use the return statement to send the result back to the caller. If no return statement is specified, the function will return None by default.\n\n\n\n\n\n\n\nTipBest practices\n\n\n\nWhen defining functions, keep the following best practices in mind:\n\nChoose descriptive function names: Use meaningful names that reflect the purpose of the function, making your code more readable and easier to understand.\nKeep functions small and focused: Each function should have a single responsibility, making it easier to test, debug, and maintain.\n\n\n\nWe can define functions to perform a wide variety of tasks. For example, we can define a function to calculate the present value of a future cash flow:\n\n1def present_value(future_value, discount_rate, periods):\n2    return future_value / (1 + discount_rate) ** periods\n\n\n# Example usage:\nfuture_value = 1000\ndiscount_rate = 0.05\nperiods = 5\n3result = present_value(future_value, discount_rate, periods)\nprint(f\"Present value: {result:.2f}\")\n\n\n1\n\nDefines a function called present_value that takes three input parameters: future_value, discount_rate, and periods.\n\n2\n\nCalculates the present value of a future cash flow using the formula from the previous section and returns the result to the caller. The code in the function body is indented to indicate that it is part of the function.\n\n3\n\nCalls the present_value function with the specified input values and stores the returned value in a variable called result. When the function is called, the input values are passed to the function as arguments in the same order as the parameters were defined. The function body is then executed, and the result is returned to the caller.\n\n\n\n\n\n\n\n\n\n\nNoteIndentation\n\n\n\nIndentation refers to the spaces or tabs used at the beginning of a line to organize code. It helps Python understand the program‚Äôs structure and which lines of code are grouped together.\nThe Python language specification mandates the use of consistent indentation for code readability and proper execution. Indentation is typically achieved using four spaces per level. It plays a crucial role in determining the scope and hierarchy of statements within control structures, such as loops and conditional statements. For example, the statements that are part of a function body must be indented to indicate that they are part of the function. The Python interpreter knows that the function body ends when the indentation level returns to the previous level.\n\n\nWe will learn more about functions in Section¬†12."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#sec-strings",
    "href": "intro-to-python/python-basics/index.html#sec-strings",
    "title": "Python Basics",
    "section": "Strings",
    "text": "Strings\nText data is often encountered in finance in the form of stock symbols, company names, descriptions, or financial reports. Understanding how to work with strings is essential for processing and manipulating text data effectively.\nStrings are sequences of characters, and they can be created using single quotes (' '), double quotes (\" \"), or triple quotes (''' ''' or \"\"\" \"\"\") for multi-line strings.\n\n\n\n\n\n\nNoteSpecial characters\n\n\n\nSome characters have special meanings in Python strings. The backslash (\\) is used to escape characters that have special meaning, such as newline (\\n) or tab (\\t). To include a backslash in a string, you need to escape it by adding another backslash before it (\\\\). Alternatively, you can use raw strings by prefixing the strings with r or R, which will treat backslashes as literal characters. For example, these two strings are equivalent:\nstr1 = \"C:\\\\Users\\\\John\"\nstr2 = r\"C:\\Users\\John\"\n\n\n\nString operations\nThe Python language provides many common string operations. Table¬†7 lists some of the most commonly used operations.\n\n\n\nTable¬†7: Common string operations\n\n\n\n\n\n\n\n\n\n\nOperation\nExample\nDescription\n\n\n\n\nConcatenate strings\nresult = str1 + \" \" + str2\nCombines two or more strings together\n\n\nRepeat strings\nresult = repeat_str * 3\nRepeats a string a specified number of times\n\n\nLength of a string\nlength = len(text)\nGets the length (number of characters) of a string\n\n\nAccess characters in a string\nfirst_char = text[0]\nRetrieves a specific character in a string\n\n\nSlice a string\nslice_text = text[0:12]\nExtracts a part of a string\n\n\nConvert case\nupper_text = text.upper()\nConverts a string to uppercase\n\n\n\nlower_text = text.lower()\nConverts a string to lowercase\n\n\nJoin a list of strings\ntext = \", \".join(companies)\nJoins a list of strings using a delimiter\n\n\nSplit a string\ncompanies = text.split(\", \")\nSplits a string into a list based on a delimiter\n\n\nReplace a substring\nnew_text = text.replace(\"Finance\", \"Python\")\nReplaces a specified substring in a string\n\n\nCheck substring existence\nresult = substring in text\nChecks if a substring exists in a string\n\n\n\n\n\n\nWe can concatenate (combine) two or more strings into a single string using the + operator.\n\nstr1 = \"Hello\"\nstr2 = \"World\"\nresult = str1 + \" \" + str2\nprint(result)\n\nThe * operator repeats a string multiple times.\n\nrepeat_str = \"Python \"\nresult = repeat_str * 3\nprint(result)\n\nThe len() function returns the string‚Äôs length (number of characters).\n\ntext = \"Finance\"\nlength = len(text)\nprint(length)\n\nSingle characters in a string can be accessed using the index of the character within square brackets ([]). Python uses zero-based indexing, so the first character in a string has index 0, the second character has index 1, and so on. You can also use negative indices to access characters from the end of a string, with the last character having index -1, the second last character having index -2, and so on.\n\ntext = \"Python\"\nfirst_char = text[0]\nlast_char = text[-1]\nprint(first_char)\nprint(last_char)\n\nExtracting a portion of a string by specifying a start and end index is called slicing. In Python, you can slice a string using the following syntax: text[start:end]. The start index is inclusive, while the end index is exclusive. If the start index is omitted, it defaults to 0. If the end index is omitted, it defaults to the length of the string.\n\ntext = \"empirical finance Python\"\nslice_text = text[0:7]\nprint(slice_text)\n\nThe upper() and lower() methods convert a string to uppercase or lowercase, respectively.\n\ntext = \"Finance\"\nupper_text = text.upper()\nlower_text = text.lower()\nprint(upper_text)\nprint(lower_text)\n\n\n\n\n\n\n\nNoteMethods vs functions\n\n\n\nA method is similar to a function but associated with a specific object or data type. In this case, upper() and lower() are methods specific to the str (string) data type. When we call the upper method on the text object using the dot notation (text.upper()), Python knows to transform the string stored in the text variable. Methods are particularly useful because they allow us to perform actions or operations specific to the object or data type they belong to, and they improve code readability by making it clear what object the method is being called on.\n\n\nThe join() method joins a list of strings into a single string using a delimiter. The delimiter can be specified as an argument to the join() method. Lists are introduced in the next section.\n\ncompanies = [\"Apple\", \"Microsoft\", \"Google\"]\ntext = \" | \".join(companies)\nprint(text)\n\nThe split() method splits a string into a list of substrings based on a delimiter. The delimiter can be specified as an argument to the split() method. If no delimiter is specified, the string is split on whitespace characters.\n\ntext = \"Apple, Microsoft, Google\"\ncompanies = text.split(\", \")\nprint(companies)\n\nThe replace() method replaces a substring in a string with another string. It takes two arguments: the substring to replace and the string to replace it with.\n\ntext = \"Introduction to Finance\"\nnew_text = text.replace(\"Finance\", \"Python\")\nprint(new_text)\n\nThe in operator checks if a substring exists in a string. It returns a boolean value, True if the substring exists in the string, and False otherwise.\n\ntext = \"Introduction to Python\"\nsubstring = \"Python\"\nresult = substring in text\nprint(result)\n\nThe Python documentation provides a complete list of string methods that you can refer to for more details.\n\n\nFormatting strings\nYou will often encounter situations where you must present or display data in a formatted, human-readable manner. F-strings are a powerful tool for formatting strings and embedding expressions or variables directly within the string. They provide a concise and easy-to-read way of formatting strings, making them an essential tool for working with text data.\nF-strings, also known as ‚Äúformatted string literals,‚Äù allow you to embed expressions, variables, or even basic arithmetic directly into a string by enclosing them in curly braces {} within the string. The expressions inside the curly braces are evaluated at runtime and then formatted according to the specified format options.\nSome key features of f-strings that are useful include:\n\nExpression Evaluation: You can embed any valid Python expression within the curly braces, including variables, arithmetic operations, or function calls. This feature enables you to generate formatted strings based on your data dynamically.\nFormatting Options: F-strings support various formatting options, such as alignment, width, precision, and thousand separators. These options can be specified within the curly braces after the expression, separated by a colon (:).\nFormat Specifiers: You can use format specifiers to control the display of numbers, such as specifying the number of decimal places, using scientific notation, or adding a percentage sign. Format specifiers are especially useful in finance when working with currency, percentages, or large numbers.\n\nTo create an f-string, prefix the string with an f character, followed by single or double quotes. You can then embed expressions or variables within the string by enclosing them in curly braces ({}). For example, this lets you concatenate strings and variables together in a single statement:\n\nticker = \"AAPL\"\nexchange = \"NASDAQ\"\ncompany_name = \"Apple, Inc.\"\nfull_name = f\"{company_name} ({exchange}:{ticker})\"\nprint(full_name)\n\nPython will convert the expression within the curly braces to a string, which can be used to convert numbers to strings.\n\nnum = 42\nnum_str = f\"{num}\"\nprint(num_str)\n\nPython evaluates the expression within the curly braces at runtime and then formats the string according to the specified format options. For example, you can use the :,.2f format option to display a number with a thousand separator and two decimal places.\n\namount = 12345.6789\nformatted_amount = f\"${amount:,.2f}\"\nprint(formatted_amount) \n\nYou can also use the :.2% format option to display a number as a percentage with two decimal places.\n\nrate = 0.05\nformatted_rate = f\"{rate:.2%}\"\nprint(formatted_rate) \n\nThe datetime module provides a datetime class to represent dates and times. The datetime class has a now() method that returns the current date and time. You can use the :%Y-%m-%d format option to display the date in YYYY-MM-DD format.\n\nfrom datetime import datetime\n\ncurrent_date = datetime.now()\nformatted_date = f\"{current_date:%Y-%m-%d}\"\nprint(formatted_date)\n\nYou can also use f-strings to align text to the left (&lt;), right (&gt;), or center (^) within a fixed-width column:\n\nticker = \"AAPL\"\nprice = 150.25\nchange = -1.25\n\n1formatted_string = f\"|{ticker:&lt;10}|{price:^10.2f}|{change:&gt;10.2f}|\"\nprint(formatted_string)\n\n\n1\n\nThe :&lt;10 format option aligns the text to the left within a 10-character column. The :^10.2f format option aligns the number to the center within a 10-character column and displays it with two decimal places. The :&gt;10.2f format option aligns the number to the right within a 10-character column and displays it with two decimal places.\n\n\n\n\nMultiline f-strings work the same way as multiline strings, except that they are prefixed with an f character. You can use multiline f-strings to create formatted strings that span multiple lines.\n\nstock = \"AAPL\"\nprice = 150.25\nchange = -1.25\n\nformatted_string = f\"\"\"\nStock:  \\t{stock}\nPrice:  \\t${price:.2f}\nChange: \\t${change:.2f}\n\"\"\"\nprint(formatted_string)\n\n\n\n\n\n\n\nNoteAlternative formatting methods\n\n\n\nWhen reading code or answers on websites such as Stack Overflow or receiving suggestions from AI-assisted coding assistant, you may encounter other string formatting methods. Before f-strings, the two primary string formatting methods in Python were %-formatting and str.format().\n%-formatting\nAlso known as printf-style formatting, %-formatting uses the % operator to replace placeholders with values. Inspired by the printf function in C, it has been available since early versions of Python. It is less readable and more error-prone than other methods.\nExample:\nformatted_string = \"%s has a balance of $%.2f\" % (name, balance)\nstr.format()\nThe str.format() method embeds placeholders using curly braces {} and replaces them with the format() method. Introduced in Python 2.6, it offers improved readability and more advanced formatting options compared to %-formatting.\nExample:\nformatted_string = \"{} has a balance of ${:,.2f}\".format(name, balance)\nAdvantages of f-strings\nI recommend using f-strings instead of %-formatting or str.format() for string formatting for the following reasons:\n\nReadability: Concise syntax with expressions and variables embedded directly.\nFlexibility: Supports any valid Python expression within curly braces.\nPerformance: Faster than other methods, evaluated at runtime.\nSimplicity: No need to specify variable order or maintain separate lists."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#sec-collections",
    "href": "intro-to-python/python-basics/index.html#sec-collections",
    "title": "Python Basics",
    "section": "Collections",
    "text": "Collections\nSequences and collections are fundamental data structures in Python that allow you to store and manipulate multiple elements in an organized manner. They differ along three dimensions: order, mutability, and indexability. An ordered collection is one where the elements are stored in a particular order, the order of the elements is important, and you can iterate over the elements in that order. A collection is mutable if you can add, remove, or modify elements, after it is created. A collection is indexable if you can refer to its elements by their index (position or key).\nTable¬†8 presents the main types of sequences and collections in Python. You are already familiar with the string type, an ordered, immutable, and indexable sequence of characters.\n\n\n\nTable¬†8: Sequences and collections in Python\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nExample\n\n\n\n\nList\nlist\nOrdered, mutable, and indexed. Allows duplicate members.\n[1, 2, 3]\n\n\nTuple\ntuple\nOrdered, immutable, and indexed. Allows duplicate members.\n(1, 2, 3)\n\n\nSet\nset\nUnordered, mutable, and unindexed. No duplicate members.\n{1, 2, 3}\n\n\nDictionary\ndict\nUnordered, mutable, and indexed. No duplicate index entries. Elements are indexed according to a key.\n{\"a\": 1, \"b\": 4}\n\n\nString\nstring\nOrdered, immutable, and indexed. Allows duplicate characters.\n\"abc\"\n\n\n\n\n\n\n\nLists\nLists in Python are ordered collections of items that can hold different data types. They are mutable, meaning that elements can be added, removed, or modified. Lists are versatile and commonly used to store and manipulate sets of related data. The elements within a list are accessed using indexes, which allow for easy retrieval and modification. Lists also support various built-in methods and operations for efficient data manipulation, such as appending, extending, sorting, and slicing.\nA list is created by enclosing a comma-separated sequence of elements within square brackets ([ ]). The elements can be of any data type, including other lists. The following code snippet creates a list of strings and a list of integers.\n\nstocks = [\"AAPL\", \"GOOG\", \"MSFT\"]\nprices = [150.25, 1200.50, 250.00]\n\nYou can access the elements of a list using their index. The index of the first element is 0, the index of the second element is 1, and so on. You can also use negative indexes to access elements from the end of the list. The index of the last element is -1, the index of the second to last element is -2, and so on. The following code snippet illustrates how to access the elements of the stocks and prices lists.\n\nfirst_stock = stocks[0]\nprint(first_stock)\n\nlast_price = prices[-1]\nprint(last_price)\n\nYou can replace the elements of a list by assigning new values to their indexes, add new elements to the list using the append() method, or delete elements from the list using the remove() method.\n\n# Replace an element\nstocks[1] = \"GOOGL\"\nprint(stocks)\n\n# Adding an element to the list\nstocks.append(\"AMZN\")\nprint(stocks)\n\n# Removing an element from the list\nstocks.remove(\"MSFT\")\nprint(stocks)\n\nYou can also use the len() function to get the length of a list, and the in operator to check if an element is present in a list.\n\n# Length of the list\nlist_length = len(stocks)\nprint(f\"Length: {list_length}\")\n\n# Checking if an element is in the list\nis_present = \"AAPL\" in stocks\nprint(f\"Is AAPL in the list? {is_present}\")\n\n\n\nTuples\nTuples are ordered collections of elements that are immutable, meaning they cannot be modified after creation. They are typically used to store related pieces of data as a single entity, and their immutability provides benefits such as ensuring data integrity and enabling safe data sharing across different parts of a program.\n\n\n\n\n\n\nNoteTuples vs lists\n\n\n\nTuples and lists in Python differ in mutability, syntax, and use cases. Tuples are commonly used for fixed data, have a slight performance advantage over lists and can be more memory-efficient. Lists are commonly used for variable data, and provide more flexibility in terms of operations and methods.\n\n\nA tuple is created by enclosing a comma-separated sequence of elements within parentheses (( )). The elements can be of any data type, including other tuples. The following code snippet creates a tuple of integers and a tuple of strings.\n\nmu = 0.1\nsigma = 0.2\ntheta = 0.5\n\nparameters = (mu, sigma, theta)\nprint(parameters)\n\nYou can access the elements of a tuple using their index, find their length using len(), just like with lists.\n\n# Accessing elements in a tuple\nsigma0 = parameters[1]\nprint(sigma0)\n\n# Length of the tuple\ntuple_length = len(parameters)\nprint(f\"Length: {tuple_length}\")\n\nTuples are immutable, so you cannot add, remove, or replace their elements directly. You can, however, create a new tuple with the modified elements. Also note that you can modify mutable elements within a tuple, such as a list.\n\na = [1, 2, 3]\nb = (\"c\", a, 2)\nprint(f\"Before appending to list a: {b}\")\n\na.append(4)\nprint(f\"After appending to list a: {b}\")\n\nb still contains the same elements, but the list a within the tuple has been modified.\n\nTuple unpacking\nTuple unpacking is a powerful feature of Python that allows you to assign multiple variables from the elements of a tuple in a single line of code. It is a form of ‚Äúdestructuring assignment‚Äù that provides a concise way to extract the elements of a tuple into individual variables.\nTo perform tuple unpacking, you use a sequence of variables on the left side of an assignment statement, followed by a tuple on the right side. When the assignment is made, each variable on the left side will be assigned the corresponding value from the tuple on the right side.\nHere is an example:\n\n# Create a tuple\nt = (1, 2, 3)\n\n# Unpack the tuple into three variables\na, b, c = t\n\n# Display the values of the variables\nprint(f\"a: {a}, b: {b}, c: {c}\")\n\nIn this example, the tuple t contains three elements: 1, 2, and 3. When the tuple is unpacked into the variables a, b, and c, each variable gets assigned the corresponding value from the tuple: a gets 1, b gets 2, and c gets 3.\nTuple unpacking can be useful in various situations. For example, when working with functions that return multiple values as a tuple, you can use tuple unpacking to assign the return values to individual variables. Here‚Äôs an example:\n\n# Define a function that returns a tuple\ndef get_top3_stocks():\n    return (\"AAPL\", \"MSFT\", \"AMZN\")\n\n# Unpack the returned tuple into three variables\nstock1, stock2, stock3 = get_top3_stocks()\n\n# Display the values of the variables\nprint(f\"Largest: {stock1}, 1nd: {stock2}, 3rd: {stock3}\")\n\nNote that the number of variables on the left side of the assignment must match the number of elements in the tuple being unpacked.\n\n\n\nSets\nSets are unordered collections of unique elements. They are defined using curly braces { } or the set() constructor. Sets do not allow duplicate values and support various operations such as intersection, union, and difference. Sets are commonly used for tasks like removing duplicates from a list, membership testing, and mathematical operations on distinct elements.\n\n# Creating a set\nunique_numbers = {1, 2, 3, 2, 1}\nprint(unique_numbers)\n\n# Adding an element to the set\nunique_numbers.add(4)\nprint(f\"Added 4: {unique_numbers}\")\n\n# Removing an element from the set\nunique_numbers.remove(1)\nprint(f\"Removed 1: {unique_numbers}\")\n\n# Checking if an element is in the set\nis_present = 2 in unique_numbers\nprint(f\"Is 2 in the set? {is_present}\")\n\n# Length of the set\nset_length = len(unique_numbers)\nprint(f\"Length: {set_length}\")\n\nSets support operations such as intersection, union, and difference, which are performed using the &, |, and - operators respectively.\n\nset1 = {1, 2, 3, 4}\nset2 = set([3, 4, 5, 6])\n\n# Intersection\nprint(f\"Intersection: {set1 & set2}\")\n\n# Union\nprint(f\"Union: {set1 | set2}\")\n\n# Difference\nprint(f\"Difference: {set1 - set2}\")\n\n\n\n\n\n\n\nNoteSets and data types\n\n\n\nSets can contain elements of different data types, including numbers, strings, and tuples. However, sets only support immutable elements, so you cannot add lists or dictionaries to a set.\n\n\n\n\nDictionaries\nDictionaries are key-value pairs that provide a way to store and retrieve data using unique keys. They are defined with curly braces { } like sets, but contain pairs of elements called items, where each item is a key-value pair separated by a colon (:).\nDictionaries are unordered and mutable, allowing for efficient data lookup and modification. They are commonly used for mapping and associating values with specific keys, making them useful for tasks like storing settings, organizing data, or building lookup tables.\n\nstock_prices = {\"AAPL\": 150.25, \"GOOGL\": 1200.50, \"MSFT\": 250.00}\nprint(stock_prices)\n\nYou access the value for a specific key using square brackets [ ], and modify the value for a key using the assignment operator =. You add new key-value pairs to a dictionary using a new key and assignment operator and remove a key-value pair using the del keyword. The len() function returns the number of key-value pairs in a dictionary.\n\n# Accessing elements in a dictionary\nprice_aapl = stock_prices[\"AAPL\"]\nprint(f\"Price for AAPL: {price_aapl:0.2f}\")\n\n# Modifying an element\nstock_prices[\"GOOGL\"] = 1205.00\nprint(f\"Modified GOOGL: {stock_prices}\")\n\n# Adding a new element to the dictionary\nstock_prices[\"AMZN\"] = 3300.00\nprint(f\"Added AMZN: {stock_prices}\")\n\n# Removing an element from the dictionary\ndel stock_prices[\"MSFT\"]\nprint(f\"Removed MSFT: {stock_prices}\")\n\n# Length of the dictionary\ndict_length = len(stock_prices)\nprint(f\"Length: {dict_length}\")\n\nThe collection module from Python‚Äôs standard library provides many other data structures such as defaultdict, OrderedDict, Counter, and deque. You can learn more about these data structures in the Python documentation."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#comparison-operators-and-branching",
    "href": "intro-to-python/python-basics/index.html#comparison-operators-and-branching",
    "title": "Python Basics",
    "section": "Comparison operators and branching",
    "text": "Comparison operators and branching\n\nComparison operators\nPython provides several comparison operators that allow you to compare values and evaluate expressions. Comparison operators can be used with various data types, such as numbers, strings, or even complex data structures, and return a boolean value (True or False). Table¬†9 lists the comparison operators available in Python.\n\n\n\nTable¬†9: Comparison operators in Python\n\n\n\n\n\nOperator\nName\nExample\nResult\n\n\n\n\n==\nEqual\n1 == 2\nFalse\n\n\n!=\nNot equal\n1 != 2\nTrue\n\n\n&gt;\nGreater than\n1 &gt; 2\nFalse\n\n\n&lt;\nLess than\n1 &lt; 2\nTrue\n\n\n&gt;=\nGreater or equal\n1 &gt;= 2\nFalse\n\n\n&lt;=\nLess or equal\n1 &lt;= 2\nTrue\n\n\nin\nMembership\n1 in [1, 2, 3]\nTrue\n\n\nis\nIdentity comparison\n1 is None\nFalse\n\n\n\n\n\n\nTo create more complex conditions, you can chain multiple comparisons in a single expression using logical operators like and, or, or not. The result of a logical operator is a boolean value (True or False). Table¬†10 lists the logical operators available in Python.\n\n\n\nTable¬†10: Logical operators in Python\n\n\n\n\n\na\nb\na and b\na or b\nnot a\n\n\n\n\nTrue\nTrue\nTrue\nTrue\nFalse\n\n\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\nFalse\nTrue\nFalse\nTrue\nTrue\n\n\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\nWarning& and | are bitwise operators, not logical operators\n\n\n\nPython also provides bitwise operators that perform bitwise operations on integers. These operators are & (bitwise AND), | (bitwise OR), ^ (bitwise XOR), ~ (bitwise NOT), &lt;&lt; (bitwise left shift), and &gt;&gt; (bitwise right shift). Most casual Python users will not need to use these operators, but they can be confusing for new users due to their similar syntax to logical operators in other programming languages. To add to the confusion, popular Python libraries like NumPy and Pandas overload the bitwise operators to perform logical operations on arrays.\nIt is crucial for beginners to understand the distinction between logical and bitwise operators and to use the appropriate operators (which are usually and, or, or not) based on their intended purpose to ensure the desired logical evaluations are achieved.\n\n\nLonger expressions can be grouped using parentheses to ensure the desired order of operations. For example, a and b or c is equivalent to (a and b) or c, whereas a and (b or c) is different. Python does not offer a built-in exclusive or (XOR) operator, but it can be achieved using a combination of other operators.\n\ndef xor(a, b):\n    return (a and not b) or (not a and b)\n\n\nprint(f\"xor(True, True)) = {xor(True, True)}\")\nprint(f\"xor(True, False)) = {xor(True, False)}\")\nprint(f\"xor(False, True)) = {xor(False, True)}\")\nprint(f\"xor(False, False)) = {xor(False, False)}\")\n\n\n\nBranching\nBranching allows your code to execute different actions based on specific conditions. The primary branching construct in Python is the if statement, which can be combined with elif (short for ‚Äúelse if‚Äù) and else clauses to create more complex decision-making structures.\nLike other compound statements in Python, the if statement uses indentation to group statements together. The general syntax for an if statement is to start with the if keyword followed by a condition, then a colon (:), and, finally, an indented block of code that will be executed if the condition evaluates to True. The elif and else clauses are optional and can be used to specify additional conditions and code blocks to execute if the initial condition evaluates to False. The elif clause is used to chain multiple conditions together, whereas the else clause is used to specify a default code block to execute if none of the previous conditions evaluate to True.\n\nprice = 150\n\nif price &gt; 100:\n    print(\"The stock price is high.\")\n\nIn the previous example, the code block is executed because the price = 150, therefore the condition price &gt; 100 evaluates to True.\nWe can add an else clause to specify a default code block to execute if the condition evaluates to False.\n\nprice = 50\n\nif price &gt; 100:\n    print(\"The stock price is high.\")\nelse:\n    print(\"The stock price is low.\")\n\nWe can add an elif clause to specify additional conditions to evaluate if the initial condition evaluates to False. The elif clause can be used multiple times to chain multiple conditions together. The elif clause is optional, but if it is used, it must come before the else clause. The else clause is also optional, but if it is used, it must come last.\nIn all cases, the code block associated with the first condition that evaluates to True will be executed, and the remaining conditions will be skipped. If none of the conditions evaluate to True, then the code block associated with the else clause will be executed. If there is no else clause, then nothing will be executed.\n\nprice = 75\n\nif price &gt; 100:\n    print(\"The stock price is high.\")\nelif price &gt; 50:\n    print(\"The stock price is moderate.\")\nelse:\n    print(\"The stock price is low.\")\n\nYou can nest if statements inside other if statements to create more complex branching structures. The code block associated with the nested if statement must be indented further than the outer if statement. The nested if statement will only be evaluated if the condition associated with the outer if statement evaluates to True. When reading nested if statements, it is helpful to read from the top down and to keep track of the indentation level to understand which code blocks are associated with which conditions.\n\nprice = 150\nvolume = 1000000\n\nif price &gt; 100:\n    if volume &gt; 500000:\n        print(\"The stock price is high and has high volume.\")\n    else:\n        print(\"The stock price is high but has low volume.\")\nelse:\n    print(\"The stock price is not high.\")\n\nConditions can be combined using the logical operators and, or, and not to create more complex conditions.\n\nprice = 150\nvolume = 1000000\n\nif price &gt; 100 and volume &gt; 500000:\n    print(\"The stock price is high and has high volume.\")\nelif price &gt; 100 or volume &gt; 500000:\n    print(\"The stock price is high or has high volume.\")\nelse:\n    print(\"The stock price is not high and has low volume.\")\n\n\n\nConditional assignment\nPython provides a convenient shorthand for assigning a value to a variable based on a condition. This is known as conditional assignment. The syntax for conditional assignment is variable = value1 if condition else value2. If the condition evaluates to True, the variable is assigned value1; otherwise, it is assigned value2.\n\nprice = 150\n\nmessage = \"The stock price is high.\" if price &gt; 100 else \"The stock price is low.\"\nprint(message)"
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#typing",
    "href": "intro-to-python/python-basics/index.html#typing",
    "title": "Python Basics",
    "section": "Typing",
    "text": "Typing\nPython is a dynamically typed language. This means that you don‚Äôt have to specify the type of a variable when you define it. The Python interpreter will automatically infer the type based on the value assigned to the variable.\nPython also supports optional type annotations, also called type hints, since version 3.5. This allows you to specify the types of variables, function arguments, and return values to improve code readability and catch potential errors early. The Python interpreter will ignore the type annotations and run the code normally. However, you can use external tools like mypy to analyze the code and check for type errors, and modern IDEs like VS Code provide built-in support for type checking.\n\nticker: str = \"AAPL\"\n\n\nstock_prices: list[float] = [150.25, 1200.50, 250.00]\n\n\n# Old version, not needed since Python 3.9\n\nfrom typing import List\n\nstock_prices: List[float] = [150.25, 1200.50, 250.00]\n\n\n# You can also specify function parameters and return types:\n\n\ndef calculate_profit(revenue: float, expenses: float) -&gt; float:\n    return revenue - expenses\n\n\nrevenue = 1000.00\nexpenses = 500.00\nprofit = calculate_profit(revenue, expenses)\n\n\n\n\n\n\n\nTipType hints in Visual Studio Code\n\n\n\nType hints are not required to run Python code, but they can be very useful to improve code readability and catch potential errors early. Modern IDEs like VS Code provide built-in support for type checking that you can enable.\nI find this quite overwhelming, so I prefer to enable type-checking only when needed. However, VS Code still uses type hints to provide useful features like hover info.\n\n\n\nTooltip when hovering over variable.\n\n\n\n\n\nTooltip when hovering over function.\n\n\n\n\nThe typing module provides a set of special types that can be used in type hints. Here are some of the most commonly used ones:\n\nAny: Any type\nOptional: An optional value (can be None)\nCallable: A function\nIterable: An iterable object (e.g., list, tuple, set)"
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#sec-functions-parameters-return-values",
    "href": "intro-to-python/python-basics/index.html#sec-functions-parameters-return-values",
    "title": "Python Basics",
    "section": "Functions: parameters and return values",
    "text": "Functions: parameters and return values\nFunctions help you organize and structure your code by encapsulating specific tasks or calculations. They allow you to define input parameters, perform operations, and return the results, making your code more flexible and maintainable. We have already written simple functions in Section¬†7; we will now look in more detail at how to define parameters and return values.\n\n\n\n\n\n\nNoteType hints in examples\n\n\n\nIn the examples below, I use type hints to indicate the type of the function parameters and return values. Type hints are not required to run Python code, but using them is a good practice as they provide helpful information to other developers (including your future self!) and tools such as linters and type checkers.\n\n\n\nParameters\nParameters are variables defined within the function signature, enabling you to pass input values to the function when it is called. Parameters can have a default value assigned to them when no value is provided during the function call. Default values can make your functions more flexible and easy to use. Using the *args and **kwargs syntax, you can pass a variable number of positional or keyword arguments to a function, providing greater flexibility for handling different input scenarios.2\n\n\n\n\n\n\nNoteParameter vs.¬†argument\n\n\n\nThe terms function parameter and argument refer to different concepts related to functions.\nFunction parameter: A function parameter is a variable defined in the function‚Äôs definition or signature. It represents a value that the function expects to receive when it is called. Parameters act as placeholders for the actual values that will be passed as arguments when the function is invoked.\nArgument: An argument is the actual value that is passed to a function when it is called. It corresponds to a specific function parameter and provides the actual data or input that the function operates on. Arguments are supplied in the function call, within parentheses, and are passed to the corresponding function parameters based on their position or using keyword arguments.\n\n\n\ndef calculate_roi(investment: float, profit: float) -&gt; float:\n    return (profit / investment) * 100.0\n\n\ninvestment = 2000.00\nprofit = 500.00\nroi = calculate_roi(investment, profit)\nprint(roi)\n\nIn the previous example, variables investment and profit are passed to the function calculate_roi() in the same order as they are defined in the function definition. This is called positional arguments. The names of the variables do not matter, only the order in which they are passed to the function. If we invert the order of the variables, the result will be different.\n\nbad_roi = calculate_roi(profit, investment)\nprint(bad_roi)\n\nPositional arguments can be confusing when the function has many parameters or when the order of the arguments is not obvious. To avoid this, you can use keyword arguments.\n\nroi1 = calculate_roi(investment=2000.00, profit=500.00)\nprint(roi1)\n\nroi2 = calculate_roi(profit=500.00, investment=2000.00)\nprint(roi2)\n\nroi3 = calculate_roi(profit=profit, investment=investment)\nprint(roi3)\n\nNote that the name of the original variables does not matter, only the name of the parameters in the function definition. In the last example, we use the same names for the variables and the parameters, but the Python interpreter does not care about that. The following code is equivalent to the previous one:\n\nx = 2000.00\ny = 500.00\n\nroi4 = calculate_roi(profit=y, investment=x)\nprint(roi4)\n\nYou can also mix positional and keyword arguments. However, positional arguments must always come before keyword arguments.\n\nroi4 = calculate_roi(investment, profit=profit)\nprint(roi4)\n\nDefault parameters are useful when you want to provide a default value for a parameter, which is used when no value is provided during the function call. This makes your functions more flexible and easy to use, as you can omit parameters that have a default value.\nTo define a default parameter, assign a value to the parameter in the function definition using =. When the function is called, the default value will be used if no value is provided for that parameter. If a value is provided, it will override the default value.\n\ndef calculate_present_value(cashflow: float, discount_rate: float = 0.1) -&gt; float:\n    return cashflow / (1 + discount_rate)\n\n\n# Uses default discount_rate of 10%\npv = calculate_present_value(cashflow=100.00)\nprint(f\"Present Value: {pv}\")\n\n# Uses discount_rate of 5%\npv2 = calculate_present_value(cashflow=100.00, discount_rate=0.05)\nprint(f\"Present Value: {pv2}\")\n\nParameters with default values must come after parameters without default values. Otherwise, the function call will raise a SyntaxError. When you call a function with default parameters, you can omit any parameters that have a default value. However, when you omit a parameter, you must use keyword parameters to specify the values for the parameters that follow it.\n\n\nPassing arguments: peek under the hood\nPython uses a mechanism called ‚Äúpassing arguments by assignment.‚Äù In simple terms, this means that when you pass an argument to a function, a copy of the reference to the object is made and assigned to the function parameter.\nWhen an immutable object (like a number, string, or tuple) is passed as an argument, it is effectively passed by value. Any modifications made to the parameter within the function do not affect the original object outside the function. Changes to the parameter create a new object rather than modifying the original one.\nOn the other hand, when a mutable object (like a list or dictionary) is passed as an argument, it is effectively passed by reference. Any modifications made to the parameter within the function will affect the original object outside the function. This is because both the parameter and the original object refer to the same memory location, so changes are reflected in both.\n\n\nReturn values\nFunctions can return a value, multiple values, or no value at all. To return a value, use the return keyword followed by the value or expression you want to return. If a function doesn‚Äôt include a return statement, it will implicitly return None. A function can contain multiple return statements, but the execution of the function will stop as soon as any of them is reached.\nA function can return a single value, such as a number, string, or a more complex data structure. A function can also return multiple values, typically in the form of a tuple. This is useful when you need to return several related results from a single function call. If a function doesn‚Äôt explicitly return a value using the return keyword, it will implicitly return None when it reaches the end of the function body.\n\ndef calculate_mean_and_median(numbers: list[float]) -&gt; tuple[float, float]:\n    mean = sum(numbers) / len(numbers)\n\n    # Sort the numbers in ascending order using the sorted() function\n    sorted_numbers = sorted(numbers)\n    length = len(sorted_numbers)\n\n    if length % 2 == 0:\n        median = (sorted_numbers[length // 2 - 1] + sorted_numbers[length // 2]) / 2\n    else:\n        median = sorted_numbers[length // 2]\n\n    return mean, median\n\n\nprices = [150.25, 1200.50, 250.00]\nmean, median = calculate_mean_and_median(prices)\nprint(f\"Mean: {mean}, Median: {median}\")\n\n\n\nScope\nIn Python, the scope of a variable refers to the region of a program where the variable is accessible and can be referenced. The scope determines the visibility and lifetime of a variable, including where it can be accessed and modified.\nWhen using Python functions, there are two main scopes to consider:\n\nLocal scope (function scope): Variables defined within a function have local scope. They are accessible only within the function where they are defined. Local variables are created when the function is called and destroyed when the function execution completes or reaches a return statement. They are not accessible outside the function.\nGlobal scope (module scope): Variables defined outside of any function in the interactive window or in a Python script, have global scope. They are accessible from anywhere within the program, including all functions.\n\nWhen a function is called, it creates a new local scope, which is independent of the global scope. Inside the function, the local scope takes precedence over the global scope. If a variable is referenced within a function, Python first checks the local scope for its existence. If not found, it then searches the global scope.\n\n# Global variable\nmessage = \"Hello\"\nx = 123\n\n\ndef say_hello(m: str):\n    # Local variable\n    message = \"Hello, World!\"\n    print(f\"local message = {message}\")\n\n    # Local variable, copied from the argument\n    print(f\"local m = {m}\")\n\n    # print(f\"global x inside function = {x}\") \n1\n\n    # Local variable\n    x = len(m)\n    print(f\"local x = {x}\")\n\n    return x\n\n\ny = say_hello(message)\n\nprint(f\"global message = {message}\")\nprint(f\"global x after function = {x}\")\nprint(f\"global y = {y}\")\n\n\n1\n\nThis will access the global x if there is no local variable with the same name. In this specific case, it will cause an error because x is actually defined later in the function.\n\n\n\n\nIf you want to modify a global variable from within a function, you can use the global keyword to indicate that the variable being referred to is a global variable rather than creating a new local variable. However, this is generally not recommended, as it can lead to unexpected side effects and make the code difficult to understand and debug.\nIt is important to carefully manage variable scope to avoid naming conflicts and unintended side effects. Understanding the scope of variables helps in organizing and managing data within functions and modules effectively."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#loops",
    "href": "intro-to-python/python-basics/index.html#loops",
    "title": "Python Basics",
    "section": "Loops",
    "text": "Loops\nLoops enable you to easily perform repetitive tasks or iterate through data structures, such as sequences and collections. Python provides two primary loop constructs: the for loop and the while loop.\n\nfor loops\nFor loops in Python are used to iterate over a sequence (e.g., a list, tuple, or string) or other iterable objects. The loop iterates through each item in the sequence, executing a block of code for each item. The ‚Äòfor‚Äô loop has the following syntax:\nfor item in sequence:\n    # code to execute for each item in the sequence\nAs for function bodies and conditional statements, the code block in a loop must be indented.\n\n\nrange() function\nThe built-in range() function in Python is often used in conjunction with for loops to generate a sequence of numbers. This function can be used to create a range of numbers with a specified start, end, and step size. The syntax for the range function is:\nrange(start, stop, step)\nThe ‚Äòstart‚Äô and ‚Äòstep‚Äô arguments are optional, with default values of 0 and 1, respectively. The ‚Äòstop‚Äô argument is required and defines the upper limit of the range (exclusive).\n\n1for i in range(5):\n    print(i)\n\n\n1\n\nThe range(5) function generates a sequence of numbers from 0 to 4 (inclusive) with a step of 1.\n\n\n\n\n\n1for i in range(2, 7):\n    print(i)\n\n\n1\n\nThe range(2, 7) function generates a sequence of numbers from 2 to 6 (inclusive) with a step of 1.\n\n\n\n\n\n1for i in range(1, 11, 2):\n    print(i)\n\n\n1\n\nThe range(1, 11, 2) function generates a sequence of numbers from 1 to 10 (inclusive) with a step of 2.\n\n\n\n\nYou can use a negative step size to generate a sequence of numbers in reverse order.\n\n1for i in range(5, 0, -1):\n    print(i)\n\n\n1\n\nThe range(5, 0, -1) function generates a sequence of numbers from 5 to 1 (inclusive) with a step of -1 (decreasing).\n\n\n\n\nYou can use the range() function to generate a sequence of numbers and iterate through them using a for loop to execute some code for each number in the sequence. In this example, we use the range() function to generate a sequence of numbers from 1 to 5 (inclusive) and calculate the compound interest for each year of an investment.\n\nprincipal = 1000\nrate = 0.05\n\nfor year in range(1, 6):\n    interest = principal * ((1 + rate) ** year - 1)\n    print(f\"Year {year}: Interest = {interest:.2f}\")\n\n\n\ncontinue and break statements\nYou can use the continue and break statements to control the flow of a for loop. The continue statement skips the current iteration and continues with the next one. The break statement terminates the loop and transfers execution to the statement immediately following the loop.\n\nfor i in range(10):\n    if i == 3:\n1        continue\n    elif i == 5:\n2        break\n    print(i)\n\n\n1\n\nSkip the rest of the code in the loop and go to the next iteration\n\n2\n\nExit the loop\n\n\n\n\n\n\nfor loops with lists\nYou can also loop over a collection of items using the for loop. For example, you can loop over a list of numbers to calculate the sum of all numbers in the list.\n\ndaily_profit_losses = [1500, 1200, 1800, 2300, 900]\n\ntotal_pl = 0\nfor pl in daily_profit_losses:\n    total_pl += pl\n\nprint(f\"Total P&L for the period: {total_pl}\")\n\n\n\n\n\n\n\nTipBuilt-in functions\n\n\n\nPython provides several built-in functions that can be used to perform common tasks. For example, the sum() function can be used to calculate the sum of all numbers in a list.\n\ndaily_profit_losses = [1500, 1200, 1800, 2300, 900]\ntotal_pl = sum(daily_profit_losses)\nprint(f\"Total P&L for the period: {total_pl}\")\n\n\n\nYou can combine two lists of the same length using the zip() built-in function to loop over both lists at the same time.\n\nstock_prices = [150.25, 1200.50, 250.00]\nquantities = [10, 5, 20]\n\ntotal_value = 0\nfor price, quantity in zip(stock_prices, quantities):\n    total_value += price * quantity\n\nprint(f\"Total value of the portfolio: {total_value:.2f}\")\n\nYou can use the enumerate() function to loop over a list and get the index of each item in the list.\n\ncash_flows = [100, 200, 300, 400, 500]\n\ndiscount_rate = 0.1\n\npresent_values = []\nfor year, cash_flow in enumerate(cash_flows):\n    present_value = cash_flow / (1 + discount_rate) ** year\n    print(f\"Year {year}: Present Value = {present_value:.2f}\")\n\n\n\n\n\n\n\nNoteIterables and iterators\n\n\n\nIn Python, an iterable is an object capable of returning its elements one at a time, such as a list, tuple, or string. An iterator is an object that keeps track of its current position within an iterable and provides a way to access the next element when required.\nMany built-in data types and functions return values in Python are iterables or iterators. For example, the range() function returns an iterator that produces a sequence of numbers. The zip() function returns an iterator that produces tuples containing elements from the input iterables. The enumerate() function returns an iterator that produces tuples containing the index and value of each item in the input iterable.\nThere are many benefits to iterators, such as better memory efficiency and allowing you to work with infinite sequences, such as the sequence of all prime numbers. However, you can‚Äôt print the result of calling an iterator like zip() directly. Instead, you must convert the iterator to a list or another collection using the list() function.\n\nstock_prices = [150.25, 1200.50, 250.00]\nquantities = [10, 5, 20]\n\nzipped = zip(stock_prices, quantities)\n\nprint(f\"zipped: {zipped}\")\nprint(f\"list(zipped): {list(zipped)}\")\n\n\n\n\n\n\nNested for loops\nYou can nest loops. The inner loop will be executed one time for each iteration of the outer loop.\nIn this example, we have a list of products, each with a name, per-item profit margin, and a list of quantities sold at different times. The outer loop iterates through each product, while the inner loop iterates through the quantities for each product. The product‚Äôs profit is calculated by multiplying its margin by the quantity sold and adding it to the product_profit variable. The total_profit variable accumulates the profit for all products.\n\nproducts = [\n    {\"name\": \"Product A\", \"margin\": 10, \"quantities\": [5, 10, 15]},\n    {\"name\": \"Product B\", \"margin\": 20, \"quantities\": [2, 4, 6]},\n    {\"name\": \"Product C\", \"margin\": 30, \"quantities\": [1, 3, 5]},\n]\n\ntotal_profit = 0\n\nfor product in products:\n    product_profit = 0\n    for quantity in product[\"quantities\"]:\n        product_profit += product[\"margin\"] * quantity\n    total_profit += product_profit\n    print(f\"Profit for {product['name']}: {product_profit}\")\n\nprint(f\"Total profit: {total_profit}\")\n\n\n\nwhile loops\nWhile loops are used to repeatedly execute a block of code as long as a specified condition is True. The while loop has the following syntax:\nwhile condition:\n    # code to execute while the condition is True\nIn this example, we use a while loop to calculate the number of years it takes for an investment to double at a given interest rate.\n\nprincipal = 1000\nrate = 0.05\nbalance = principal\ntarget = principal * 2\nyears = 0\n\nwhile balance &lt; target:\n    interest = balance * rate\n    balance += interest\n    years += 1\n\nprint(f\"It takes {years} years for the investment to double.\")\n\nYou can use the continue statement to skip the rest of the code in the current iteration and continue with the next one and the break statement to exit a while loop before the condition becomes False."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#list-and-dictionary-comprehensions",
    "href": "intro-to-python/python-basics/index.html#list-and-dictionary-comprehensions",
    "title": "Python Basics",
    "section": "List and dictionary comprehensions",
    "text": "List and dictionary comprehensions\nPython supports list and dictionary comprehensions, which allow you to create lists and dictionaries in a concise and efficient manner by transforming or filtering items from another iterable, such as a range, a tuple or another list. Comprehensions can be confusing at first, but they are a powerful tool worth learning.\n\nList comprehensions\nList comprehension syntax consists of square brackets containing an expression followed by a for clause, then zero or more for or if clauses. The expression can be anything, meaning you can put in all kinds of objects in lists.\n\n# This is perfectly valid:\nsquared1 = []\nfor x in range(10):\n    squared1.append(x * x)\n\nprint(squared1)\n\n# This is much shorter!\nsquared2 = [x * x for x in range(10)]\n\nprint(squared2)\n\nYou can use list comprehensions to transform items from a list into a new list using complex expressions.\n\n# Calculate the percentage change for a list of stock prices\nstock_prices = [150.25, 1200.50, 250.00, 175.00, 305.75]\npercentage_changes = [\n    (stock_prices[i + 1] - stock_prices[i]) / stock_prices[i] * 100\n    for i in range(len(stock_prices) - 1)\n]\n\nprint(\"Percentage changes:\", percentage_changes)\n\n\n\nDictionary comprehensions\nSimilar to list comprehensions, dictionary comprehensions use a single line of code to define the structure of the new dictionary.\n\n# Create a dictionary mapping numbers to their squares\nsquares = {i: i**2 for i in range(1, 6)}\n\nprint(squares)\n\n\n\nFiltering and transforming\nYou can use conditional statements in list and dictionary comprehensions to filter items from the source iterable.\n\n# Create a dictionary mapping even numbers to their cubes\neven_cubes = {i: i**3 for i in range(1, 6) if i % 2 == 0}\n\nprint(even_cubes)\n\nYou can transform the items in the source iterable before adding them to the new list or dictionary.\n\n# List of stock symbols and prices\nstock_data = [(\"AAPL\", 150.25), (\"GOOG\", 1200.50), (\"MSFT\", 250.00)]\n\n# Create a dictionary mapping lowercase stock symbols to their prices\nstocks = {symbol.lower(): price for symbol, price in stock_data}\n\nprint(stocks)\n\n\n\nNested comprehensions\nYou can nest comprehensions inside other comprehensions to create complex data structures.\n\n# Create a list of (stock, prices) tuples\nstock_data = [\n    (\"AAPL\", [150.25, 150.50, 150.75]),\n    (\"GOOG\", [1200.50, 1201.00]),\n    (\"MSFT\", [250.00, 250.25, 250.50, 250.75]),\n]\n\n1stocks = [(symbol, price) for symbol, prices in stock_data for price in prices]\nprint(stocks)\n\n\n1\n\nThe comprehension is evaluated from left to right, so the prices variable is available in the second for clause.\n\n\n\n\n\n\n\n\n\n\nWarningNested comprehensions vs readability\n\n\n\nNested comprehensions with more than two levels can be difficult to read, so you should avoid them if possible. If you find yourself nesting comprehensions, it‚Äôs probably a good idea to use a regular for loop instead. Remember, code readability is more important than brevity."
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#pattern-matching",
    "href": "intro-to-python/python-basics/index.html#pattern-matching",
    "title": "Python Basics",
    "section": "Pattern matching",
    "text": "Pattern matching\nPattern matching is a powerful feature introduced in Python 3.10. It allows you to match the structure of data and execute code based on the shape and contents of that data. It is particularly useful for working with complex data structures and can lead to cleaner and more readable code.\n\n\n\n\n\n\nCautionPython 3.10+ only\n\n\n\nPattern matching is a new feature introduced in Python 3.10, released on October 4, 2021. If you‚Äôre using an older version of Python, or your code will be running on a system with an older version of Python, you should avoid using pattern matching.\n\n\nPattern matching is implemented using the match statement, which is similar to a switch-case statement in other languages but with more advanced capabilities. The match statement takes an expression and a series of cases. Each case is a pattern that is matched against the expression in turn. If the pattern matches, the code in that case is executed, otherwise, the next case is checked. The match statement can also have a case with the wildcard pattern _, which will always match. If no pattern matches, a MatchError is raised.\n\ndef process_transaction(transaction: tuple):\n    match transaction:\n        case (\"deposit\", amount):\n            print(f\"Deposit: {amount:.2f}\")\n        case (\"withdraw\", amount):\n            print(f\"Withdraw: {amount:.2f}\")\n        case (\"transfer\", amount, recipient):\n            print(f\"Transfer {amount:.2f} to {recipient}\")\n        case _:\n            print(\"Unknown transaction\")\n\nprocess_transaction((\"deposit\", 1000))\nprocess_transaction((\"burn\", 100.00))\nprocess_transaction((\"transfer\", 500.00, \"John Doe\"))\nprocess_transaction((\"withdraw\", 250.00))"
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#additional-resources",
    "href": "intro-to-python/python-basics/index.html#additional-resources",
    "title": "Python Basics",
    "section": "Additional resources",
    "text": "Additional resources\n\nPython 3.12 documentation\nLubanovic, Bill. Introducing Python, 2nd Edition, O‚ÄôReilly Media, Inc., 2019"
  },
  {
    "objectID": "intro-to-python/python-basics/index.html#footnotes",
    "href": "intro-to-python/python-basics/index.html#footnotes",
    "title": "Python Basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA pseudo-random number is a sequence of numbers that appear random but are generated using a deterministic algorithm.‚Ü©Ô∏é\nI do not recommend using variable-length parameters unless you have a specific need for them, as they can make your code more complex and harder to read and understand.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vincent Codes Finance",
    "section": "",
    "text": "Claude Code for Data Analysis\n\n\n\n\n\n\nVS Code\n\nLLM\n\nPython\n\nMachine Learning\n\nAI\n\n\n\n\n\n\n\n\n\nSep 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUse Devcontainers for safe and replicable research\n\n\n\n\n\n\nVisual Studio Code\n\nReproducibility\n\nTutorial\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Ollama with Continue as an AI-Powered Coding and Writing Assistant\n\n\n\n\n\n\nVS Code\n\nCopilot\n\nLocal AI\n\nLLM\n\nPython\n\nMachine Learning\n\nAI\n\nWriting\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial news sentiment analysis in Python\n\n\n\n\n\n\nPython\n\nSentiment\n\nBert\n\nLLM\n\nMachine Learning\n\nAI\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nScrape financial data from SEC Edgar with Python and edgartools\n\n\n\n\n\n\nPython\n\nScraping\n\n\n\n\n\n\n\n\n\nApr 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSummarize and query PDFs with AI using Ollama\n\n\n\n\n\n\nLocal AI\n\nLLM\n\nPython\n\nMachine Learning\n\nAI\n\n\n\n\n\n\n\n\n\nMar 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSpeed up your scientific computing workflows with MLX\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Ollama as a ChatGPT Replacement\n\n\n\n\n\n\nLocal AI\n\nLLM\n\nMachine Learning\n\nAI\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient reading and parsing of large CSV files in Python with Pandas and Arrow\n\n\n\n\n\n\nPython\n\nPandas\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWriting LaTeX Documents in VS Code\n\n\n\n\n\n\nVS Code\n\nLaTeX\n\nWriting\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing GitHub for academic research\n\n\n\n\n\n\nGit/GitHub\n\nReproducibility\n\nTutorial\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating standard errors in time series data with Python and statsmodels\n\n\n\n\n\n\nPython\n\nEconometrics\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating standard errors in panel data with Python and linearmodels\n\n\n\n\n\n\nPython\n\nEconometrics\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPython basic tutorial\n\n\n\n\n\n\nPython\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Python 3.12\n\n\n\n\n\n\nPython\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNew Year, new blog\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\nNo matching items\nReuseCC BY-NC-SA 4.0"
  }
]